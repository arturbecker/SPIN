{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Normalize\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "from models import hmr, SMPL\n",
    "from utils.imutils import crop\n",
    "from utils.renderer import Renderer\n",
    "import config\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_from_openpose(openpose_file, rescale=1.2, detection_thresh=0.2):\n",
    "    \"\"\"Get center and scale for bounding box from openpose detections.\"\"\"\n",
    "    with open(openpose_file, 'r') as f:\n",
    "        keypoints = json.load(f)['people'][0]['pose_keypoints_2d']\n",
    "    keypoints = np.reshape(np.array(keypoints), (-1,3))\n",
    "    valid = keypoints[:,-1] > detection_thresh\n",
    "    valid_keypoints = keypoints[valid][:,:-1]\n",
    "    center = valid_keypoints.mean(axis=0)\n",
    "    bbox_size = (valid_keypoints.max(axis=0) - valid_keypoints.min(axis=0)).max()\n",
    "    # adjust bounding box tightness\n",
    "    scale = bbox_size / 200.0\n",
    "    scale *= rescale\n",
    "    return center, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_from_json(bbox_file):\n",
    "    \"\"\"Get center and scale of bounding box from bounding box annotations.\n",
    "    The expected format is [top_left(x), top_left(y), width, height].\n",
    "    \"\"\"\n",
    "    with open(bbox_file, 'r') as f:\n",
    "        bbox = np.array(json.load(f)['bbox']).astype(np.float32)\n",
    "    ul_corner = bbox[:2]\n",
    "    center = ul_corner + 0.5 * bbox[2:]\n",
    "    width = max(bbox[2], bbox[3])\n",
    "    scale = width / 200.0\n",
    "    # make sure the bounding box is rectangular\n",
    "    return center, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_file, bbox_file, openpose_file, input_res=224):\n",
    "    \"\"\"Read image, do preprocessing and possibly crop it according to the bounding box.\n",
    "    If there are bounding box annotations, use them to crop the image.\n",
    "    If no bounding box is specified but openpose detections are available, use them to get the bounding box.\n",
    "    \"\"\"\n",
    "    normalize_img = Normalize(mean=constants.IMG_NORM_MEAN, std=constants.IMG_NORM_STD)\n",
    "    img = cv2.imread(img_file)[:,:,::-1].copy() # PyTorch does not support negative stride at the moment\n",
    "    if bbox_file is None and openpose_file is None:\n",
    "        # Assume that the person is centerered in the image\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        center = np.array([width // 2, height // 2])\n",
    "        scale = max(height, width) / 200\n",
    "    else:\n",
    "        if bbox_file is not None:\n",
    "            center, scale = bbox_from_json(bbox_file)\n",
    "        elif openpose_file is not None:\n",
    "            center, scale = bbox_from_openpose(openpose_file)\n",
    "    img = crop(img, center, scale, (input_res, input_res))\n",
    "    img = img.astype(np.float32) / 255.\n",
    "    img = torch.from_numpy(img).permute(2,0,1)\n",
    "    norm_img = normalize_img(img.clone())[None]\n",
    "    return img, norm_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--outfile'], dest='outfile', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help='Filename of output images. If not set use input filename.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--checkpoint', required=True, help='Path to pretrained checkpoint')\n",
    "parser.add_argument('--img', type=str, required=True, help='Path to input image')\n",
    "parser.add_argument('--bbox', type=str, default=None, help='Path to .json file containing bounding box coordinates')\n",
    "parser.add_argument('--openpose', type=str, default=None, help='Path to .json containing openpose detections')\n",
    "parser.add_argument('--outfile', type=str, default=None, help='Filename of output images. If not set use input filename.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args(['--checkpoint=data/model_checkpoint.pt','--img=examples/im1010.jpg'])\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    # Load pretrained model\n",
    "    model = hmr(config.SMPL_MEAN_PARAMS).to(device)\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "    # Load SMPL model\n",
    "    smpl = SMPL(config.SMPL_MODEL_DIR,\n",
    "                batch_size=1,\n",
    "                create_transl=False).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Setup renderer for visualization\n",
    "    renderer = Renderer(focal_length=constants.FOCAL_LENGTH, img_res=constants.IMG_RES, faces=smpl.faces)\n",
    "\n",
    "\n",
    "    # Preprocess input image and generate predictions\n",
    "    img, norm_img = process_image(args.img, args.bbox, args.openpose, input_res=constants.IMG_RES)\n",
    "    with torch.no_grad():\n",
    "        pred_rotmat, pred_betas, pred_camera = model(norm_img.to(device))\n",
    "        pred_output = smpl(betas=pred_betas, body_pose=pred_rotmat[:,1:], global_orient=pred_rotmat[:,0].unsqueeze(1), pose2rot=False)\n",
    "        pred_vertices = pred_output.vertices\n",
    "        \n",
    "    # Calculate camera parameters for rendering\n",
    "    camera_translation = torch.stack([pred_camera[:,1], pred_camera[:,2], 2*constants.FOCAL_LENGTH/(constants.IMG_RES * pred_camera[:,0] +1e-9)],dim=-1)\n",
    "    camera_translation = camera_translation[0].cpu().numpy()\n",
    "    pred_vertices = pred_vertices[0].cpu().numpy()\n",
    "    img = img.permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    \n",
    "    # Render parametric shape\n",
    "    img_shape = renderer(pred_vertices, [0,0,1], np.ones_like(img))\n",
    "    \n",
    "    # Render side views\n",
    "    aroundy = cv2.Rodrigues(np.array([0, np.radians(90.), 0]))[0]\n",
    "    center = pred_vertices.mean(axis=0)\n",
    "    rot_vertices = np.dot((pred_vertices - center), aroundy) + center\n",
    "    \n",
    "    # Render non-parametric shape\n",
    "    img_shape_side = renderer(rot_vertices, camera_translation, np.ones_like(img))\n",
    "\n",
    "    outfile = args.img.split('.')[0] if args.outfile is None else args.outfile\n",
    "\n",
    "    # Save reconstructions\n",
    "    cv2.imwrite(outfile + '_shape.png', 255 * img_shape[:,:,::-1])\n",
    "    cv2.imwrite(outfile + '_shape_side.png', 255 * img_shape_side[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an iterator based on the concept of basic rotations https://en.wikipedia.org/wiki/Rotation_matrix#Basic_rotations\n",
    "\n",
    "t_pose = torch.zeros(1,24,3,3,device='cuda')\n",
    "t_pose[:] = torch.eye(3)\n",
    "t_betas = torch.zeros(1,10,device='cuda')\n",
    "\n",
    "t_pose_model = smpl(betas=t_betas, body_pose=t_pose[:,1:], global_orient=t_pose[:,0].unsqueeze(1), pose2rot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'permute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4fe9dd32af3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpred_vertices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_pose_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpred_vertices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_vertices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'permute'"
     ]
    }
   ],
   "source": [
    "# Setup renderer for visualization\n",
    "renderer = Renderer(focal_length=constants.FOCAL_LENGTH, img_res=constants.IMG_RES, faces=smpl.faces)\n",
    "\n",
    "# Calculate camera parameters for rendering\n",
    "camera_translation = torch.stack([pred_camera[:,1], pred_camera[:,2], 2*constants.FOCAL_LENGTH/(constants.IMG_RES * pred_camera[:,0] +1e-9)],dim=-1)\n",
    "camera_translation = camera_translation[0].cpu().numpy()\n",
    "pred_vertices = t_pose_model.vertices\n",
    "pred_vertices = pred_vertices[0].cpu().numpy()\n",
    "img = img.permute(1,2,0).cpu().numpy()\n",
    "\n",
    "\n",
    "# Render parametric shape\n",
    "img_shape = renderer(pred_vertices, camera_translation, np.ones_like(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args(['--checkpoint=data/model_checkpoint.pt','--img=examples/im1010.jpg'])\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    # Load pretrained model\n",
    "    model = hmr(config.SMPL_MEAN_PARAMS).to(device)\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "    # Load SMPL model\n",
    "    smpl = SMPL(config.SMPL_MODEL_DIR,\n",
    "                batch_size=1,\n",
    "                create_transl=False).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Setup renderer for visualization\n",
    "    renderer = Renderer(focal_length=constants.FOCAL_LENGTH, img_res=constants.IMG_RES, faces=smpl.faces)\n",
    "\n",
    "\n",
    "    # Preprocess input image and generate predictions\n",
    "    img, norm_img = process_image(args.img, args.bbox, args.openpose, input_res=constants.IMG_RES)\n",
    "    with torch.no_grad():\n",
    "        pred_rotmat, pred_betas, pred_camera = model(norm_img.to(device))\n",
    "        #pred_output = smpl(betas=pred_betas, body_pose=pred_rotmat[:,1:], global_orient=pred_rotmat[:,0].unsqueeze(1), pose2rot=False)\n",
    "        #pred_output = smpl(betas=pred_betas, body_pose=zero_pose, global_orient=pred_rotmat[:,0].unsqueeze(1), pose2rot=False)\n",
    "        #pred_vertices = pred_output.vertices\n",
    "        \n",
    "    # Calculate camera parameters for rendering\n",
    "    camera_translation = torch.stack([pred_camera[:,1], pred_camera[:,2], 2*constants.FOCAL_LENGTH/(constants.IMG_RES * pred_camera[:,0] +1e-9)],dim=-1)\n",
    "    camera_translation = camera_translation[0].cpu().numpy()\n",
    "    pred_vertices = pred_vertices[0].cpu().numpy()\n",
    "    img = img.permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    \n",
    "    # Render parametric shape\n",
    "    img_shape = renderer(pred_vertices, camera_translation, img)\n",
    "    \n",
    "    # Render side views\n",
    "    aroundy = cv2.Rodrigues(np.array([0, np.radians(90.), 0]))[0]\n",
    "    center = pred_vertices.mean(axis=0)\n",
    "    rot_vertices = np.dot((pred_vertices - center), aroundy) + center\n",
    "    \n",
    "    # Render non-parametric shape\n",
    "    img_shape_side = renderer(rot_vertices, camera_translation, np.ones_like(img))\n",
    "\n",
    "    outfile = args.img.split('.')[0] if args.outfile is None else args.outfile\n",
    "\n",
    "    # Save reconstructions\n",
    "    cv2.imwrite(outfile + '_shape.png', 255 * img_shape[:,:,::-1])\n",
    "    cv2.imwrite(outfile + '_shape_side.png', 255 * img_shape_side[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0316,  0.6420,  1.0889,  1.4400, -0.2099,  0.1531, -0.1142,  0.3719,\n",
       "          0.0332, -0.0497]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendr\n",
    "from opendr.renderer import ColoredRenderer\n",
    "from opendr.lighting import LambertianPointLight\n",
    "from opendr.camera import ProjectPoints\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0449,  0.4941,  0.0896],\n",
       "         [ 0.0395,  0.4814,  0.0996],\n",
       "         [ 0.0500,  0.4761,  0.0910],\n",
       "         ...,\n",
       "         [-0.0748,  0.4281,  0.0046],\n",
       "         [-0.0754,  0.4292,  0.0067],\n",
       "         [-0.0778,  0.4276,  0.0092]]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_pose_model.vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fed0edadd58b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m480\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamera\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProjectPoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mrn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrustum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'near'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'far'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'width'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'height'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbgcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chumpy/ch.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mdefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparm_declarations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mdefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chumpy/ch.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chumpy/ch.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value, itr)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname_in_dterms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname_in_props\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dterms'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Make ourselves not the parent of the old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chumpy/ch.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm_order\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mdefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparm_declarations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "m = t_pose_model\n",
    "\n",
    "rn = ColoredRenderer()\n",
    "\n",
    "## Assign attributes to renderer\n",
    "w, h = (640, 480)\n",
    "\n",
    "rn.camera = ProjectPoints(v=m, rt=np.zeros(3), t=np.array([0, 0, 2.]), f=np.array([w,w])/2., c=np.array([w,h])/2., k=np.zeros(5))\n",
    "rn.frustum = {'near': 1., 'far': 10., 'width': w, 'height': h}\n",
    "rn.set(v=m, f=m.f, bgcolor=np.zeros(3))\n",
    "\n",
    "## Construct point light source\n",
    "rn.vc = LambertianPointLight(\n",
    "    f=t_pose_model.f,\n",
    "    v=rn.v,\n",
    "    num_verts=len(m),\n",
    "    light_pos=np.array([-1000,-1000,-2000]),\n",
    "    vc=np.ones_like(m)*.9,\n",
    "    light_color=np.array([1., 1., 1.]))\n",
    "\n",
    "\n",
    "\n",
    "## Since we are in Docker without access to X, it's better to save the images. This is easier with matplotlib than with openCV, because cv2.imwrite requires the image to be converted to a compatible form first.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(rn.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0316,  0.6420,  1.0889,  1.4400, -0.2099,  0.1531, -0.1142,  0.3719,\n",
       "          0.0332, -0.0497]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 3, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rotmat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 0.,  0.,  1.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [-1.,  0.,  0.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]],\n",
       "\n",
       "         [[ 1.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.],\n",
       "          [ 0.,  0.,  1.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_pose = torch.zeros(1,23,3,3,device='cuda')\n",
    "zero_pose[:] = torch.eye(3)\n",
    "zero_pose[0,13] = torch.tensor([[0,0,1],\n",
    "                   [0,1,0],\n",
    "                   [-1,0,0]]\n",
    "zero_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.9642, -0.1700,  0.2037],\n",
       "          [ 0.2270,  0.9261, -0.3014],\n",
       "          [-0.1374,  0.3369,  0.9315]],\n",
       "\n",
       "         [[ 0.9875,  0.1243,  0.0967],\n",
       "          [-0.1569,  0.7206,  0.6753],\n",
       "          [ 0.0142, -0.6821,  0.7311]],\n",
       "\n",
       "         [[ 0.9873,  0.0194, -0.1575],\n",
       "          [-0.0382,  0.9924, -0.1169],\n",
       "          [ 0.1541,  0.1214,  0.9806]],\n",
       "\n",
       "         [[ 0.9480, -0.0332, -0.3166],\n",
       "          [-0.1000,  0.9131, -0.3952],\n",
       "          [ 0.3022,  0.4063,  0.8623]],\n",
       "\n",
       "         [[ 0.9985, -0.0471, -0.0268],\n",
       "          [ 0.0416,  0.9832, -0.1778],\n",
       "          [ 0.0348,  0.1764,  0.9837]],\n",
       "\n",
       "         [[ 1.0000,  0.0058,  0.0049],\n",
       "          [-0.0055,  0.9977, -0.0675],\n",
       "          [-0.0053,  0.0674,  0.9977]],\n",
       "\n",
       "         [[ 0.9756,  0.1856,  0.1170],\n",
       "          [-0.1846,  0.9826, -0.0200],\n",
       "          [-0.1187, -0.0021,  0.9929]],\n",
       "\n",
       "         [[ 0.9586, -0.2435, -0.1473],\n",
       "          [ 0.2284,  0.9671, -0.1122],\n",
       "          [ 0.1698,  0.0740,  0.9827]],\n",
       "\n",
       "         [[ 0.9954,  0.0024, -0.0959],\n",
       "          [-0.0077,  0.9985, -0.0548],\n",
       "          [ 0.0956,  0.0553,  0.9939]],\n",
       "\n",
       "         [[ 0.9806, -0.1868,  0.0600],\n",
       "          [ 0.1658,  0.9526,  0.2552],\n",
       "          [-0.1048, -0.2403,  0.9650]],\n",
       "\n",
       "         [[ 0.9177,  0.3939,  0.0522],\n",
       "          [-0.3968,  0.9155,  0.0659],\n",
       "          [-0.0218, -0.0811,  0.9965]],\n",
       "\n",
       "         [[ 0.9994, -0.0019,  0.0338],\n",
       "          [ 0.0038,  0.9984, -0.0561],\n",
       "          [-0.0336,  0.0562,  0.9979]],\n",
       "\n",
       "         [[ 0.9111,  0.4098, -0.0445],\n",
       "          [-0.4122,  0.9044, -0.1100],\n",
       "          [-0.0048,  0.1186,  0.9929]],\n",
       "\n",
       "         [[ 0.9009, -0.4277, -0.0742],\n",
       "          [ 0.4213,  0.9027, -0.0878],\n",
       "          [ 0.1046,  0.0479,  0.9934]],\n",
       "\n",
       "         [[ 1.0000, -0.0055,  0.0041],\n",
       "          [ 0.0064,  0.9556, -0.2946],\n",
       "          [-0.0023,  0.2946,  0.9556]],\n",
       "\n",
       "         [[ 0.6423,  0.7020, -0.3076],\n",
       "          [-0.7519,  0.6550, -0.0751],\n",
       "          [ 0.1488,  0.2795,  0.9485]],\n",
       "\n",
       "         [[ 0.5923, -0.7641,  0.2557],\n",
       "          [ 0.8033,  0.5354, -0.2607],\n",
       "          [ 0.0623,  0.3598,  0.9309]],\n",
       "\n",
       "         [[ 0.6126, -0.1846, -0.7685],\n",
       "          [-0.0243,  0.9675, -0.2517],\n",
       "          [ 0.7900,  0.1729,  0.5882]],\n",
       "\n",
       "         [[ 0.4777,  0.2469,  0.8431],\n",
       "          [ 0.0893,  0.9411, -0.3261],\n",
       "          [-0.8740,  0.2311,  0.4275]],\n",
       "\n",
       "         [[ 0.9944,  0.0020, -0.1057],\n",
       "          [-0.0062,  0.9992, -0.0399],\n",
       "          [ 0.1055,  0.0403,  0.9936]],\n",
       "\n",
       "         [[ 0.9918, -0.0537,  0.1159],\n",
       "          [ 0.0505,  0.9983,  0.0301],\n",
       "          [-0.1173, -0.0240,  0.9928]],\n",
       "\n",
       "         [[ 0.9776,  0.2022, -0.0591],\n",
       "          [-0.1864,  0.9608,  0.2051],\n",
       "          [ 0.0982, -0.1895,  0.9770]],\n",
       "\n",
       "         [[ 0.9695, -0.2311,  0.0819],\n",
       "          [ 0.2163,  0.9635,  0.1576],\n",
       "          [-0.1153, -0.1351,  0.9841]]]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_output.body_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4695,  0.0377, -0.8821],\n",
       "         [ 0.0895, -0.9960,  0.0050],\n",
       "         [-0.8784, -0.0813, -0.4709]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rotmat[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rotmat[:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rotmat[:,1:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4695,  0.0377, -0.8821],\n",
       "          [ 0.0895, -0.9960,  0.0050],\n",
       "          [-0.8784, -0.0813, -0.4709]]]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rotmat[:,0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rotmat[:,0].unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
