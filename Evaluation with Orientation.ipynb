{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script can be used to evaluate a trained model on 3D pose/shape and masks/part segmentation. You first need to download the datasets and preprocess them.\n",
    "Example usage:\n",
    "```\n",
    "python3 eval.py --checkpoint=data/model_checkpoint.pt --dataset=h36m-p1 --log_freq=20\n",
    "```\n",
    "Running the above command will compute the MPJPE and Reconstruction Error on the Human3.6M dataset (Protocol I). The ```--dataset``` option can take different values based on the type of evaluation you want to perform:\n",
    "1. Human3.6M Protocol 1 ```--dataset=h36m-p1```\n",
    "2. Human3.6M Protocol 2 ```--dataset=h36m-p2```\n",
    "3. 3DPW ```--dataset=3dpw```\n",
    "4. LSP ```--dataset=lsp```\n",
    "5. MPI-INF-3DHP ```--dataset=mpi-inf-3dhp```\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import torchgeometry as tgm\n",
    "\n",
    "import config\n",
    "import constants\n",
    "from models import hmr, SMPL\n",
    "from datasets import BaseDataset\n",
    "from utils.imutils import uncrop\n",
    "from utils.pose_utils import reconstruction_error\n",
    "from utils.part_utils import PartRenderer\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# Define command-line arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--checkpoint', default=None, help='Path to network checkpoint')\n",
    "parser.add_argument('--dataset', default='h36m-p1', choices=['h36m-p1', 'h36m-p2', 'lsp', '3dpw', 'mpi-inf-3dhp'], help='Choose evaluation dataset')\n",
    "parser.add_argument('--log_freq', default=50, type=int, help='Frequency of printing intermediate results')\n",
    "parser.add_argument('--batch_size', default=32, help='Batch size for testing')\n",
    "parser.add_argument('--shuffle', default=False, action='store_true', help='Shuffle data')\n",
    "parser.add_argument('--num_workers', default=8, type=int, help='Number of processes for data loading')\n",
    "parser.add_argument('--result_file', default=None, help='If set, save detections to a .npz file')\n",
    "\n",
    "def run_evaluation(model, dataset_name, dataset, result_file,\n",
    "                   batch_size=32, img_res=224, \n",
    "                   num_workers=32, shuffle=False, log_freq=50):\n",
    "    \"\"\"Run evaluation on the datasets and metrics we report in the paper. \"\"\"\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # Transfer model to the GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Load SMPL model\n",
    "    smpl_neutral = SMPL(config.SMPL_MODEL_DIR,\n",
    "                        create_transl=False).to(device)\n",
    "    smpl_male = SMPL(config.SMPL_MODEL_DIR,\n",
    "                     gender='male',\n",
    "                     create_transl=False).to(device)\n",
    "    smpl_female = SMPL(config.SMPL_MODEL_DIR,\n",
    "                       gender='female',\n",
    "                       create_transl=False).to(device)\n",
    "    \n",
    "    renderer = PartRenderer()\n",
    "    \n",
    "    # Regressor for H36m joints\n",
    "    J_regressor = torch.from_numpy(np.load(config.JOINT_REGRESSOR_H36M)).float()\n",
    "    \n",
    "    save_results = result_file is not None\n",
    "    # Disable shuffling if you want to save the results\n",
    "    if save_results:\n",
    "        shuffle=False\n",
    "    # Create dataloader for the dataset\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    \n",
    "    # Pose metrics\n",
    "    # MPJPE and Reconstruction error for the non-parametric and parametric shapes\n",
    "    mpjpe = np.zeros(len(dataset))\n",
    "    recon_err = np.zeros(len(dataset))\n",
    "    mpjpe_smpl = np.zeros(len(dataset))\n",
    "    recon_err_smpl = np.zeros(len(dataset))\n",
    "    \n",
    "    # Including mean orientation error\n",
    "    moe = np.zeros(len(dataset))\n",
    "\n",
    "    # Shape metrics\n",
    "    # Mean per-vertex error\n",
    "    shape_err = np.zeros(len(dataset))\n",
    "    shape_err_smpl = np.zeros(len(dataset))\n",
    "\n",
    "    # Mask and part metrics\n",
    "    # Accuracy\n",
    "    accuracy = 0.\n",
    "    parts_accuracy = 0.\n",
    "    # True positive, false positive and false negative\n",
    "    tp = np.zeros((2,1))\n",
    "    fp = np.zeros((2,1))\n",
    "    fn = np.zeros((2,1))\n",
    "    parts_tp = np.zeros((7,1))\n",
    "    parts_fp = np.zeros((7,1))\n",
    "    parts_fn = np.zeros((7,1))\n",
    "    # Pixel count accumulators\n",
    "    pixel_count = 0\n",
    "    parts_pixel_count = 0\n",
    "\n",
    "    # Store SMPL parameters\n",
    "    smpl_pose = np.zeros((len(dataset), 72))\n",
    "    smpl_betas = np.zeros((len(dataset), 10))\n",
    "    smpl_camera = np.zeros((len(dataset), 3))\n",
    "    pred_joints = np.zeros((len(dataset), 17, 3))\n",
    "\n",
    "    eval_pose = False\n",
    "    eval_masks = False\n",
    "    eval_parts = False\n",
    "    eval_orientation = False # Adding the orientation parameter\n",
    "    # Choose appropriate evaluation for each dataset\n",
    "    if dataset_name == 'h36m-p1' or dataset_name == 'h36m-p2' or dataset_name == 'mpi-inf-3dhp':\n",
    "        eval_pose = True\n",
    "    elif dataset_name == 'lsp':\n",
    "        eval_masks = True\n",
    "        eval_parts = True\n",
    "        annot_path = config.DATASET_FOLDERS['upi-s1h']\n",
    "    elif dataset_name == '3dpw':\n",
    "        eval_orientation = True\n",
    "        eval_pose = True\n",
    "        \n",
    "\n",
    "    joint_mapper_h36m = constants.H36M_TO_J17 if dataset_name == 'mpi-inf-3dhp' else constants.H36M_TO_J14\n",
    "    joint_mapper_gt = constants.J24_TO_J17 if dataset_name == 'mpi-inf-3dhp' else constants.J24_TO_J14\n",
    "    # Iterate over the entire dataset\n",
    "    for step, batch in enumerate(tqdm(data_loader, desc='Eval', total=len(data_loader))):\n",
    "        # Get ground truth annotations from the batch\n",
    "        gt_pose = batch['pose'].to(device)\n",
    "        gt_betas = batch['betas'].to(device)\n",
    "        gt_vertices = smpl_neutral(betas=gt_betas, body_pose=gt_pose[:, 3:], global_orient=gt_pose[:, :3]).vertices\n",
    "        images = batch['img'].to(device)\n",
    "        gender = batch['gender'].to(device)\n",
    "        curr_batch_size = images.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_rotmat, pred_betas, pred_camera = model(images)\n",
    "            pred_output = smpl_neutral(betas=pred_betas, body_pose=pred_rotmat[:,1:], global_orient=pred_rotmat[:,0].unsqueeze(1), pose2rot=False)\n",
    "            pred_vertices = pred_output.vertices\n",
    "\n",
    "        if save_results:\n",
    "            rot_pad = torch.tensor([0,0,1], dtype=torch.float32, device=device).view(1,3,1)\n",
    "            rotmat = torch.cat((pred_rotmat.view(-1, 3, 3), rot_pad.expand(curr_batch_size * 24, -1, -1)), dim=-1)\n",
    "            pred_pose = tgm.rotation_matrix_to_angle_axis(rotmat).contiguous().view(-1, 72)\n",
    "            smpl_pose[step * batch_size:step * batch_size + curr_batch_size, :] = pred_pose.cpu().numpy()\n",
    "            smpl_betas[step * batch_size:step * batch_size + curr_batch_size, :]  = pred_betas.cpu().numpy()\n",
    "            smpl_camera[step * batch_size:step * batch_size + curr_batch_size, :]  = pred_camera.cpu().numpy()\n",
    "        \n",
    "        # Orientation evaluation\n",
    "        if eval_orientation:\n",
    "            # Get ground truth orientation (already stored in gt_pose)\n",
    "            gt_rotvec = torch.zeros(curr_batch_size,24,3) # Have to have an array of this shape to input into the rotation object (from 32,72 to 32,24,3)\n",
    "            i = 0\n",
    "            for row in gt_pose:\n",
    "                gt_rotvec[i] = torch.reshape(row,(24, -1))\n",
    "                i+=1\n",
    "\n",
    "            gt_euler = np.zeros((curr_batch_size, 24, 3)) # Using numpy here because it works with the rotation library\n",
    "            i = 0\n",
    "            for row in gt_rotvec:\n",
    "                r = R.from_rotvec(row)\n",
    "                gt_euler[i] = R.as_euler(r, 'xyz', degrees=True)\n",
    "                i+=1\n",
    "            \n",
    "            \n",
    "            # Get Euler representation of the predictions too:\n",
    "            \n",
    "            pred_euler = np.zeros((curr_batch_size,24,3)) # Has to be a numpy array because it works with Rotation\n",
    "\n",
    "            # For each row in pred_rotmat convert it to a Rotation object and write it into a corresponding\n",
    "            # row in pred_euler as Euler angles\n",
    "\n",
    "            i=0\n",
    "            for row in pred_rotmat:\n",
    "                r = R.from_dcm(row.cpu())\n",
    "                pred_euler[i] = R.as_euler(r, 'xyz', degrees=True)\n",
    "                i+=1\n",
    "                \n",
    "            orientation_error = torch.sqrt((torch.from_numpy(gt_euler[:,1:,:]).to(device) -\n",
    "                                            torch.from_numpy(pred_euler[:,1:,:]).to(device))**2).sum(dim=-1).mean(dim=-1)\n",
    "            print(\"orientation_error\")\n",
    "            print(orientation_error)\n",
    "            print()\n",
    "            \n",
    "            moe[step * batch_size:step * batch_size + curr_batch_size] = orientation_error.cpu()\n",
    "            \n",
    "        # 3D pose evaluation\n",
    "        if eval_pose:\n",
    "            # Regressor broadcasting\n",
    "            J_regressor_batch = J_regressor[None, :].expand(pred_vertices.shape[0], -1, -1).to(device)\n",
    "            # Get 14 ground truth joints\n",
    "            if 'h36m' in dataset_name or 'mpi-inf' in dataset_name:\n",
    "                gt_keypoints_3d = batch['pose_3d'].cuda()\n",
    "                gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_gt, :-1]\n",
    "            # For 3DPW get the 14 common joints from the rendered shape\n",
    "            else:\n",
    "                gt_vertices = smpl_male(global_orient=gt_pose[:,:3], body_pose=gt_pose[:,3:], betas=gt_betas).vertices \n",
    "                gt_vertices_female = smpl_female(global_orient=gt_pose[:,:3], body_pose=gt_pose[:,3:], betas=gt_betas).vertices \n",
    "                gt_vertices[gender==1, :, :] = gt_vertices_female[gender==1, :, :]\n",
    "                gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices)\n",
    "                gt_pelvis = gt_keypoints_3d[:, [0],:].clone()\n",
    "                gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :]\n",
    "                gt_keypoints_3d = gt_keypoints_3d - gt_pelvis \n",
    "\n",
    "\n",
    "            # Get 14 predicted joints from the mesh\n",
    "            pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices)\n",
    "            if save_results:\n",
    "                pred_joints[step * batch_size:step * batch_size + curr_batch_size, :, :]  = pred_keypoints_3d.cpu().numpy()\n",
    "            pred_pelvis = pred_keypoints_3d[:, [0],:].clone()\n",
    "            pred_keypoints_3d = pred_keypoints_3d[:, joint_mapper_h36m, :]\n",
    "            pred_keypoints_3d = pred_keypoints_3d - pred_pelvis \n",
    "\n",
    "            # Absolute error (MPJPE)\n",
    "            error = torch.sqrt(((pred_keypoints_3d - gt_keypoints_3d) ** 2).sum(dim=-1)).mean(dim=-1).cpu().numpy()\n",
    "            mpjpe[step * batch_size:step * batch_size + curr_batch_size] = error\n",
    "\n",
    "            # Reconstuction_error\n",
    "            r_error = reconstruction_error(pred_keypoints_3d.cpu().numpy(), gt_keypoints_3d.cpu().numpy(), reduction=None)\n",
    "            recon_err[step * batch_size:step * batch_size + curr_batch_size] = r_error\n",
    "\n",
    "\n",
    "        # If mask or part evaluation, render the mask and part images\n",
    "        if eval_masks or eval_parts:\n",
    "            mask, parts = renderer(pred_vertices, pred_camera)\n",
    "\n",
    "        # Mask evaluation (for LSP)\n",
    "        if eval_masks:\n",
    "            center = batch['center'].cpu().numpy()\n",
    "            scale = batch['scale'].cpu().numpy()\n",
    "            # Dimensions of original image\n",
    "            orig_shape = batch['orig_shape'].cpu().numpy()\n",
    "            for i in range(curr_batch_size):\n",
    "                # After rendering, convert imate back to original resolution\n",
    "                pred_mask = uncrop(mask[i].cpu().numpy(), center[i], scale[i], orig_shape[i]) > 0\n",
    "                # Load gt mask\n",
    "                gt_mask = cv2.imread(os.path.join(annot_path, batch['maskname'][i]), 0) > 0\n",
    "                # Evaluation consistent with the original UP-3D code\n",
    "                accuracy += (gt_mask == pred_mask).sum()\n",
    "                pixel_count += np.prod(np.array(gt_mask.shape))\n",
    "                for c in range(2):\n",
    "                    cgt = gt_mask == c\n",
    "                    cpred = pred_mask == c\n",
    "                    tp[c] += (cgt & cpred).sum()\n",
    "                    fp[c] +=  (~cgt & cpred).sum()\n",
    "                    fn[c] +=  (cgt & ~cpred).sum()\n",
    "                f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "        # Part evaluation (for LSP)\n",
    "        if eval_parts:\n",
    "            center = batch['center'].cpu().numpy()\n",
    "            scale = batch['scale'].cpu().numpy()\n",
    "            orig_shape = batch['orig_shape'].cpu().numpy()\n",
    "            for i in range(curr_batch_size):\n",
    "                pred_parts = uncrop(parts[i].cpu().numpy().astype(np.uint8), center[i], scale[i], orig_shape[i])\n",
    "                # Load gt part segmentation\n",
    "                gt_parts = cv2.imread(os.path.join(annot_path, batch['partname'][i]), 0)\n",
    "                # Evaluation consistent with the original UP-3D code\n",
    "                # 6 parts + background\n",
    "                for c in range(7):\n",
    "                   cgt = gt_parts == c\n",
    "                   cpred = pred_parts == c\n",
    "                   cpred[gt_parts == 255] = 0\n",
    "                   parts_tp[c] += (cgt & cpred).sum()\n",
    "                   parts_fp[c] +=  (~cgt & cpred).sum()\n",
    "                   parts_fn[c] +=  (cgt & ~cpred).sum()\n",
    "                gt_parts[gt_parts == 255] = 0\n",
    "                pred_parts[pred_parts == 255] = 0\n",
    "                parts_f1 = 2 * parts_tp / (2 * parts_tp + parts_fp + parts_fn)\n",
    "                parts_accuracy += (gt_parts == pred_parts).sum()\n",
    "                parts_pixel_count += np.prod(np.array(gt_parts.shape))\n",
    "\n",
    "        # Print intermediate results during evaluation\n",
    "        if step % log_freq == log_freq - 1:\n",
    "            if eval_pose:\n",
    "                print('MPJPE: ' + str(1000 * mpjpe[:step * batch_size].mean()))\n",
    "                print('Reconstruction Error: ' + str(1000 * recon_err[:step * batch_size].mean()))\n",
    "                print()\n",
    "            if eval_masks:\n",
    "                print('Accuracy: ', accuracy / pixel_count)\n",
    "                print('F1: ', f1.mean())\n",
    "                print()\n",
    "            if eval_parts:\n",
    "                print('Parts Accuracy: ', parts_accuracy / parts_pixel_count)\n",
    "                print('Parts F1 (BG): ', parts_f1[[0,1,2,3,4,5,6]].mean())\n",
    "                print()\n",
    "            if eval_orientation:\n",
    "                print('Orientation error: ' + str(moe[:step * batch_size].mean()))\n",
    "\n",
    "    # Save reconstructions to a file for further processing\n",
    "    if save_results:\n",
    "        np.savez(result_file, pred_joints=pred_joints, pose=smpl_pose, betas=smpl_betas, camera=smpl_camera)\n",
    "    # Print final results during evaluation\n",
    "    print('*** Final Results ***')\n",
    "    print()\n",
    "    if eval_pose:\n",
    "        print('MPJPE: ' + str(1000 * mpjpe.mean()))\n",
    "        print('Reconstruction Error: ' + str(1000 * recon_err.mean()))\n",
    "        print()\n",
    "    if eval_masks:\n",
    "        print('Accuracy: ', accuracy / pixel_count)\n",
    "        print('F1: ', f1.mean())\n",
    "        print()\n",
    "    if eval_parts:\n",
    "        print('Parts Accuracy: ', parts_accuracy / parts_pixel_count)\n",
    "        print('Parts F1 (BG): ', parts_f1[[0,1,2,3,4,5,6]].mean())\n",
    "        print()\n",
    "    if eval_orientation:\n",
    "        print('Orientation Error: ' + str(moe.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # python3 eval.py --checkpoint=data/model_checkpoint.pt --dataset=h36m-p1 --log_freq=20 // example code\n",
    "    args = parser.parse_args(['--checkpoint=data/model_checkpoint.pt','--dataset=3dpw', '--log_freq=20'])\n",
    "    # Here we inserted our own arguments list\n",
    "    \n",
    "    model = hmr(config.SMPL_MEAN_PARAMS)\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    # Setup evaluation dataset\n",
    "    dataset = BaseDataset(None, args.dataset, is_train=False)\n",
    "    # Run evaluation\n",
    "    run_evaluation(model, args.dataset, dataset, args.result_file,\n",
    "                   batch_size=args.batch_size,\n",
    "                   shuffle=args.shuffle,\n",
    "                   log_freq=args.log_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first sanity check should be whether ground truth compared to ground truth yields zero error\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "img_path = 'examples/image_00502_crop.jpg'\n",
    "\n",
    "pickle_path = 'data/3dpw/sequenceFiles/validation/courtyard_basketball_01.pkl'\n",
    "\n",
    "frame = 502\n",
    "\n",
    "# Load the .pkl sequence file containing the ground-truth information from 3dpw\n",
    "\n",
    "seq = pkl.load(open(pickle_path,'rb'),encoding='latin-1') # opening the sequence file, latin-1 encoding for making it compatible with python3\n",
    "\n",
    "gt_pose = torch.tensor(seq['poses'][0][frame]).unsqueeze(0)\n",
    "\n",
    "# print(gt_pose)\n",
    "\n",
    "pred_rotmat = torch.zeros(1, 24, 3, 3)\n",
    "\n",
    "q = R.from_rotvec(gt_pose.reshape(1,24,-1)[0])\n",
    "\n",
    "pred_rotmat = torch.tensor(R.as_dcm(q)).unsqueeze(0)\n",
    "\n",
    "# print(pred_rotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "curr_batch_size = 1\n",
    "\n",
    "print(\"gt_pose\", gt_pose.shape, gt_pose)\n",
    "\n",
    "print(\"pred_rotmat\", pred_rotmat.shape, pred_rotmat)\n",
    "\n",
    "# Get ground truth orientation (already stored in gt_pose)\n",
    "gt_rotvec = torch.zeros(curr_batch_size,24,3) # Have to have an array of this shape to input into the rotation object (from 32,72 to 32,24,3)\n",
    "i = 0\n",
    "for row in gt_pose:\n",
    "    gt_rotvec[i] = torch.reshape(row,(24, -1))\n",
    "    i+=1\n",
    "print(\"gt_rotvec\", gt_rotvec.shape, gt_rotvec)\n",
    "    \n",
    "gt_euler = np.zeros((curr_batch_size, 24, 3)) # Using numpy here because it works with the rotation library\n",
    "i = 0\n",
    "for row in gt_rotvec:\n",
    "    r = R.from_rotvec(row)\n",
    "    gt_euler[i] = R.as_euler(r, 'xyz', degrees=True)\n",
    "    i+=1\n",
    "\n",
    "print(\"gt_euler\", gt_euler.shape, gt_euler)\n",
    "\n",
    "# Get Euler representation of the predictions too:\n",
    "\n",
    "pred_euler = np.zeros((curr_batch_size,24,3)) # Has to be a numpy array because it works with Rotation\n",
    "\n",
    "# For each row in pred_rotmat convert it to a Rotation object and write it into a corresponding\n",
    "# row in pred_euler as Euler angles\n",
    "\n",
    "i=0\n",
    "for row in pred_rotmat:\n",
    "    r = R.from_dcm(row.cpu())\n",
    "    pred_euler[i] = R.as_euler(r, 'xyz', degrees=True)\n",
    "    i+=1\n",
    "\n",
    "print(\"pred_euler\", pred_euler.shape, pred_euler)\n",
    "    \n",
    "    \n",
    "orientation_error_non_reduced = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                torch.from_numpy(pred_euler).to(device))**2)\n",
    "print(orientation_error_non_reduced)\n",
    "    \n",
    "orientation_error = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                torch.from_numpy(pred_euler).to(device))**2).sum(dim=-1).mean(dim=-1)\n",
    "# The reduction above is wrong. For a 90 degree error in one angle, it averages out 3.75 degrees, which\n",
    "# is 90/24. The correct reduction would be a mean of 1.25 (90/72), because there are 72 angles (3 for each part)\n",
    "# To remove the root, add [:,1:,:] to gt_euler and pred_euler above\n",
    "\n",
    "orientation_error_new = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                torch.from_numpy(pred_euler).to(device))**2).mean()\n",
    "# This reduction is more accurate because it averages the error per part and then the error across parts\n",
    "# It is equivalent to .mean(dim=-1).mean(dim=-1)\n",
    "\n",
    "print(\"orientation_error\")\n",
    "print(orientation_error)\n",
    "print()\n",
    "print(\"orientation_error_new\")\n",
    "print(orientation_error_new)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_orientation(gt, pred):\n",
    "    # Taking as input two axis_angle representations\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    curr_batch_size = 1\n",
    "\n",
    "    gt_pose = gt\n",
    "\n",
    "    print(\"gt_pose\", gt_pose.shape, gt_pose)\n",
    "\n",
    "    q = R.from_rotvec(pred.reshape(1,24,-1)[0])\n",
    "\n",
    "    pred_rotmat = torch.tensor(R.as_dcm(q)).unsqueeze(0)\n",
    "\n",
    "    print(\"pred_rotmat\", pred_rotmat.shape, pred_rotmat)\n",
    "\n",
    "    # Get ground truth orientation (already stored in gt_pose)\n",
    "    gt_rotvec = torch.zeros(curr_batch_size,24,3) # Have to have an array of this shape to input into the rotation object (from 32,72 to 32,24,3)\n",
    "    i = 0\n",
    "    for row in gt_pose:\n",
    "        gt_rotvec[i] = torch.reshape(row,(24, -1))\n",
    "        i+=1\n",
    "    print(\"gt_rotvec\", gt_rotvec.shape, gt_rotvec)\n",
    "\n",
    "    gt_euler = np.zeros((curr_batch_size, 24, 3)) # Using numpy here because it works with the rotation library\n",
    "    i = 0\n",
    "    for row in gt_rotvec:\n",
    "        r = R.from_rotvec(row)\n",
    "        gt_euler[i] = R.as_euler(r, 'xyz', degrees=True)\n",
    "        i+=1\n",
    "\n",
    "    print(\"gt_euler\", gt_euler.shape, gt_euler)\n",
    "\n",
    "    # Get Euler representation of the predictions too:\n",
    "\n",
    "    pred_euler = np.zeros((curr_batch_size,24,3)) # Has to be a numpy array because it works with Rotation\n",
    "\n",
    "    # For each row in pred_rotmat convert it to a Rotation object and write it into a corresponding\n",
    "    # row in pred_euler as Euler angles\n",
    "\n",
    "    i=0\n",
    "    for row in pred_rotmat:\n",
    "        r = R.from_dcm(row.cpu())\n",
    "        pred_euler[i] = R.as_euler(r, 'xyz', degrees=True)\n",
    "        i+=1\n",
    "\n",
    "    print(\"pred_euler\", pred_euler.shape, pred_euler)\n",
    "\n",
    "\n",
    "    orientation_error_non_reduced = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                    torch.from_numpy(pred_euler).to(device))**2)\n",
    "    print(\"error per part\", orientation_error_non_reduced)\n",
    "\n",
    "    orientation_error = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                    torch.from_numpy(pred_euler).to(device))**2).sum(dim=-1).mean(dim=-1)\n",
    "    # The reduction above is wrong. For a 90 degree error in one angle, it averages out 3.75 degrees, which\n",
    "    # is 90/24. The correct reduction would be a mean of 1.25 (90/72), because there are 72 angles (3 for each part)\n",
    "    # To remove the root, add [:,1:,:] to gt_euler and pred_euler above\n",
    "\n",
    "    orientation_error_new = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                    torch.from_numpy(pred_euler).to(device))**2).mean()\n",
    "    # This reduction is more accurate because it averages the error per part and then the error across parts\n",
    "    # It is equivalent to .mean(dim=-1).mean(dim=-1)\n",
    "\n",
    "    print(\"orientation_error\")\n",
    "    print(orientation_error.item())\n",
    "    print()\n",
    "    print(\"orientation_error_new\")\n",
    "    print(orientation_error_new.item())\n",
    "    print()\n",
    "\n",
    "    #moe[step * batch_size:step * batch_size + curr_batch_size] = orientation_error.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def compare_orientation_rotvec(gt, pred):\n",
    "    # Taking as input two axis_angle representations\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    curr_batch_size = 1\n",
    "\n",
    "    gt_pose = gt\n",
    "\n",
    "    print(\"gt_pose\", gt_pose.shape, gt_pose)\n",
    "    print(\"pred_pose\", pred.shape, pred)\n",
    "\n",
    "    # Get ground truth orientation (already stored in gt_pose)\n",
    "    gt_rotvec = torch.zeros((curr_batch_size,24,3), dtype=torch.double) # Have to have an array of this shape to input into the rotation object (from 32,72 to 32,24,3)\n",
    "    i = 0\n",
    "    for row in gt_pose:\n",
    "        gt_rotvec[i] = torch.reshape(row,(24, -1))\n",
    "        i+=1\n",
    "    print(\"gt_rotvec\", gt_rotvec.shape, gt_rotvec)\n",
    "    \n",
    "    # Get prediction as rotation vectors\n",
    "    \n",
    "    r = R.from_rotvec(pred.reshape(1,24,-1)[0])\n",
    "    pred_rotvec = torch.tensor(R.as_rotvec(r)).unsqueeze(0)\n",
    "\n",
    "    print(\"pred_rotvec\", pred_rotvec.shape, pred_rotvec)\n",
    "\n",
    "    orientation_error_non_reduced = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2))\n",
    "    \n",
    "    print(\"error per part\", orientation_error_non_reduced)\n",
    "\n",
    "    orientation_error = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2).sum(dim=-1).mean(dim=-1))\n",
    "    # The reduction above is wrong. For a 90 degree error in one angle, it averages out 3.75 degrees, which\n",
    "    # is 90/24. The correct reduction would be a mean of 1.25 (90/72), because there are 72 angles (3 for each part)\n",
    "    # To remove the root, add [:,1:,:] to gt_euler and pred_euler above\n",
    "\n",
    "    orientation_error_new = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2).mean())\n",
    "    # This reduction is more accurate because it averages the error per part and then the error across parts\n",
    "    # It is equivalent to .mean(dim=-1).mean(dim=-1)\n",
    "\n",
    "    print(\"orientation_error\")\n",
    "    print(orientation_error.item())\n",
    "    print()\n",
    "    print(\"orientation_error_new\")\n",
    "    print(orientation_error_new.item())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_pose torch.Size([1, 72]) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "pred_pose torch.Size([1, 72]) tensor([[0.7854, 0.7854, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "gt_rotvec torch.Size([1, 24, 3]) tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]], dtype=torch.float64)\n",
      "pred_rotvec torch.Size([1, 24, 3]) tensor([[[0.7854, 0.7854, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]]], dtype=torch.float64)\n",
      "error per part tensor([[[45.0000, 45.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], dtype=torch.float64)\n",
      "orientation_error\n",
      "3.750000104353257\n",
      "\n",
      "orientation_error_new\n",
      "1.250000034784419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(1,72)\n",
    "b = torch.zeros(1,72)\n",
    "\n",
    "b[0][0] = np.pi/4\n",
    "b[0][1] = np.pi/4\n",
    "\n",
    "compare_orientation_rotvec(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [[(1/np.sqrt(2)),0,(1/np.sqrt(2))],\n",
    "    [1/2, (1/np.sqrt(2)), -1/2],\n",
    "    [-1/2, (1/np.sqrt(2)), 1/2]]\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a rotation of 45 degrees around x multiplied on the\n",
    "# left by a rotation of 45 degrees around y\n",
    "\n",
    "m = [[(1/np.sqrt(2)),1/2,1/2],\n",
    "    [0, (1/np.sqrt(2)), -(1/np.sqrt(2))],\n",
    "    [-(1/np.sqrt(2)), 1/2, 1/2]]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = R.from_dcm(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R.as_euler(n, 'xyz', degrees=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = R.from_euler('xyz', [45, 45, 0], degrees=True)\n",
    "\n",
    "print(o.as_dcm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
