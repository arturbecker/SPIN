{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation_evaluation(gt_pose, pred_rotmat, batch_size, curr_batch_size, step):\n",
    "\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from scipy.spatial.transform import Rotation as R\n",
    "    \n",
    "    # Orientation evaluation\n",
    "    # Taking as input gt_pose in axis-angle representation and pred_rotmat in rotation matrix representation\n",
    "\n",
    "    gt_rotvec = torch.zeros((curr_batch_size,24,3), dtype=torch.double) # Reshaping the axis-angle (curr_batch_size, 72) to (curr_batch_size, 24, 3) for rotation vector compatibility\n",
    "\n",
    "    for i, row in enumerate(gt_pose):\n",
    "        gt_rotvec[i] = torch.reshape(row,(24, -1))\n",
    "\n",
    "    # Get prediction as rotation vectors\n",
    "\n",
    "    pred_rotvec_arr = np.zeros((curr_batch_size,24,3)) # Has to be a numpy array because it works with Rotation\n",
    "\n",
    "    for i, row in enumerate(pred_rotmat):\n",
    "        r = R.from_dcm(row.cpu()) # create the rotation object from the rotation matrix\n",
    "        pred_rotvec_arr[i] = R.as_rotvec(r) # write it as rotation vectors in pred_rotvec_arr\n",
    "\n",
    "    pred_rotvec = torch.from_numpy(pred_rotvec_arr) # transform it to a tensor\n",
    "\n",
    "    #print(\"pred_rotvec\", pred_rotvec.shape, pred_rotvec)\n",
    "\n",
    "    orientation_error_per_part = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2))\n",
    "    # This gives the error per angle of each body part\n",
    "\n",
    "    orientation_error = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2).mean(dim=[1,2]))\n",
    "    # This reduction is more accurate because it averages the error per part and then the error across parts\n",
    "    # It is equivalent to .mean(dim=-1).mean(dim=-1)\n",
    "\n",
    "    return orientation_error_per_part, orientation_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cell below is a copy from the pure .py file, except for args\n",
    "\n",
    "Ignoring orientation evaluation for now\n",
    "\n",
    "See the __main__ section for how arguments are inserted into the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   2%|▏         | 20/1110 [00:52<15:58,  1.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 81.37874403282215\n",
      "Reconstruction Error: 41.73969471900675\n",
      "\n",
      "Orientation error: 8.301217391926139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   4%|▎         | 40/1110 [01:02<13:37,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 93.01356022031261\n",
      "Reconstruction Error: 45.72069009867473\n",
      "\n",
      "Orientation error: 8.917448099172002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   5%|▌         | 60/1110 [01:25<08:26,  2.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 101.09796959480619\n",
      "Reconstruction Error: 51.432238564179364\n",
      "\n",
      "Orientation error: 9.283912416846842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   7%|▋         | 80/1110 [01:39<32:11,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 98.7467867649408\n",
      "Reconstruction Error: 52.1927023537846\n",
      "\n",
      "Orientation error: 9.357562113788697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   9%|▉         | 100/1110 [01:52<06:46,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 98.17523845574921\n",
      "Reconstruction Error: 52.06985538002254\n",
      "\n",
      "Orientation error: 9.492716857255799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  11%|█         | 120/1110 [02:09<10:07,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.07387304662124\n",
      "Reconstruction Error: 52.26824514395675\n",
      "\n",
      "Orientation error: 9.43975521834012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  13%|█▎        | 140/1110 [02:29<09:10,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.53847144434911\n",
      "Reconstruction Error: 54.08132505995094\n",
      "\n",
      "Orientation error: 9.605082343641998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  14%|█▍        | 160/1110 [03:00<19:55,  1.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 98.54347701696972\n",
      "Reconstruction Error: 56.634753527348195\n",
      "\n",
      "Orientation error: 9.98932664327239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  16%|█▌        | 180/1110 [03:08<07:45,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 99.73491341728905\n",
      "Reconstruction Error: 58.15416857451929\n",
      "\n",
      "Orientation error: 10.074377547260031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  18%|█▊        | 200/1110 [03:24<07:59,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 99.64782742832567\n",
      "Reconstruction Error: 60.30048787694462\n",
      "\n",
      "Orientation error: 10.417564531309836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  20%|█▉        | 220/1110 [03:35<08:05,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 101.72668339038209\n",
      "Reconstruction Error: 60.335253251846204\n",
      "\n",
      "Orientation error: 10.298260807738503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  22%|██▏       | 240/1110 [03:51<35:16,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 101.2261059012451\n",
      "Reconstruction Error: 59.34126633336324\n",
      "\n",
      "Orientation error: 10.098604911945815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  23%|██▎       | 260/1110 [03:58<05:48,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 101.41501063166338\n",
      "Reconstruction Error: 59.57728412031991\n",
      "\n",
      "Orientation error: 9.885730506637461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  25%|██▌       | 280/1110 [04:08<05:26,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 102.28858076288002\n",
      "Reconstruction Error: 59.867952756899136\n",
      "\n",
      "Orientation error: 9.770727506474236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  27%|██▋       | 300/1110 [04:16<06:35,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 101.2368943148221\n",
      "Reconstruction Error: 59.40012613301287\n",
      "\n",
      "Orientation error: 9.699317643240029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  29%|██▉       | 320/1110 [04:30<06:36,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 100.787596164715\n",
      "Reconstruction Error: 58.77969346847093\n",
      "\n",
      "Orientation error: 9.638661989302124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  31%|███       | 340/1110 [04:41<07:20,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 99.71019655986686\n",
      "Reconstruction Error: 58.42986403965147\n",
      "\n",
      "Orientation error: 9.644130566835804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  32%|███▏      | 360/1110 [04:50<07:26,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 99.22611367989923\n",
      "Reconstruction Error: 58.74572166184633\n",
      "\n",
      "Orientation error: 9.702722487781003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  34%|███▍      | 380/1110 [04:58<04:47,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 97.52136656196203\n",
      "Reconstruction Error: 58.06407751699806\n",
      "\n",
      "Orientation error: 9.666777709845965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  36%|███▌      | 400/1110 [05:08<06:05,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.97976527603711\n",
      "Reconstruction Error: 58.0197731418101\n",
      "\n",
      "Orientation error: 9.65897574341051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  38%|███▊      | 420/1110 [05:17<05:01,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 97.04290869254574\n",
      "Reconstruction Error: 58.60944849390065\n",
      "\n",
      "Orientation error: 9.630202759782692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  40%|███▉      | 440/1110 [05:28<06:24,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 97.13350864210015\n",
      "Reconstruction Error: 58.6497318643996\n",
      "\n",
      "Orientation error: 9.65047314931424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  41%|████▏     | 460/1110 [05:38<05:25,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.64615257318519\n",
      "Reconstruction Error: 58.50512616200401\n",
      "\n",
      "Orientation error: 9.62443830232045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  43%|████▎     | 480/1110 [05:52<05:47,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.80476736015055\n",
      "Reconstruction Error: 58.182424580700705\n",
      "\n",
      "Orientation error: 9.600506167123314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  45%|████▌     | 500/1110 [06:02<05:03,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.07492126911723\n",
      "Reconstruction Error: 58.44852769103943\n",
      "\n",
      "Orientation error: 9.630988569460293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  47%|████▋     | 520/1110 [06:16<06:21,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.764759606444\n",
      "Reconstruction Error: 58.31344841397592\n",
      "\n",
      "Orientation error: 9.604472781011264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  49%|████▊     | 540/1110 [06:26<05:18,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.77269418519734\n",
      "Reconstruction Error: 58.25498443650227\n",
      "\n",
      "Orientation error: 9.580903662937596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  50%|█████     | 560/1110 [06:39<04:52,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.86103236683675\n",
      "Reconstruction Error: 58.5607300873401\n",
      "\n",
      "Orientation error: 9.60804681764662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  52%|█████▏    | 580/1110 [06:50<03:29,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 97.26141928825834\n",
      "Reconstruction Error: 58.600691197615355\n",
      "\n",
      "Orientation error: 9.62137952931423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  54%|█████▍    | 601/1110 [07:09<07:49,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 98.29730211023649\n",
      "Reconstruction Error: 58.67116694960149\n",
      "\n",
      "Orientation error: 9.660806977401156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  56%|█████▌    | 620/1110 [07:15<03:34,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 98.23011851563722\n",
      "Reconstruction Error: 59.05669454760889\n",
      "\n",
      "Orientation error: 9.712031446028323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  58%|█████▊    | 640/1110 [07:27<02:25,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 97.67185774781554\n",
      "Reconstruction Error: 58.979304686096874\n",
      "\n",
      "Orientation error: 9.688309270678893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  59%|█████▉    | 660/1110 [07:36<04:32,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 97.18698598820735\n",
      "Reconstruction Error: 58.87917421603779\n",
      "\n",
      "Orientation error: 9.706401420119855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  61%|██████▏   | 680/1110 [07:43<03:29,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.99483215800296\n",
      "Reconstruction Error: 58.78070092597834\n",
      "\n",
      "Orientation error: 9.719306826425552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  63%|██████▎   | 700/1110 [07:52<02:19,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.77892331344053\n",
      "Reconstruction Error: 58.85737517482877\n",
      "\n",
      "Orientation error: 9.692356570138148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  65%|██████▍   | 720/1110 [07:59<03:08,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.41934391015607\n",
      "Reconstruction Error: 59.00512574593859\n",
      "\n",
      "Orientation error: 9.6439248069934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  67%|██████▋   | 740/1110 [08:11<03:01,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.87445049866447\n",
      "Reconstruction Error: 58.89322934042113\n",
      "\n",
      "Orientation error: 9.620443120744646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  68%|██████▊   | 760/1110 [08:23<04:10,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.28066178641745\n",
      "Reconstruction Error: 58.57721777647864\n",
      "\n",
      "Orientation error: 9.588335140923272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  70%|███████   | 780/1110 [08:47<03:33,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.32241956027468\n",
      "Reconstruction Error: 58.396075461424445\n",
      "\n",
      "Orientation error: 9.550964598636195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  72%|███████▏  | 800/1110 [09:02<05:12,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.03338752057724\n",
      "Reconstruction Error: 57.998542555867786\n",
      "\n",
      "Orientation error: 9.503799512276396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  74%|███████▍  | 820/1110 [09:14<01:28,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 94.77129218207504\n",
      "Reconstruction Error: 57.795886928912125\n",
      "\n",
      "Orientation error: 9.477754139324057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  76%|███████▌  | 840/1110 [09:28<03:47,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 94.97227546023827\n",
      "Reconstruction Error: 57.86474806755277\n",
      "\n",
      "Orientation error: 9.458699660016924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  77%|███████▋  | 860/1110 [09:36<01:35,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 94.6700266467942\n",
      "Reconstruction Error: 57.93397202313964\n",
      "\n",
      "Orientation error: 9.442182616950156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  79%|███████▉  | 880/1110 [09:46<01:25,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.16668808374996\n",
      "Reconstruction Error: 57.82050719398336\n",
      "\n",
      "Orientation error: 9.47823908191114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  81%|████████  | 900/1110 [10:02<08:35,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.60778589541863\n",
      "Reconstruction Error: 58.482298644847994\n",
      "\n",
      "Orientation error: 9.488770803599863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  83%|████████▎ | 920/1110 [10:11<01:20,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.92476836754054\n",
      "Reconstruction Error: 58.696438816692215\n",
      "\n",
      "Orientation error: 9.525968189919906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  85%|████████▍ | 940/1110 [10:24<01:17,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.42709904449275\n",
      "Reconstruction Error: 59.011896208689386\n",
      "\n",
      "Orientation error: 9.56078473060671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  86%|████████▋ | 960/1110 [10:33<01:12,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.57400069709496\n",
      "Reconstruction Error: 59.12613980964941\n",
      "\n",
      "Orientation error: 9.60315614591921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  88%|████████▊ | 980/1110 [10:42<00:59,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.40454657848372\n",
      "Reconstruction Error: 59.10551876262062\n",
      "\n",
      "Orientation error: 9.593283433895452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  90%|█████████ | 1000/1110 [10:50<00:55,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.15514202476214\n",
      "Reconstruction Error: 58.99938160780346\n",
      "\n",
      "Orientation error: 9.5840790235374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  92%|█████████▏| 1020/1110 [10:58<00:35,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.85081390019505\n",
      "Reconstruction Error: 58.96727392843891\n",
      "\n",
      "Orientation error: 9.566197215521473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  94%|█████████▎| 1040/1110 [11:05<00:23,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 95.69092004460663\n",
      "Reconstruction Error: 58.8947771665218\n",
      "\n",
      "Orientation error: 9.54798952087937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  96%|█████████▌| 1061/1110 [11:13<00:10,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.1986018673559\n",
      "Reconstruction Error: 59.07717778272021\n",
      "\n",
      "Orientation error: 9.545936897417256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  97%|█████████▋| 1081/1110 [11:15<00:02,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.5447576704782\n",
      "Reconstruction Error: 59.155131101707966\n",
      "\n",
      "Orientation error: 9.5267607877964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  99%|█████████▉| 1101/1110 [11:17<00:00,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 96.87599752032011\n",
      "Reconstruction Error: 59.31143589761427\n",
      "\n",
      "Orientation error: 9.518863738219308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 1110/1110 [11:18<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Final Results ***\n",
      "\n",
      "MPJPE: 96.94085740828478\n",
      "Reconstruction Error: 59.285746753744455\n",
      "\n",
      "Orientation Error: 9.5149063225102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script can be used to evaluate a trained model on 3D pose/shape and masks/part segmentation. You first need to download the datasets and preprocess them.\n",
    "Example usage:\n",
    "```\n",
    "python3 eval.py --checkpoint=data/model_checkpoint.pt --dataset=h36m-p1 --log_freq=20\n",
    "```\n",
    "Running the above command will compute the MPJPE and Reconstruction Error on the Human3.6M dataset (Protocol I). The ```--dataset``` option can take different values based on the type of evaluation you want to perform:\n",
    "1. Human3.6M Protocol 1 ```--dataset=h36m-p1```\n",
    "2. Human3.6M Protocol 2 ```--dataset=h36m-p2```\n",
    "3. 3DPW ```--dataset=3dpw```\n",
    "4. LSP ```--dataset=lsp```\n",
    "5. MPI-INF-3DHP ```--dataset=mpi-inf-3dhp```\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import torchgeometry as tgm\n",
    "\n",
    "import config\n",
    "import constants\n",
    "from models import hmr, SMPL\n",
    "from datasets import BaseDataset\n",
    "from utils.imutils import uncrop\n",
    "from utils.pose_utils import reconstruction_error\n",
    "from utils.part_utils import PartRenderer\n",
    "\n",
    "# Define command-line arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--checkpoint', default=None, help='Path to network checkpoint')\n",
    "parser.add_argument('--dataset', default='h36m-p1', choices=['h36m-p1', 'h36m-p2', 'lsp', '3dpw', 'mpi-inf-3dhp'], help='Choose evaluation dataset')\n",
    "parser.add_argument('--log_freq', default=50, type=int, help='Frequency of printing intermediate results')\n",
    "parser.add_argument('--batch_size', default=32, help='Batch size for testing')\n",
    "parser.add_argument('--shuffle', default=False, action='store_true', help='Shuffle data')\n",
    "parser.add_argument('--num_workers', default=8, type=int, help='Number of processes for data loading')\n",
    "parser.add_argument('--result_file', default=None, help='If set, save detections to a .npz file')\n",
    "\n",
    "def run_evaluation(model, dataset_name, dataset, result_file,\n",
    "                   batch_size=32, img_res=224, \n",
    "                   num_workers=32, shuffle=False, log_freq=50):\n",
    "    \"\"\"Run evaluation on the datasets and metrics we report in the paper. \"\"\"\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # Transfer model to the GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Load SMPL model\n",
    "    smpl_neutral = SMPL(config.SMPL_MODEL_DIR,\n",
    "                        create_transl=False).to(device)\n",
    "    smpl_male = SMPL(config.SMPL_MODEL_DIR,\n",
    "                     gender='male',\n",
    "                     create_transl=False).to(device)\n",
    "    smpl_female = SMPL(config.SMPL_MODEL_DIR,\n",
    "                       gender='female',\n",
    "                       create_transl=False).to(device)\n",
    "    \n",
    "    renderer = PartRenderer()\n",
    "    \n",
    "    # Regressor for H36m joints\n",
    "    J_regressor = torch.from_numpy(np.load(config.JOINT_REGRESSOR_H36M)).float()\n",
    "    \n",
    "    save_results = result_file is not None\n",
    "    # Disable shuffling if you want to save the results\n",
    "    if save_results:\n",
    "        shuffle=False\n",
    "    # Create dataloader for the dataset\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    \n",
    "    \"\"\"\n",
    "    Initializing the new error tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Per joint position error\n",
    "    pjpe = torch.zeros(len(dataset), 14)\n",
    "    \n",
    "    # Initializing the tensors Per joint angular error (mean and per part)\n",
    "    mpjae = np.zeros(len(dataset))\n",
    "    pjae = torch.zeros(len(dataset), 24, 3)\n",
    "    \n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    \n",
    "    # Pose metrics\n",
    "    # MPJPE and Reconstruction error for the non-parametric and parametric shapes\n",
    "    mpjpe = np.zeros(len(dataset))\n",
    "    recon_err = np.zeros(len(dataset))\n",
    "    mpjpe_smpl = np.zeros(len(dataset))\n",
    "    recon_err_smpl = np.zeros(len(dataset))\n",
    "\n",
    "    # Shape metrics\n",
    "    # Mean per-vertex error\n",
    "    shape_err = np.zeros(len(dataset))\n",
    "    shape_err_smpl = np.zeros(len(dataset))\n",
    "\n",
    "    # Mask and part metrics\n",
    "    # Accuracy\n",
    "    accuracy = 0.\n",
    "    parts_accuracy = 0.\n",
    "    # True positive, false positive and false negative\n",
    "    tp = np.zeros((2,1))\n",
    "    fp = np.zeros((2,1))\n",
    "    fn = np.zeros((2,1))\n",
    "    parts_tp = np.zeros((7,1))\n",
    "    parts_fp = np.zeros((7,1))\n",
    "    parts_fn = np.zeros((7,1))\n",
    "    # Pixel count accumulators\n",
    "    pixel_count = 0\n",
    "    parts_pixel_count = 0\n",
    "\n",
    "    # Store SMPL parameters\n",
    "    smpl_pose = np.zeros((len(dataset), 72))\n",
    "    smpl_betas = np.zeros((len(dataset), 10))\n",
    "    smpl_camera = np.zeros((len(dataset), 3))\n",
    "    pred_joints = np.zeros((len(dataset), 17, 3))\n",
    "\n",
    "    eval_pose = False\n",
    "    eval_masks = False\n",
    "    eval_parts = False\n",
    "    # Choose appropriate evaluation for each dataset\n",
    "    if dataset_name == 'h36m-p1' or dataset_name == 'h36m-p2' or dataset_name == '3dpw' or dataset_name == 'mpi-inf-3dhp':\n",
    "        eval_pose = True\n",
    "    elif dataset_name == 'lsp':\n",
    "        eval_masks = True\n",
    "        eval_parts = True\n",
    "        annot_path = config.DATASET_FOLDERS['upi-s1h']\n",
    "\n",
    "    joint_mapper_h36m = constants.H36M_TO_J17 if dataset_name == 'mpi-inf-3dhp' else constants.H36M_TO_J14\n",
    "    joint_mapper_gt = constants.J24_TO_J17 if dataset_name == 'mpi-inf-3dhp' else constants.J24_TO_J14\n",
    "    # Iterate over the entire dataset\n",
    "    for step, batch in enumerate(tqdm(data_loader, desc='Eval', total=len(data_loader))):\n",
    "        # Get ground truth annotations from the batch\n",
    "        gt_pose = batch['pose'].to(device)\n",
    "        gt_betas = batch['betas'].to(device)\n",
    "        gt_vertices = smpl_neutral(betas=gt_betas, body_pose=gt_pose[:, 3:], global_orient=gt_pose[:, :3]).vertices\n",
    "        images = batch['img'].to(device)\n",
    "        gender = batch['gender'].to(device)\n",
    "        curr_batch_size = images.shape[0]\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward pass - here we will try to use ground-truth betas to assess the difference in error\n",
    "        \n",
    "        Somehow the smpl_neutral function can intake both axis-angles (as in gt_vertices)\n",
    "        and rotation matrices (as in pred_output); that's thanks to the pose2rot argument\n",
    "        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            pred_rotmat, pred_betas, pred_camera = model(images)\n",
    "            #print(\"pred_rotmat\", pred_rotmat, pred_rotmat.shape) # torch.Size([32, 24, 3, 3]), in rotation matrix\n",
    "            #print(\"gt_pose\", gt_pose, gt_pose.shape) # torch.Size([32, 72]), in axis-angles\n",
    "            pred_output = smpl_neutral(betas=pred_betas, body_pose=pred_rotmat[:,1:], global_orient=pred_rotmat[:,0].unsqueeze(1), pose2rot=False) # pred_betas and pred_pose (original)\n",
    "            #pred_output = smpl_neutral(betas=gt_betas, body_pose=pred_rotmat[:,1:], global_orient=pred_rotmat[:,0].unsqueeze(1), pose2rot=False) # gt_betas and pred_pose\n",
    "            #pred_output = smpl_neutral(betas=gt_betas, body_pose=gt_pose[:, 3:], global_orient=gt_pose[:, :3]) # gt_betas and gt_pose\n",
    "            #pred_output = smpl_neutral(betas=pred_betas, body_pose=gt_pose[:, 3:], global_orient=gt_pose[:, :3]) # pred_betas and gt_pose\n",
    "            pred_vertices = pred_output.vertices\n",
    "\n",
    "        if save_results:\n",
    "            rot_pad = torch.tensor([0,0,1], dtype=torch.float32, device=device).view(1,3,1)\n",
    "            rotmat = torch.cat((pred_rotmat.view(-1, 3, 3), rot_pad.expand(curr_batch_size * 24, -1, -1)), dim=-1)\n",
    "            pred_pose = tgm.rotation_matrix_to_angle_axis(rotmat).contiguous().view(-1, 72)\n",
    "            smpl_pose[step * batch_size:step * batch_size + curr_batch_size, :] = pred_pose.cpu().numpy()\n",
    "            smpl_betas[step * batch_size:step * batch_size + curr_batch_size, :]  = pred_betas.cpu().numpy()\n",
    "            smpl_camera[step * batch_size:step * batch_size + curr_batch_size, :]  = pred_camera.cpu().numpy()\n",
    "            \n",
    "        \"\"\"\n",
    "        Orientation error evaluation\n",
    "        \"\"\"\n",
    "        \n",
    "        orientation_error_per_part, orientation_error = orientation_evaluation(gt_pose, pred_rotmat, batch_size, curr_batch_size, step)\n",
    "        \n",
    "        mpjae[step * batch_size:step * batch_size + curr_batch_size] = orientation_error\n",
    "        pjae[step*batch_size : step*batch_size + curr_batch_size] = orientation_error_per_part\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "            \n",
    "        # 3D pose evaluation\n",
    "        if eval_pose:\n",
    "            # Regressor broadcasting\n",
    "            J_regressor_batch = J_regressor[None, :].expand(pred_vertices.shape[0], -1, -1).to(device)\n",
    "            # Get 14 ground truth joints\n",
    "            if 'h36m' in dataset_name or 'mpi-inf' in dataset_name:\n",
    "                gt_keypoints_3d = batch['pose_3d'].cuda()\n",
    "                gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_gt, :-1]\n",
    "            # For 3DPW get the 14 common joints from the rendered shape\n",
    "            else:\n",
    "                gt_vertices = smpl_male(global_orient=gt_pose[:,:3], body_pose=gt_pose[:,3:], betas=gt_betas).vertices \n",
    "                gt_vertices_female = smpl_female(global_orient=gt_pose[:,:3], body_pose=gt_pose[:,3:], betas=gt_betas).vertices \n",
    "                gt_vertices[gender==1, :, :] = gt_vertices_female[gender==1, :, :]\n",
    "                gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices)\n",
    "                gt_pelvis = gt_keypoints_3d[:, [0],:].clone()\n",
    "                gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :]\n",
    "                gt_keypoints_3d = gt_keypoints_3d - gt_pelvis \n",
    "\n",
    "\n",
    "            # Get 14 predicted joints from the mesh\n",
    "            pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices)\n",
    "            if save_results:\n",
    "                pred_joints[step * batch_size:step * batch_size + curr_batch_size, :, :]  = pred_keypoints_3d.cpu().numpy()\n",
    "            pred_pelvis = pred_keypoints_3d[:, [0],:].clone()\n",
    "            pred_keypoints_3d = pred_keypoints_3d[:, joint_mapper_h36m, :]\n",
    "            pred_keypoints_3d = pred_keypoints_3d - pred_pelvis \n",
    "\n",
    "            # Absolute error (MPJPE)\n",
    "            error = torch.sqrt(((pred_keypoints_3d - gt_keypoints_3d) ** 2).sum(dim=-1)).mean(dim=-1).cpu().numpy()\n",
    "            mpjpe[step * batch_size:step * batch_size + curr_batch_size] = error\n",
    "\n",
    "            # Reconstuction_error\n",
    "            r_error = reconstruction_error(pred_keypoints_3d.cpu().numpy(), gt_keypoints_3d.cpu().numpy(), reduction=None)\n",
    "            recon_err[step * batch_size:step * batch_size + curr_batch_size] = r_error\n",
    "\n",
    "\n",
    "        # If mask or part evaluation, render the mask and part images\n",
    "        if eval_masks or eval_parts:\n",
    "            mask, parts = renderer(pred_vertices, pred_camera)\n",
    "\n",
    "        # Mask evaluation (for LSP)\n",
    "        if eval_masks:\n",
    "            center = batch['center'].cpu().numpy()\n",
    "            scale = batch['scale'].cpu().numpy()\n",
    "            # Dimensions of original image\n",
    "            orig_shape = batch['orig_shape'].cpu().numpy()\n",
    "            for i in range(curr_batch_size):\n",
    "                # After rendering, convert imate back to original resolution\n",
    "                pred_mask = uncrop(mask[i].cpu().numpy(), center[i], scale[i], orig_shape[i]) > 0\n",
    "                # Load gt mask\n",
    "                gt_mask = cv2.imread(os.path.join(annot_path, batch['maskname'][i]), 0) > 0\n",
    "                # Evaluation consistent with the original UP-3D code\n",
    "                accuracy += (gt_mask == pred_mask).sum()\n",
    "                pixel_count += np.prod(np.array(gt_mask.shape))\n",
    "                for c in range(2):\n",
    "                    cgt = gt_mask == c\n",
    "                    cpred = pred_mask == c\n",
    "                    tp[c] += (cgt & cpred).sum()\n",
    "                    fp[c] +=  (~cgt & cpred).sum()\n",
    "                    fn[c] +=  (cgt & ~cpred).sum()\n",
    "                f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "        # Part evaluation (for LSP)\n",
    "        if eval_parts:\n",
    "            center = batch['center'].cpu().numpy()\n",
    "            scale = batch['scale'].cpu().numpy()\n",
    "            orig_shape = batch['orig_shape'].cpu().numpy()\n",
    "            for i in range(curr_batch_size):\n",
    "                pred_parts = uncrop(parts[i].cpu().numpy().astype(np.uint8), center[i], scale[i], orig_shape[i])\n",
    "                # Load gt part segmentation\n",
    "                gt_parts = cv2.imread(os.path.join(annot_path, batch['partname'][i]), 0)\n",
    "                # Evaluation consistent with the original UP-3D code\n",
    "                # 6 parts + background\n",
    "                for c in range(7):\n",
    "                   cgt = gt_parts == c\n",
    "                   cpred = pred_parts == c\n",
    "                   cpred[gt_parts == 255] = 0\n",
    "                   parts_tp[c] += (cgt & cpred).sum()\n",
    "                   parts_fp[c] +=  (~cgt & cpred).sum()\n",
    "                   parts_fn[c] +=  (cgt & ~cpred).sum()\n",
    "                gt_parts[gt_parts == 255] = 0\n",
    "                pred_parts[pred_parts == 255] = 0\n",
    "                parts_f1 = 2 * parts_tp / (2 * parts_tp + parts_fp + parts_fn)\n",
    "                parts_accuracy += (gt_parts == pred_parts).sum()\n",
    "                parts_pixel_count += np.prod(np.array(gt_parts.shape))\n",
    "\n",
    "        # Print intermediate results during evaluation\n",
    "        if step % log_freq == log_freq - 1:\n",
    "            if eval_pose:\n",
    "                print('MPJPE: ' + str(1000 * mpjpe[:step * batch_size].mean()))\n",
    "                print('Reconstruction Error: ' + str(1000 * recon_err[:step * batch_size].mean()))\n",
    "                print()\n",
    "            if eval_masks:\n",
    "                print('Accuracy: ', accuracy / pixel_count)\n",
    "                print('F1: ', f1.mean())\n",
    "                print()\n",
    "            if eval_parts:\n",
    "                print('Parts Accuracy: ', parts_accuracy / parts_pixel_count)\n",
    "                print('Parts F1 (BG): ', parts_f1[[0,1,2,3,4,5,6]].mean())\n",
    "                print()\n",
    "                \n",
    "            ''''''\n",
    "            print('Orientation error: ' + str(mpjae[:step * batch_size].mean()))\n",
    "            ''''''\n",
    "\n",
    "    # Save reconstructions to a file for further processing\n",
    "    if save_results:\n",
    "        np.savez(result_file, pred_joints=pred_joints, pose=smpl_pose, betas=smpl_betas, camera=smpl_camera)\n",
    "    # Print final results during evaluation\n",
    "    print('*** Final Results ***')\n",
    "    print()\n",
    "    if eval_pose:\n",
    "        print('MPJPE: ' + str(1000 * mpjpe.mean()))\n",
    "        print('Reconstruction Error: ' + str(1000 * recon_err.mean()))\n",
    "        print()\n",
    "    if eval_masks:\n",
    "        print('Accuracy: ', accuracy / pixel_count)\n",
    "        print('F1: ', f1.mean())\n",
    "        print()\n",
    "    if eval_parts:\n",
    "        print('Parts Accuracy: ', parts_accuracy / parts_pixel_count)\n",
    "        print('Parts F1 (BG): ', parts_f1[[0,1,2,3,4,5,6]].mean())\n",
    "        print()\n",
    "        \n",
    "    \"\"\"\n",
    "    Saving predictions to files for data analysis\n",
    "    pjae actually shows no difference, since it is always calculated using pred_rotmat. for a change in orientation error,\n",
    "    the model would need to change to take gt_betas into account at inference time for example.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Orientation Error: ' + str(mpjae.mean()))\n",
    "    \n",
    "    torch.save(pjpe, 'pred_betas and pred_pose') # Uncomment to save the raw data to a file\n",
    "    torch.save(pjae, 'pred_betas and pred_pose') # Uncomment to save the raw data to a file\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # args = parser.parse_args() # Original arg parsing\n",
    "    args = parser.parse_args(['--checkpoint=data/model_checkpoint.pt','--dataset=3dpw', '--log_freq=20']) # Inserting our own arguments (jupyter specific)\n",
    "    # This is the line we base our arguments on: python3 eval.py --checkpoint=data/model_checkpoint.pt --dataset=h36m-p1 --log_freq=20\n",
    "    \n",
    "    model = hmr(config.SMPL_MEAN_PARAMS)\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    # Setup evaluation dataset\n",
    "    dataset = BaseDataset(None, args.dataset, is_train=False)\n",
    "    # Run evaluation\n",
    "    run_evaluation(model, args.dataset, dataset, args.result_file,\n",
    "                   batch_size=args.batch_size,\n",
    "                   shuffle=args.shuffle,\n",
    "                   log_freq=args.log_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working code cell\n",
    "\n",
    "\"\"\"\n",
    "This script can be used to evaluate a trained model on 3D pose/shape and masks/part segmentation. You first need to download the datasets and preprocess them.\n",
    "Example usage:\n",
    "```\n",
    "python3 eval.py --checkpoint=data/model_checkpoint.pt --dataset=h36m-p1 --log_freq=20\n",
    "```\n",
    "Running the above command will compute the MPJPE and Reconstruction Error on the Human3.6M dataset (Protocol I). The ```--dataset``` option can take different values based on the type of evaluation you want to perform:\n",
    "1. Human3.6M Protocol 1 ```--dataset=h36m-p1```\n",
    "2. Human3.6M Protocol 2 ```--dataset=h36m-p2```\n",
    "3. 3DPW ```--dataset=3dpw```\n",
    "4. LSP ```--dataset=lsp```\n",
    "5. MPI-INF-3DHP ```--dataset=mpi-inf-3dhp```\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import torchgeometry as tgm\n",
    "\n",
    "import config\n",
    "import constants\n",
    "from models import hmr, SMPL\n",
    "from datasets import BaseDataset\n",
    "from utils.imutils import uncrop\n",
    "from utils.pose_utils import reconstruction_error\n",
    "from utils.part_utils import PartRenderer\n",
    "\n",
    "# Define command-line arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--checkpoint', default=None, help='Path to network checkpoint')\n",
    "parser.add_argument('--dataset', default='h36m-p1', choices=['h36m-p1', 'h36m-p2', 'lsp', '3dpw', 'mpi-inf-3dhp'], help='Choose evaluation dataset')\n",
    "parser.add_argument('--log_freq', default=50, type=int, help='Frequency of printing intermediate results')\n",
    "parser.add_argument('--batch_size', default=32, help='Batch size for testing')\n",
    "parser.add_argument('--shuffle', default=False, action='store_true', help='Shuffle data')\n",
    "parser.add_argument('--num_workers', default=8, type=int, help='Number of processes for data loading')\n",
    "parser.add_argument('--result_file', default=None, help='If set, save detections to a .npz file')\n",
    "\n",
    "def run_evaluation(model, dataset_name, dataset, result_file,\n",
    "                   batch_size=32, img_res=224, \n",
    "                   num_workers=32, shuffle=False, log_freq=50):\n",
    "    \"\"\"Run evaluation on the datasets and metrics we report in the paper. \"\"\"\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # Transfer model to the GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Load SMPL model\n",
    "    smpl_neutral = SMPL(config.SMPL_MODEL_DIR,\n",
    "                        create_transl=False).to(device)\n",
    "    smpl_male = SMPL(config.SMPL_MODEL_DIR,\n",
    "                     gender='male',\n",
    "                     create_transl=False).to(device)\n",
    "    smpl_female = SMPL(config.SMPL_MODEL_DIR,\n",
    "                       gender='female',\n",
    "                       create_transl=False).to(device)\n",
    "    \n",
    "    renderer = PartRenderer()\n",
    "    \n",
    "    # Regressor for H36m joints\n",
    "    J_regressor = torch.from_numpy(np.load(config.JOINT_REGRESSOR_H36M)).float()\n",
    "    \n",
    "    save_results = result_file is not None\n",
    "    # Disable shuffling if you want to save the results\n",
    "    if save_results:\n",
    "        shuffle=False\n",
    "    # Create dataloader for the dataset\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    \n",
    "    # Pose metrics\n",
    "    # MPJPE and Reconstruction error for the non-parametric and parametric shapes\n",
    "    mpjpe = np.zeros(len(dataset))\n",
    "    recon_err = np.zeros(len(dataset))\n",
    "    mpjpe_smpl = np.zeros(len(dataset))\n",
    "    recon_err_smpl = np.zeros(len(dataset))\n",
    "\n",
    "    # Shape metrics\n",
    "    # Mean per-vertex error\n",
    "    shape_err = np.zeros(len(dataset))\n",
    "    shape_err_smpl = np.zeros(len(dataset))\n",
    "\n",
    "    # Mask and part metrics\n",
    "    # Accuracy\n",
    "    accuracy = 0.\n",
    "    parts_accuracy = 0.\n",
    "    # True positive, false positive and false negative\n",
    "    tp = np.zeros((2,1))\n",
    "    fp = np.zeros((2,1))\n",
    "    fn = np.zeros((2,1))\n",
    "    parts_tp = np.zeros((7,1))\n",
    "    parts_fp = np.zeros((7,1))\n",
    "    parts_fn = np.zeros((7,1))\n",
    "    # Pixel count accumulators\n",
    "    pixel_count = 0\n",
    "    parts_pixel_count = 0\n",
    "\n",
    "    # Store SMPL parameters\n",
    "    smpl_pose = np.zeros((len(dataset), 72))\n",
    "    smpl_betas = np.zeros((len(dataset), 10))\n",
    "    smpl_camera = np.zeros((len(dataset), 3))\n",
    "    pred_joints = np.zeros((len(dataset), 17, 3))\n",
    "\n",
    "    eval_pose = False\n",
    "    eval_masks = False\n",
    "    eval_parts = False\n",
    "    # Choose appropriate evaluation for each dataset\n",
    "    if dataset_name == 'h36m-p1' or dataset_name == 'h36m-p2' or dataset_name == '3dpw' or dataset_name == 'mpi-inf-3dhp':\n",
    "        eval_pose = True\n",
    "    elif dataset_name == 'lsp':\n",
    "        eval_masks = True\n",
    "        eval_parts = True\n",
    "        annot_path = config.DATASET_FOLDERS['upi-s1h']\n",
    "\n",
    "    joint_mapper_h36m = constants.H36M_TO_J17 if dataset_name == 'mpi-inf-3dhp' else constants.H36M_TO_J14\n",
    "    joint_mapper_gt = constants.J24_TO_J17 if dataset_name == 'mpi-inf-3dhp' else constants.J24_TO_J14\n",
    "    # Iterate over the entire dataset\n",
    "    for step, batch in enumerate(tqdm(data_loader, desc='Eval', total=len(data_loader))):\n",
    "        # Get ground truth annotations from the batch\n",
    "        gt_pose = batch['pose'].to(device)\n",
    "        gt_betas = batch['betas'].to(device)\n",
    "        gt_vertices = smpl_neutral(betas=gt_betas, body_pose=gt_pose[:, 3:], global_orient=gt_pose[:, :3]).vertices\n",
    "        images = batch['img'].to(device)\n",
    "        gender = batch['gender'].to(device)\n",
    "        curr_batch_size = images.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_rotmat, pred_betas, pred_camera = model(images)\n",
    "            pred_output = smpl_neutral(betas=pred_betas, body_pose=pred_rotmat[:,1:], global_orient=pred_rotmat[:,0].unsqueeze(1), pose2rot=False)\n",
    "            pred_vertices = pred_output.vertices\n",
    "\n",
    "        if save_results:\n",
    "            rot_pad = torch.tensor([0,0,1], dtype=torch.float32, device=device).view(1,3,1)\n",
    "            rotmat = torch.cat((pred_rotmat.view(-1, 3, 3), rot_pad.expand(curr_batch_size * 24, -1, -1)), dim=-1)\n",
    "            pred_pose = tgm.rotation_matrix_to_angle_axis(rotmat).contiguous().view(-1, 72)\n",
    "            smpl_pose[step * batch_size:step * batch_size + curr_batch_size, :] = pred_pose.cpu().numpy()\n",
    "            smpl_betas[step * batch_size:step * batch_size + curr_batch_size, :]  = pred_betas.cpu().numpy()\n",
    "            smpl_camera[step * batch_size:step * batch_size + curr_batch_size, :]  = pred_camera.cpu().numpy()\n",
    "            \n",
    "        # 3D pose evaluation\n",
    "        if eval_pose:\n",
    "            # Regressor broadcasting\n",
    "            J_regressor_batch = J_regressor[None, :].expand(pred_vertices.shape[0], -1, -1).to(device)\n",
    "            # Get 14 ground truth joints\n",
    "            if 'h36m' in dataset_name or 'mpi-inf' in dataset_name:\n",
    "                gt_keypoints_3d = batch['pose_3d'].cuda()\n",
    "                gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_gt, :-1]\n",
    "            # For 3DPW get the 14 common joints from the rendered shape\n",
    "            else:\n",
    "                gt_vertices = smpl_male(global_orient=gt_pose[:,:3], body_pose=gt_pose[:,3:], betas=gt_betas).vertices \n",
    "                gt_vertices_female = smpl_female(global_orient=gt_pose[:,:3], body_pose=gt_pose[:,3:], betas=gt_betas).vertices \n",
    "                gt_vertices[gender==1, :, :] = gt_vertices_female[gender==1, :, :]\n",
    "                gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices)\n",
    "                gt_pelvis = gt_keypoints_3d[:, [0],:].clone()\n",
    "                gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :]\n",
    "                gt_keypoints_3d = gt_keypoints_3d - gt_pelvis \n",
    "\n",
    "\n",
    "            # Get 14 predicted joints from the mesh\n",
    "            pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices)\n",
    "            if save_results:\n",
    "                pred_joints[step * batch_size:step * batch_size + curr_batch_size, :, :]  = pred_keypoints_3d.cpu().numpy()\n",
    "            pred_pelvis = pred_keypoints_3d[:, [0],:].clone()\n",
    "            pred_keypoints_3d = pred_keypoints_3d[:, joint_mapper_h36m, :]\n",
    "            pred_keypoints_3d = pred_keypoints_3d - pred_pelvis \n",
    "\n",
    "            # Absolute error (MPJPE)\n",
    "            error = torch.sqrt(((pred_keypoints_3d - gt_keypoints_3d) ** 2).sum(dim=-1)).mean(dim=-1).cpu().numpy()\n",
    "            mpjpe[step * batch_size:step * batch_size + curr_batch_size] = error\n",
    "\n",
    "            # Reconstuction_error\n",
    "            r_error = reconstruction_error(pred_keypoints_3d.cpu().numpy(), gt_keypoints_3d.cpu().numpy(), reduction=None)\n",
    "            recon_err[step * batch_size:step * batch_size + curr_batch_size] = r_error\n",
    "\n",
    "\n",
    "        # If mask or part evaluation, render the mask and part images\n",
    "        if eval_masks or eval_parts:\n",
    "            mask, parts = renderer(pred_vertices, pred_camera)\n",
    "\n",
    "        # Mask evaluation (for LSP)\n",
    "        if eval_masks:\n",
    "            center = batch['center'].cpu().numpy()\n",
    "            scale = batch['scale'].cpu().numpy()\n",
    "            # Dimensions of original image\n",
    "            orig_shape = batch['orig_shape'].cpu().numpy()\n",
    "            for i in range(curr_batch_size):\n",
    "                # After rendering, convert imate back to original resolution\n",
    "                pred_mask = uncrop(mask[i].cpu().numpy(), center[i], scale[i], orig_shape[i]) > 0\n",
    "                # Load gt mask\n",
    "                gt_mask = cv2.imread(os.path.join(annot_path, batch['maskname'][i]), 0) > 0\n",
    "                # Evaluation consistent with the original UP-3D code\n",
    "                accuracy += (gt_mask == pred_mask).sum()\n",
    "                pixel_count += np.prod(np.array(gt_mask.shape))\n",
    "                for c in range(2):\n",
    "                    cgt = gt_mask == c\n",
    "                    cpred = pred_mask == c\n",
    "                    tp[c] += (cgt & cpred).sum()\n",
    "                    fp[c] +=  (~cgt & cpred).sum()\n",
    "                    fn[c] +=  (cgt & ~cpred).sum()\n",
    "                f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "        # Part evaluation (for LSP)\n",
    "        if eval_parts:\n",
    "            center = batch['center'].cpu().numpy()\n",
    "            scale = batch['scale'].cpu().numpy()\n",
    "            orig_shape = batch['orig_shape'].cpu().numpy()\n",
    "            for i in range(curr_batch_size):\n",
    "                pred_parts = uncrop(parts[i].cpu().numpy().astype(np.uint8), center[i], scale[i], orig_shape[i])\n",
    "                # Load gt part segmentation\n",
    "                gt_parts = cv2.imread(os.path.join(annot_path, batch['partname'][i]), 0)\n",
    "                # Evaluation consistent with the original UP-3D code\n",
    "                # 6 parts + background\n",
    "                for c in range(7):\n",
    "                   cgt = gt_parts == c\n",
    "                   cpred = pred_parts == c\n",
    "                   cpred[gt_parts == 255] = 0\n",
    "                   parts_tp[c] += (cgt & cpred).sum()\n",
    "                   parts_fp[c] +=  (~cgt & cpred).sum()\n",
    "                   parts_fn[c] +=  (cgt & ~cpred).sum()\n",
    "                gt_parts[gt_parts == 255] = 0\n",
    "                pred_parts[pred_parts == 255] = 0\n",
    "                parts_f1 = 2 * parts_tp / (2 * parts_tp + parts_fp + parts_fn)\n",
    "                parts_accuracy += (gt_parts == pred_parts).sum()\n",
    "                parts_pixel_count += np.prod(np.array(gt_parts.shape))\n",
    "\n",
    "        # Print intermediate results during evaluation\n",
    "        if step % log_freq == log_freq - 1:\n",
    "            if eval_pose:\n",
    "                print('MPJPE: ' + str(1000 * mpjpe[:step * batch_size].mean()))\n",
    "                print('Reconstruction Error: ' + str(1000 * recon_err[:step * batch_size].mean()))\n",
    "                print()\n",
    "            if eval_masks:\n",
    "                print('Accuracy: ', accuracy / pixel_count)\n",
    "                print('F1: ', f1.mean())\n",
    "                print()\n",
    "            if eval_parts:\n",
    "                print('Parts Accuracy: ', parts_accuracy / parts_pixel_count)\n",
    "                print('Parts F1 (BG): ', parts_f1[[0,1,2,3,4,5,6]].mean())\n",
    "                print()\n",
    "\n",
    "    # Save reconstructions to a file for further processing\n",
    "    if save_results:\n",
    "        np.savez(result_file, pred_joints=pred_joints, pose=smpl_pose, betas=smpl_betas, camera=smpl_camera)\n",
    "    # Print final results during evaluation\n",
    "    print('*** Final Results ***')\n",
    "    print()\n",
    "    if eval_pose:\n",
    "        print('MPJPE: ' + str(1000 * mpjpe.mean()))\n",
    "        print('Reconstruction Error: ' + str(1000 * recon_err.mean()))\n",
    "        print()\n",
    "    if eval_masks:\n",
    "        print('Accuracy: ', accuracy / pixel_count)\n",
    "        print('F1: ', f1.mean())\n",
    "        print()\n",
    "    if eval_parts:\n",
    "        print('Parts Accuracy: ', parts_accuracy / parts_pixel_count)\n",
    "        print('Parts F1 (BG): ', parts_f1[[0,1,2,3,4,5,6]].mean())\n",
    "        print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # args = parser.parse_args() # Original arg parsing\n",
    "    args = parser.parse_args(['--checkpoint=data/model_checkpoint.pt','--dataset=3dpw', '--log_freq=20']) # Inserting our own arguments (jupyter specific)\n",
    "    # This is the line we base our arguments on: python3 eval.py --checkpoint=data/model_checkpoint.pt --dataset=h36m-p1 --log_freq=20\n",
    "    \n",
    "    model = hmr(config.SMPL_MEAN_PARAMS)\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    # Setup evaluation dataset\n",
    "    dataset = BaseDataset(None, args.dataset, is_train=False)\n",
    "    # Run evaluation\n",
    "    run_evaluation(model, args.dataset, dataset, args.result_file,\n",
    "                   batch_size=args.batch_size,\n",
    "                   shuffle=args.shuffle,\n",
    "                   log_freq=args.log_freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
