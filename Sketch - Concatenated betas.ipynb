{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import BaseDataset\n",
    "\n",
    "# Is options the instance of a class? In the end mixed_dataset only passes it to base_dataset, which uses\n",
    "# noise factor, scale factor and rotation factor\n",
    "\n",
    "class Options:\n",
    "    pass\n",
    "\n",
    "options = Options()\n",
    "\n",
    "options.noise_factor = 0.4\n",
    "options.scale_factor = 0.25\n",
    "options.rot_factor = 30\n",
    "\n",
    "train_ds = BaseDataset(options, '3dpw', is_train=False) # is_train has to be false since 3dpw is only used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0] # Now 3dpw is in ../Datasets/3dpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BaseDataset(options, 'mpii', is_train=True) # Loading mpii to check if it has_smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BaseDataset(options, 'mpi-inf-3dhp') # Loading mpii to check is has_smpl = True\n",
    "\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_t = train_ds[60000]['img']\n",
    "\n",
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[3200]['betas'] - train_ds[0]['betas'] # Betas are the same for the length of the clip, and for each clip with the same subject!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.resnet as resnet\n",
    "import numpy as np\n",
    "import math\n",
    "from utils.geometry import rot6d_to_rotmat\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\" Redefinition of Bottleneck residual block\n",
    "        Adapted from the official PyTorch implementation\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class HMR(nn.Module):\n",
    "    \"\"\" SMPL Iterative Regressor with ResNet50 backbone\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, layers, smpl_mean_params):\n",
    "        self.inplanes = 64\n",
    "        super(HMR, self).__init__()\n",
    "        npose = 24 * 6\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc1 = nn.Linear(512 * block.expansion + npose + 13, 1024) # -10 to remove betas from fully connected layer\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.decpose = nn.Linear(1024, npose)\n",
    "        # self.decshape = nn.Linear(1024, 10)\n",
    "        self.deccam = nn.Linear(1024, 3)\n",
    "        nn.init.xavier_uniform_(self.decpose.weight, gain=0.01)\n",
    "        # nn.init.xavier_uniform_(self.decshape.weight, gain=0.01)\n",
    "        nn.init.xavier_uniform_(self.deccam.weight, gain=0.01)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        mean_params = np.load(smpl_mean_params)\n",
    "        init_pose = torch.from_numpy(mean_params['pose'][:]).unsqueeze(0)\n",
    "        # init_shape = torch.from_numpy(mean_params['shape'][:].astype('float32')).unsqueeze(0)\n",
    "        init_cam = torch.from_numpy(mean_params['cam']).unsqueeze(0)\n",
    "        self.register_buffer('init_pose', init_pose)\n",
    "        # self.register_buffer('init_shape', init_shape)\n",
    "        self.register_buffer('init_cam', init_cam)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, betas, init_pose=None, init_cam=None, n_iter=3): # Removed init_shape=None, inserted betas\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        if init_pose is None:\n",
    "            init_pose = self.init_pose.expand(batch_size, -1)\n",
    "        #if init_shape is None:\n",
    "        #    init_shape = self.init_shape.expand(batch_size, -1)\n",
    "        if init_cam is None:\n",
    "            init_cam = self.init_cam.expand(batch_size, -1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        xf = self.avgpool(x4)\n",
    "        xf = xf.view(xf.size(0), -1)\n",
    "\n",
    "        pred_pose = init_pose\n",
    "        #pred_shape = init_shape\n",
    "        gt_shape = betas\n",
    "        pred_cam = init_cam\n",
    "        for i in range(n_iter):\n",
    "            xc = torch.cat([xf, pred_pose, gt_shape, pred_cam],1) # replaced pred_shape with gt_shape\n",
    "            xc = self.fc1(xc)\n",
    "            xc = self.drop1(xc)\n",
    "            xc = self.fc2(xc)\n",
    "            xc = self.drop2(xc)\n",
    "            pred_pose = self.decpose(xc) + pred_pose\n",
    "            # pred_shape = self.decshape(xc) + pred_shape\n",
    "            pred_cam = self.deccam(xc) + pred_cam\n",
    "            \n",
    "        \n",
    "        pred_rotmat = rot6d_to_rotmat(pred_pose).view(batch_size, 24, 3, 3)\n",
    "\n",
    "        return pred_rotmat, pred_cam # removed pred_shape\n",
    "\n",
    "def hmr(smpl_mean_params, pretrained=True, **kwargs):\n",
    "    \"\"\" Constructs an HMR model with ResNet50 backbone.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = HMR(Bottleneck, [3, 4, 6, 3],  smpl_mean_params, **kwargs)\n",
    "    if pretrained:\n",
    "        resnet_imagenet = resnet.resnet50(pretrained=True)\n",
    "        model.load_state_dict(resnet_imagenet.state_dict(),strict=False)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "model = hmr(config.SMPL_MEAN_PARAMS, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_ds[0]['img'].unsqueeze(0)\n",
    "betas = train_ds[0]['betas'].unsqueeze(0)\n",
    "\n",
    "model(x,betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchgeometry import angle_axis_to_rotation_matrix, rotation_matrix_to_angle_axis\n",
    "import cv2\n",
    "\n",
    "from datasets import MixedDataset\n",
    "from datasets import BaseDataset\n",
    "from models import hmr, SMPL\n",
    "from smplify import SMPLify\n",
    "from utils.geometry import batch_rodrigues, perspective_projection, estimate_translation\n",
    "from utils.renderer import Renderer\n",
    "from utils import BaseTrainer\n",
    "\n",
    "import config\n",
    "import constants\n",
    "# from .fits_dict import FitsDict\n",
    "\n",
    "\n",
    "class Trainer(BaseTrainer):\n",
    "    \n",
    "    def init_fn(self):\n",
    "        self.train_ds = MixedDataset(self.options, ignore_3d=self.options.ignore_3d, is_train=True) # This has to be is_train=True\n",
    "        #self.train_ds = BaseDataset(self.options, '3dpw', is_train=False)\n",
    "\n",
    "        self.model = hmr(config.SMPL_MEAN_PARAMS, pretrained=True).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(),\n",
    "                                          lr=self.options.lr,\n",
    "                                          weight_decay=0)\n",
    "        self.smpl = SMPL(config.SMPL_MODEL_DIR,\n",
    "                         batch_size=self.options.batch_size,\n",
    "                         create_transl=False).to(self.device)\n",
    "        # Per-vertex loss on the shape\n",
    "        self.criterion_shape = nn.L1Loss().to(self.device)\n",
    "        # Keypoint (2D and 3D) loss\n",
    "        # No reduction because confidence weighting needs to be applied\n",
    "        self.criterion_keypoints = nn.MSELoss(reduction='none').to(self.device)\n",
    "        # Loss for SMPL parameter regression\n",
    "        self.criterion_regr = nn.MSELoss().to(self.device)\n",
    "        self.models_dict = {'model': self.model}\n",
    "        self.optimizers_dict = {'optimizer': self.optimizer}\n",
    "        self.focal_length = constants.FOCAL_LENGTH\n",
    "\n",
    "        # Initialize SMPLify fitting module\n",
    "        self.smplify = SMPLify(step_size=1e-2, batch_size=self.options.batch_size, num_iters=self.options.num_smplify_iters, focal_length=self.focal_length)\n",
    "        if self.options.pretrained_checkpoint is not None:\n",
    "            self.load_pretrained(checkpoint_file=self.options.pretrained_checkpoint)\n",
    "\n",
    "        # Load dictionary of fits\n",
    "        #self.fits_dict = FitsDict(self.options, self.train_ds)\n",
    "\n",
    "        # Create renderer\n",
    "        self.renderer = Renderer(focal_length=self.focal_length, img_res=self.options.img_res, faces=self.smpl.faces)\n",
    "\n",
    "    #def finalize(self):\n",
    "        #self.fits_dict.save()\n",
    "\n",
    "    def keypoint_loss(self, pred_keypoints_2d, gt_keypoints_2d, openpose_weight, gt_weight):\n",
    "        \"\"\" Compute 2D reprojection loss on the keypoints.\n",
    "        The loss is weighted by the confidence.\n",
    "        The available keypoints are different for each dataset.\n",
    "        \"\"\"\n",
    "        conf = gt_keypoints_2d[:, :, -1].unsqueeze(-1).clone()\n",
    "        conf[:, :25] *= openpose_weight\n",
    "        conf[:, 25:] *= gt_weight\n",
    "        loss = (conf * self.criterion_keypoints(pred_keypoints_2d, gt_keypoints_2d[:, :, :-1])).mean()\n",
    "        return loss\n",
    "\n",
    "    def keypoint_3d_loss(self, pred_keypoints_3d, gt_keypoints_3d, has_pose_3d):\n",
    "        \"\"\"Compute 3D keypoint loss for the examples that 3D keypoint annotations are available.\n",
    "        The loss is weighted by the confidence.\n",
    "        \"\"\"\n",
    "        pred_keypoints_3d = pred_keypoints_3d[:, 25:, :]\n",
    "        conf = gt_keypoints_3d[:, :, -1].unsqueeze(-1).clone()\n",
    "        gt_keypoints_3d = gt_keypoints_3d[:, :, :-1].clone()\n",
    "        gt_keypoints_3d = gt_keypoints_3d[has_pose_3d == 1]\n",
    "        conf = conf[has_pose_3d == 1]\n",
    "        pred_keypoints_3d = pred_keypoints_3d[has_pose_3d == 1]\n",
    "        if len(gt_keypoints_3d) > 0:\n",
    "            gt_pelvis = (gt_keypoints_3d[:, 2,:] + gt_keypoints_3d[:, 3,:]) / 2\n",
    "            gt_keypoints_3d = gt_keypoints_3d - gt_pelvis[:, None, :]\n",
    "            pred_pelvis = (pred_keypoints_3d[:, 2,:] + pred_keypoints_3d[:, 3,:]) / 2\n",
    "            pred_keypoints_3d = pred_keypoints_3d - pred_pelvis[:, None, :]\n",
    "            return (conf * self.criterion_keypoints(pred_keypoints_3d, gt_keypoints_3d)).mean()\n",
    "        else:\n",
    "            return torch.FloatTensor(1).fill_(0.).to(self.device)\n",
    "\n",
    "    def shape_loss(self, pred_vertices, gt_vertices, has_smpl):\n",
    "        \"\"\"Compute per-vertex loss on the shape for the examples that SMPL annotations are available.\"\"\"\n",
    "        pred_vertices_with_shape = pred_vertices[has_smpl == 1]\n",
    "        gt_vertices_with_shape = gt_vertices[has_smpl == 1]\n",
    "        if len(gt_vertices_with_shape) > 0:\n",
    "            return self.criterion_shape(pred_vertices_with_shape, gt_vertices_with_shape)\n",
    "        else:\n",
    "            return torch.FloatTensor(1).fill_(0.).to(self.device)\n",
    "\n",
    "    def smpl_losses(self, pred_rotmat, gt_pose, has_smpl): # Remove pred_betas and gt_betas\n",
    "        pred_rotmat_valid = pred_rotmat[has_smpl == 1]\n",
    "        gt_rotmat_valid = batch_rodrigues(gt_pose.view(-1,3)).view(-1, 24, 3, 3)[has_smpl == 1]\n",
    "        # pred_betas_valid = pred_betas[has_smpl == 1]\n",
    "        #gt_betas_valid = gt_betas[has_smpl == 1]\n",
    "        if len(pred_rotmat_valid) > 0:\n",
    "            loss_regr_pose = self.criterion_regr(pred_rotmat_valid, gt_rotmat_valid)\n",
    "            #loss_regr_betas = self.criterion_regr(pred_betas_valid, gt_betas_valid)\n",
    "        else:\n",
    "            loss_regr_pose = torch.FloatTensor(1).fill_(0.).to(self.device)\n",
    "            #loss_regr_betas = torch.FloatTensor(1).fill_(0.).to(self.device)\n",
    "        return loss_regr_pose # remove loss_regr_betas\n",
    "\n",
    "    def train_step(self, input_batch):\n",
    "        self.model.train()\n",
    "\n",
    "        # Get data from the batch\n",
    "        images = input_batch['img'] # input image\n",
    "        gt_keypoints_2d = input_batch['keypoints'] # 2D keypoints\n",
    "        gt_pose = input_batch['pose'] # SMPL pose parameters\n",
    "        gt_betas = input_batch['betas'] # SMPL beta parameters\n",
    "        gt_joints = input_batch['pose_3d'] # 3D pose\n",
    "        has_smpl = input_batch['has_smpl'].byte() # flag that indicates whether SMPL parameters are valid\n",
    "        has_pose_3d = input_batch['has_pose_3d'].byte() # flag that indicates whether 3D pose is valid\n",
    "        is_flipped = input_batch['is_flipped'] # flag that indicates whether image was flipped during data augmentation\n",
    "        rot_angle = input_batch['rot_angle'] # rotation angle used for data augmentation\n",
    "        dataset_name = input_batch['dataset_name'] # name of the dataset the image comes from\n",
    "        indices = input_batch['sample_index'] # index of example inside its dataset\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        # Get GT vertices and model joints\n",
    "        # Note that gt_model_joints is different from gt_joints as it comes from SMPL\n",
    "        gt_out = self.smpl(betas=gt_betas, body_pose=gt_pose[:,3:], global_orient=gt_pose[:,:3])\n",
    "        gt_model_joints = gt_out.joints\n",
    "        gt_vertices = gt_out.vertices\n",
    "\n",
    "        # Get current best fits from the dictionary # Commenting out all the optimized fits saved in the dictionary\n",
    "        #opt_pose, opt_betas = self.fits_dict[(dataset_name, indices.cpu(), rot_angle.cpu(), is_flipped.cpu())]\n",
    "        #opt_pose = opt_pose.to(self.device)\n",
    "        #opt_betas = opt_betas.to(self.device)\n",
    "        #opt_output = self.smpl(betas=gt_betas, body_pose=opt_pose[:,3:], global_orient=opt_pose[:,:3]) # Bypass the need for opt_betas by replacing it with gt_betas here\n",
    "        #opt_vertices = opt_output.vertices\n",
    "        #opt_joints = opt_output.joints\n",
    "\n",
    "\n",
    "        # De-normalize 2D keypoints from [-1,1] to pixel space\n",
    "        gt_keypoints_2d_orig = gt_keypoints_2d.clone()\n",
    "        gt_keypoints_2d_orig[:, :, :-1] = 0.5 * self.options.img_res * (gt_keypoints_2d_orig[:, :, :-1] + 1)\n",
    "\n",
    "        # Estimate camera translation given the model joints and 2D keypoints\n",
    "        # by minimizing a weighted least squares loss\n",
    "        #gt_cam_t = estimate_translation(gt_model_joints, gt_keypoints_2d_orig, focal_length=self.focal_length, img_size=self.options.img_res) # Respectfully commenting this out\n",
    "\n",
    "        #opt_cam_t = estimate_translation(opt_joints, gt_keypoints_2d_orig, focal_length=self.focal_length, img_size=self.options.img_res)\n",
    "\n",
    "\n",
    "        #opt_joint_loss = self.smplify.get_fitting_loss(opt_pose, gt_betas, opt_cam_t, # replace opt_betas with gt_betas\n",
    "        #                                               0.5 * self.options.img_res * torch.ones(batch_size, 2, device=self.device),\n",
    "        #                                               gt_keypoints_2d_orig).mean(dim=-1)\n",
    "\n",
    "        # Feed images in the network to predict camera and SMPL parameters\n",
    "        pred_rotmat, pred_camera = self.model(images, gt_betas) # Remove pred_betas, Feed betas into the network\n",
    "\n",
    "        pred_output = self.smpl(betas=gt_betas, body_pose=pred_rotmat[:,1:], global_orient=pred_rotmat[:,0].unsqueeze(1), pose2rot=False) # Replace pred_betas with gt_betas\n",
    "        pred_vertices = pred_output.vertices\n",
    "        pred_joints = pred_output.joints\n",
    "\n",
    "        # Convert Weak Perspective Camera [s, tx, ty] to camera translation [tx, ty, tz] in 3D given the bounding box size\n",
    "        # This camera translation can be used in a full perspective projection\n",
    "        pred_cam_t = torch.stack([pred_camera[:,1],\n",
    "                                  pred_camera[:,2],\n",
    "                                  2*self.focal_length/(self.options.img_res * pred_camera[:,0] +1e-9)],dim=-1)\n",
    "\n",
    "\n",
    "        camera_center = torch.zeros(batch_size, 2, device=self.device)\n",
    "        pred_keypoints_2d = perspective_projection(pred_joints,\n",
    "                                                   rotation=torch.eye(3, device=self.device).unsqueeze(0).expand(batch_size, -1, -1),\n",
    "                                                   translation=pred_cam_t,\n",
    "                                                   focal_length=self.focal_length,\n",
    "                                                   camera_center=camera_center)\n",
    "        # Normalize keypoints to [-1,1]\n",
    "        pred_keypoints_2d = pred_keypoints_2d / (self.options.img_res / 2.)\n",
    "\n",
    "        if self.options.run_smplify:\n",
    "\n",
    "            # Convert predicted rotation matrices to axis-angle\n",
    "            pred_rotmat_hom = torch.cat([pred_rotmat.detach().view(-1, 3, 3).detach(), torch.tensor([0,0,1], dtype=torch.float32,\n",
    "                device=self.device).view(1, 3, 1).expand(batch_size * 24, -1, -1)], dim=-1)\n",
    "            pred_pose = rotation_matrix_to_angle_axis(pred_rotmat_hom).contiguous().view(batch_size, -1)\n",
    "            # tgm.rotation_matrix_to_angle_axis returns NaN for 0 rotation, so manually hack it\n",
    "            pred_pose[torch.isnan(pred_pose)] = 0.0\n",
    "\n",
    "            # Run SMPLify optimization starting from the network prediction\n",
    "            new_opt_vertices, new_opt_joints,\\\n",
    "            new_opt_pose, new_opt_betas,\\\n",
    "            new_opt_cam_t, new_opt_joint_loss = self.smplify(\n",
    "                                        pred_pose.detach(), gt_betas.detach(), # Replace pred_betas.detach() with gt_betas\n",
    "                                        pred_cam_t.detach(),\n",
    "                                        0.5 * self.options.img_res * torch.ones(batch_size, 2, device=self.device),\n",
    "                                        gt_keypoints_2d_orig)\n",
    "            new_opt_joint_loss = new_opt_joint_loss.mean(dim=-1)\n",
    "\n",
    "            # Will update the dictionary for the examples where the new loss is less than the current one\n",
    "            #update = (new_opt_joint_loss < opt_joint_loss)\n",
    "            \n",
    "\n",
    "            #opt_joint_loss[update] = new_opt_joint_loss[update]\n",
    "            #opt_vertices[update, :] = new_opt_vertices[update, :]\n",
    "            #opt_joints[update, :] = new_opt_joints[update, :]\n",
    "            #opt_pose[update, :] = new_opt_pose[update, :]\n",
    "            #opt_betas[update, :] = new_opt_betas[update, :]\n",
    "            #opt_cam_t[update, :] = new_opt_cam_t[update, :]\n",
    "\n",
    "\n",
    "            #self.fits_dict[(dataset_name, indices.cpu(), rot_angle.cpu(), is_flipped.cpu(), update.cpu())] = (opt_pose.cpu(), opt_betas.cpu())\n",
    "\n",
    "        else:\n",
    "            update = torch.zeros(batch_size, device=self.device).byte()\n",
    "\n",
    "        # Replace extreme betas with zero betas\n",
    "        #opt_betas[(opt_betas.abs() > 3).any(dim=-1)] = 0.\n",
    "\n",
    "        # Replace the optimized parameters with the ground truth parameters, if available\n",
    "        #opt_vertices[has_smpl, :, :] = gt_vertices[has_smpl, :, :]\n",
    "        #opt_cam_t[has_smpl, :] = gt_cam_t[has_smpl, :]\n",
    "        #opt_joints[has_smpl, :, :] = gt_model_joints[has_smpl, :, :]\n",
    "        #opt_pose[has_smpl, :] = gt_pose[has_smpl, :]\n",
    "        #opt_betas[has_smpl, :] = gt_betas[has_smpl, :]\n",
    "\n",
    "        # Assert whether a fit is valid by comparing the joint loss with the threshold\n",
    "        #valid_fit = (opt_joint_loss < self.options.smplify_threshold).to(self.device)\n",
    "        # Add the examples with GT parameters to the list of valid fits\n",
    "        #valid_fit = valid_fit | has_smpl\n",
    "\n",
    "        #opt_keypoints_2d = perspective_projection(opt_joints,\n",
    "        #                                          rotation=torch.eye(3, device=self.device).unsqueeze(0).expand(batch_size, -1, -1),\n",
    "        #                                          translation=opt_cam_t,\n",
    "        #                                          focal_length=self.focal_length,\n",
    "        #                                          camera_center=camera_center)\n",
    "\n",
    "\n",
    "        #opt_keypoints_2d = opt_keypoints_2d / (self.options.img_res / 2.)\n",
    "\n",
    "\n",
    "        # Compute loss on SMPL parameters\n",
    "        loss_regr_pose = self.smpl_losses(pred_rotmat, gt_pose, 1) # Remove loss_regr_betas, pred_betas, and opt_betas, replace opt_pose with gt_pose and valid_fit with 1\n",
    "\n",
    "        # Compute 2D reprojection loss for the keypoints\n",
    "        loss_keypoints = self.keypoint_loss(pred_keypoints_2d, gt_keypoints_2d,\n",
    "                                            self.options.openpose_train_weight,\n",
    "                                            self.options.gt_train_weight)\n",
    "\n",
    "        # Compute 3D keypoint loss\n",
    "        loss_keypoints_3d = self.keypoint_3d_loss(pred_joints, gt_joints, has_pose_3d)\n",
    "\n",
    "        # Per-vertex loss for the shape\n",
    "        loss_shape = self.shape_loss(pred_vertices, gt_vertices, 1) # replace opt_vertices with gt_vertices, valid_fit with 1\n",
    "\n",
    "        # Compute total loss\n",
    "        # The last component is a loss that forces the network to predict positive depth values\n",
    "        loss = self.options.shape_loss_weight * loss_shape +\\\n",
    "               self.options.keypoint_loss_weight * loss_keypoints +\\\n",
    "               self.options.keypoint_loss_weight * loss_keypoints_3d +\\\n",
    "               self.options.pose_loss_weight * loss_regr_pose +\\\n",
    "               ((torch.exp(-pred_camera[:,0]*10)) ** 2 ).mean()\n",
    "        loss *= 60\n",
    "        # Remove self.options.beta_loss_weight * loss_regr_betas +\\\n",
    "\n",
    "        # Do backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Pack output arguments for tensorboard logging\n",
    "        output = {'pred_vertices': pred_vertices.detach(),\n",
    "                  #'opt_vertices': opt_vertices, # Remove optimized values\n",
    "                  'pred_cam_t': pred_cam_t.detach()\n",
    "                  #'opt_cam_t': opt_cam_t}\n",
    "                 }\n",
    "        losses = {'loss': loss.detach().item(),\n",
    "                  'loss_keypoints': loss_keypoints.detach().item(),\n",
    "                  'loss_keypoints_3d': loss_keypoints_3d.detach().item(),\n",
    "                  'loss_regr_pose': loss_regr_pose.detach().item(),\n",
    "                  # 'loss_regr_betas': loss_regr_betas.detach().item(), # Remove loss_regr_betas\n",
    "                  'loss_shape': loss_shape.detach().item()}\n",
    "\n",
    "        return output, losses\n",
    "\n",
    "    def train_summaries(self, input_batch, output, losses):\n",
    "        images = input_batch['img']\n",
    "        images = images * torch.tensor([0.229, 0.224, 0.225], device=images.device).reshape(1,3,1,1)\n",
    "        images = images + torch.tensor([0.485, 0.456, 0.406], device=images.device).reshape(1,3,1,1)\n",
    "\n",
    "        pred_vertices = output['pred_vertices']\n",
    "        #opt_vertices = output['opt_vertices']\n",
    "        pred_cam_t = output['pred_cam_t']\n",
    "        #opt_cam_t = output['opt_cam_t']\n",
    "        images_pred = self.renderer.visualize_tb(pred_vertices, pred_cam_t, images)\n",
    "        #images_opt = self.renderer.visualize_tb(opt_vertices, opt_cam_t, images)\n",
    "        self.summary_writer.add_image('pred_shape', images_pred, self.step_count)\n",
    "        #self.summary_writer.add_image('opt_shape', images_opt, self.step_count)\n",
    "        for loss_name, val in losses.items():\n",
    "            self.summary_writer.add_scalar(loss_name, val, self.step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the definition of different heterogeneous datasets used for training\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#from .base_dataset import BaseDataset\n",
    "\n",
    "class MixedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, options, **kwargs):\n",
    "        #self.dataset_list = ['h36m', 'lsp-orig', 'mpii', 'lspet', 'coco', 'mpi-inf-3dhp']\n",
    "        self.dataset_list = ['3dpw','mpi-inf-3dhp']\n",
    "        #self.dataset_dict = {'h36m': 0, 'lsp-orig': 1, 'mpii': 2, 'lspet': 3, 'coco': 4, 'mpi-inf-3dhp': 5}\n",
    "        self.dataset_dict = {'3dpw': 0, 'mpi-inf-3dhp': 1}\n",
    "        \n",
    "        self.datasets = [BaseDataset(options, ds, **kwargs) for ds in self.dataset_list]\n",
    "        total_length = sum([len(ds) for ds in self.datasets])\n",
    "        length_itw = sum([len(ds) for ds in self.datasets[1:-1]])\n",
    "        self.length = max([len(ds) for ds in self.datasets])\n",
    "\n",
    "        \"\"\"\n",
    "        Data distribution inside each batch:\n",
    "        30% H36M - 60% ITW - 10% MPI-INF\n",
    "        \"\"\"\n",
    "        self.partition = [.5,\n",
    "                          0.5]\n",
    "        self.partition = np.array(self.partition).cumsum()\n",
    "        print(self.partition)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        p = np.random.rand()\n",
    "        for i in range(2): # Change range 6 to 2, since only two datasets will be used\n",
    "            if p <= self.partition[i]:\n",
    "                return self.datasets[i][index % len(self.datasets[i])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132022\n",
      "96507\n",
      "[0.5 1. ]\n"
     ]
    }
   ],
   "source": [
    "class Options:\n",
    "    pass\n",
    "\n",
    "options = Options()\n",
    "\n",
    "options.noise_factor = 0.4\n",
    "options.scale_factor = 0.25\n",
    "options.rot_factor = 30\n",
    "\n",
    "train_ds = MixedDataset(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[-1.8782, -1.8782, -1.8782,  ..., -1.8097, -1.8097, -1.8268],\n",
       "          [-1.8782, -1.8782, -1.8782,  ..., -1.8097, -1.8097, -1.8268],\n",
       "          [-1.8782, -1.8782, -1.8782,  ..., -1.8097, -1.8097, -1.8268],\n",
       "          ...,\n",
       "          [-1.2445, -1.2445, -1.2617,  ..., -1.0733, -1.0733, -1.0904],\n",
       "          [-1.2445, -1.2445, -1.2617,  ..., -1.0562, -1.0562, -1.0733],\n",
       "          [-1.2274, -1.2617, -1.2788,  ..., -1.0904, -1.0904, -1.0904]],\n",
       " \n",
       "         [[-1.2479, -1.2479, -1.2479,  ..., -1.0903, -1.0903, -1.1604],\n",
       "          [-1.2479, -1.2479, -1.2479,  ..., -1.0903, -1.0903, -1.1604],\n",
       "          [-1.2479, -1.2479, -1.2479,  ..., -1.0903, -1.0903, -1.1604],\n",
       "          ...,\n",
       "          [ 0.3452,  0.3803,  0.3803,  ...,  0.5553,  0.5553,  0.5378],\n",
       "          [ 0.3452,  0.3978,  0.3803,  ...,  0.5903,  0.5903,  0.5553],\n",
       "          [ 0.3803,  0.3803,  0.3452,  ...,  0.5553,  0.5553,  0.5553]],\n",
       " \n",
       "         [[-1.6302, -1.6302, -1.6302,  ..., -1.5779, -1.5779, -1.5953],\n",
       "          [-1.6302, -1.6302, -1.6302,  ..., -1.5779, -1.5779, -1.5953],\n",
       "          [-1.6302, -1.6302, -1.6302,  ..., -1.5953, -1.5779, -1.5953],\n",
       "          ...,\n",
       "          [-1.1770, -1.1596, -1.1770,  ..., -1.0898, -1.0898, -1.0898],\n",
       "          [-1.1944, -1.1944, -1.1944,  ..., -1.0724, -1.0724, -1.0898],\n",
       "          [-1.1944, -1.1944, -1.2119,  ..., -1.0898, -1.0898, -1.0898]]]),\n",
       " 'pose': tensor([-1.2542,  0.3589, -2.7972, -0.1335,  0.0944,  0.2480, -1.4249,  0.0435,\n",
       "         -0.2233,  0.3972, -0.1752,  0.0810,  1.7729, -0.4635, -0.2110,  1.9695,\n",
       "         -0.0933,  0.4074, -0.2283,  0.0793, -0.0325, -0.2238,  0.4303,  0.0210,\n",
       "         -0.5695, -0.0620,  0.0457,  0.0820, -0.0037, -0.0047, -0.5360, -0.1224,\n",
       "          0.0243, -0.1271, -0.2063, -0.3081,  0.2484, -0.0065, -0.0811, -0.0595,\n",
       "          0.1053,  0.3752, -0.0545, -0.0440, -0.4483, -0.0531,  0.1025,  0.1012,\n",
       "         -0.0906,  0.1943, -0.2868, -0.3036, -0.1691,  0.2579,  0.0404, -0.1799,\n",
       "         -0.0532, -0.2183,  0.0916,  0.0147, -0.0731, -0.2106,  0.1251, -0.2020,\n",
       "          0.2198, -0.2350,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]),\n",
       " 'betas': tensor([ 0.2916,  0.4177, -1.2688, -3.6987,  3.3167, -3.0961, -5.4095,  1.3103,\n",
       "         -0.4943,  0.3893]),\n",
       " 'imgname': '../Datasets/3dhp/S3/Seq1/imageFrames/video_1/frame_000383.jpg',\n",
       " 'pose_3d': tensor([[ 0.1414,  0.4345,  0.1267,  1.0000],\n",
       "         [ 0.3858,  0.1225,  0.1711,  1.0000],\n",
       "         [ 0.0942,  0.0130, -0.0843,  1.0000],\n",
       "         [-0.0942, -0.0130,  0.0843,  1.0000],\n",
       "         [-0.0481,  0.4619,  0.2539,  1.0000],\n",
       "         [-0.3580,  0.4121,  0.0079,  1.0000],\n",
       "         [ 0.4401, -0.4168, -0.6685,  1.0000],\n",
       "         [ 0.3105, -0.4180, -0.4568,  1.0000],\n",
       "         [ 0.1660, -0.4357, -0.1569,  1.0000],\n",
       "         [-0.0421, -0.4809,  0.0474,  1.0000],\n",
       "         [-0.2980, -0.5613,  0.2454,  1.0000],\n",
       "         [-0.4715, -0.6251,  0.4111,  1.0000],\n",
       "         [ 0.0663, -0.4980, -0.0670,  1.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0073, -0.2366, -0.0450,  1.0000],\n",
       "         [ 0.1180, -0.5732, -0.0564,  1.0000],\n",
       "         [ 0.2215, -0.7247, -0.0672,  1.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]),\n",
       " 'keypoints': tensor([[ 0.4107, -0.7500,  0.5359],\n",
       "         [ 0.0446, -0.5982,  0.6636],\n",
       "         [ 0.1696, -0.5804,  0.6141],\n",
       "         [ 0.3571, -0.5446,  0.4881],\n",
       "         [ 0.5893, -0.5804,  0.5475],\n",
       "         [-0.1429, -0.6875,  0.6179],\n",
       "         [-0.4911, -0.7500,  0.6087],\n",
       "         [-0.7054, -0.8393,  0.5966],\n",
       "         [-0.0893,  0.4107,  0.6483],\n",
       "         [ 0.0268,  0.3929,  0.6146],\n",
       "         [ 0.6518,  0.5536,  0.6289],\n",
       "         [ 0.3393,  1.1607,  0.6398],\n",
       "         [-0.1786,  0.4107,  0.5984],\n",
       "         [-0.0268,  1.0357,  0.6361],\n",
       "         [-0.6964,  1.1250,  0.6221],\n",
       "         [ 0.3929, -0.7857,  0.5609],\n",
       "         [-4.7232, -6.8125,  0.0000],\n",
       "         [ 0.3036, -0.8036,  0.6526],\n",
       "         [-4.7232, -6.8125,  0.0000],\n",
       "         [-0.7500,  1.2857,  0.3666],\n",
       "         [-0.7679,  1.2500,  0.3712],\n",
       "         [-0.8214,  1.0536,  0.5329],\n",
       "         [ 0.5714,  1.1964,  0.4961],\n",
       "         [ 0.5000,  1.2321,  0.5426],\n",
       "         [ 0.2589,  1.2143,  0.5912],\n",
       "         [ 0.2679,  1.0893,  1.0000],\n",
       "         [ 0.7321,  0.5089,  1.0000],\n",
       "         [ 0.0982,  0.3214,  1.0000],\n",
       "         [-0.1875,  0.2589,  1.0000],\n",
       "         [-0.0357,  1.1161,  1.0000],\n",
       "         [-0.7232,  1.0714,  1.0000],\n",
       "         [ 0.6071, -0.5982,  1.0000],\n",
       "         [ 0.4018, -0.5625,  1.0000],\n",
       "         [ 0.2143, -0.5625,  1.0000],\n",
       "         [-0.1071, -0.6250,  1.0000],\n",
       "         [-0.4911, -0.7411,  1.0000],\n",
       "         [-0.7232, -0.8304,  1.0000],\n",
       "         [ 0.0536, -0.6696,  1.0000],\n",
       "         [-4.7232, -6.8125,  0.0000],\n",
       "         [-0.0446,  0.2857,  1.0000],\n",
       "         [-4.7232, -6.8125,  0.0000],\n",
       "         [-0.0536, -0.1607,  1.0000],\n",
       "         [ 0.1607, -0.8125,  1.0000],\n",
       "         [ 0.3571, -1.0982,  1.0000],\n",
       "         [-4.7232, -6.8125,  0.0000],\n",
       "         [-4.7232, -6.8125,  0.0000],\n",
       "         [-4.7232, -6.8125,  0.0000],\n",
       "         [-4.7232, -6.8125,  0.0000],\n",
       "         [-4.7232, -6.8125,  0.0000]]),\n",
       " 'has_smpl': 1.0,\n",
       " 'has_pose_3d': 1,\n",
       " 'scale': 1.519821,\n",
       " 'center': array([ 719.8405, 1037.781 ], dtype=float32),\n",
       " 'orig_shape': array([2048, 2048]),\n",
       " 'is_flipped': 0,\n",
       " 'rot_angle': 0.0,\n",
       " 'gender': -1,\n",
       " 'sample_index': 23493,\n",
       " 'dataset_name': 'mpi-inf-3dhp',\n",
       " 'maskname': '',\n",
       " 'partname': ''}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[120000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
