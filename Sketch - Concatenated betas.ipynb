{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import BaseDataset\n",
    "\n",
    "# Is options the instance of a class? In the end mixed_dataset only passes it to base_dataset, which uses\n",
    "# noise factor, scale factor and rotation factor\n",
    "\n",
    "class Options:\n",
    "    pass\n",
    "\n",
    "options = Options()\n",
    "\n",
    "options.noise_factor = 1\n",
    "options.scale_factor = 1\n",
    "options.rot_factor = 1\n",
    "\n",
    "train_ds = BaseDataset(options, '3dpw', is_train=False) # is_train has to be false since 3dpw is only used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[-2.1179, -2.1179, -1.7754,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -1.7754,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -1.7754,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -1.7754,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -1.7754,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -1.8268,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[-2.0357, -2.0357, -1.7556,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -1.7556,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -1.7556,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -1.7031,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -1.7031,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -1.7556,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[-1.8044, -1.8044, -1.6302,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.6302,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.6302,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.4907,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.4907,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.5430,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
       " 'pose': tensor([-2.9397e+00, -1.0410e-03,  3.1818e-02,  4.7312e-02,  4.8609e-02,\n",
       "         -3.4900e-02,  4.2976e-02,  2.1482e-02,  7.2871e-02,  2.3877e-02,\n",
       "         -9.7733e-03,  1.8996e-02, -2.2319e-02, -5.4317e-02,  5.5338e-03,\n",
       "          1.7168e-03, -4.4522e-02, -4.1843e-02, -2.7451e-02, -5.2092e-03,\n",
       "         -5.6466e-03, -1.5187e-02, -8.1737e-02,  5.6110e-02, -2.1019e-02,\n",
       "          1.1807e-02, -3.0516e-02, -2.1826e-02, -2.3024e-03, -7.1679e-03,\n",
       "          5.6936e-03,  1.3876e-03, -4.3822e-02, -1.4592e-04,  1.1120e-02,\n",
       "         -3.1642e-04, -8.9678e-02,  5.9827e-02, -2.8936e-02,  9.1496e-03,\n",
       "         -1.8441e-01, -2.9704e-01, -1.0146e-02,  2.4868e-01,  2.8471e-01,\n",
       "          1.9619e-01,  6.9565e-04,  5.9755e-02,  2.1419e-01, -1.0884e-01,\n",
       "         -1.1611e+00,  2.0869e-01,  6.0795e-02,  1.2005e+00, -3.0475e-01,\n",
       "         -1.9099e-01, -1.5218e-01, -2.2567e-01,  1.7310e-01,  1.0341e-01,\n",
       "         -5.0697e-02, -9.1013e-03,  9.1049e-02, -1.2819e-01,  6.3646e-02,\n",
       "         -5.1857e-02, -1.7663e-01, -3.4416e-02, -1.6535e-01, -1.1512e-01,\n",
       "          1.0264e-01,  1.9353e-01]),\n",
       " 'betas': tensor([ 0.9101,  0.4228,  1.3988, -1.5478,  0.3159,  0.9758,  0.7574,  0.0121,\n",
       "         -1.3913, -0.7966]),\n",
       " 'imgname': '../Datasets/3dpw/imageFiles/downtown_enterShop_00/image_00000.jpg',\n",
       " 'pose_3d': tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " 'keypoints': tensor([[-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000],\n",
       "         [-0.9643, -1.7857,  0.0000]]),\n",
       " 'has_smpl': 1.0,\n",
       " 'has_pose_3d': 0,\n",
       " 'scale': 5.915760000000001,\n",
       " 'center': array([ 574.765, 1066.94 ], dtype=float32),\n",
       " 'orig_shape': array([1920, 1080]),\n",
       " 'is_flipped': 0,\n",
       " 'rot_angle': 0.0,\n",
       " 'gender': 0,\n",
       " 'sample_index': 0,\n",
       " 'dataset_name': '3dpw',\n",
       " 'maskname': '',\n",
       " 'partname': ''}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0] # Now 3dpw is in ../Datasets/3dpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BaseDataset(options, 'mpii', is_train=True) # Loading mpii to check if it has_smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
       " 'pose': tensor([ 0.0000,  0.0000, -0.0082,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]),\n",
       " 'betas': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'imgname': '../Datasets/mpii_human_pose_v1/images/015601864.jpg',\n",
       " 'pose_3d': tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " 'keypoints': tensor([[ 0.1250, -0.1518,  0.8461],\n",
       "         [ 0.0893, -0.1875,  0.6864],\n",
       "         [ 0.0268, -0.1964,  0.6000],\n",
       "         [-0.0446, -0.1875,  0.7327],\n",
       "         [ 0.0179, -0.1161,  0.7466],\n",
       "         [ 0.1429, -0.1786,  0.7363],\n",
       "         [ 0.1429, -0.0804,  0.6607],\n",
       "         [ 0.1250,  0.0179,  0.7470],\n",
       "         [ 0.0446, -0.1518,  0.4839],\n",
       "         [ 0.0089, -0.1518,  0.4686],\n",
       "         [ 0.0357, -0.0446,  0.6893],\n",
       "         [ 0.0446,  0.1250,  0.6911],\n",
       "         [ 0.0804, -0.1429,  0.4593],\n",
       "         [ 0.0982, -0.0536,  0.2694],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [ 0.1071, -0.1696,  0.8765],\n",
       "         [ 0.1339, -0.1696,  0.8174],\n",
       "         [ 0.0804, -0.1964,  0.7934],\n",
       "         [ 0.1429, -0.1875,  0.0629],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [ 0.0357,  0.1786,  0.6645],\n",
       "         [ 0.0268,  0.1786,  0.6262],\n",
       "         [ 0.0536,  0.1339,  0.5111],\n",
       "         [ 0.0446,  0.1339,  1.0000],\n",
       "         [ 0.0357, -0.0446,  1.0000],\n",
       "         [-0.0268, -0.1607,  1.0000],\n",
       "         [ 0.0804, -0.1518,  1.0000],\n",
       "         [ 0.0982, -0.1071,  1.0000],\n",
       "         [ 0.0893, -0.0982,  1.0000],\n",
       "         [ 0.0179, -0.1161,  1.0000],\n",
       "         [-0.0536, -0.1875,  1.0000],\n",
       "         [ 0.0089, -0.1786,  1.0000],\n",
       "         [ 0.1429, -0.1607,  1.0000],\n",
       "         [ 0.1429, -0.0804,  1.0000],\n",
       "         [ 0.1339,  0.0179,  1.0000],\n",
       "         [ 0.0625, -0.1518,  1.0000],\n",
       "         [ 0.1429, -0.2679,  1.0000],\n",
       "         [ 0.0268, -0.1518,  1.0000],\n",
       "         [ 0.0804, -0.1696,  1.0000],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [-0.8214, -0.4018,  0.0000],\n",
       "         [-0.8214, -0.4018,  0.0000]]),\n",
       " 'has_smpl': 0.0,\n",
       " 'has_pose_3d': 0,\n",
       " 'scale': 7.24509647421168,\n",
       " 'center': array([594.    , 302.3157], dtype=float32),\n",
       " 'orig_shape': array([ 720, 1280]),\n",
       " 'is_flipped': 0,\n",
       " 'rot_angle': 0.46800232,\n",
       " 'gender': -1,\n",
       " 'sample_index': 0,\n",
       " 'dataset_name': 'mpii',\n",
       " 'maskname': '',\n",
       " 'partname': ''}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[-1.4500, -1.4500, -1.4500,  ...,  1.6495,  1.5125,  1.4098],\n",
       "          [-1.4500, -1.4500, -1.4500,  ...,  1.6838,  1.5468,  1.4098],\n",
       "          [-1.4158, -1.4158, -1.4158,  ...,  1.7352,  1.5810,  1.4440],\n",
       "          ...,\n",
       "          [-1.2617, -1.2617, -1.2959,  ..., -1.4158, -1.4158, -1.4158],\n",
       "          [-1.2617, -1.2959, -1.3130,  ..., -1.4158, -1.4158, -1.4158],\n",
       "          [-1.2617, -1.2959, -1.3130,  ..., -1.4158, -1.4158, -1.4158]],\n",
       " \n",
       "         [[-0.7227, -0.7227, -0.7227,  ...,  1.6933,  1.5882,  1.5007],\n",
       "          [-0.7227, -0.7227, -0.7227,  ...,  1.7108,  1.6057,  1.5007],\n",
       "          [-0.7052, -0.7052, -0.7052,  ...,  1.7458,  1.6232,  1.5182],\n",
       "          ...,\n",
       "          [-0.4951, -0.4951, -0.4776,  ..., -0.7052, -0.7052, -0.7052],\n",
       "          [-0.4776, -0.4776, -0.4601,  ..., -0.6702, -0.6702, -0.6702],\n",
       "          [-0.4776, -0.4776, -0.4601,  ..., -0.6702, -0.6702, -0.6702]],\n",
       " \n",
       "         [[-0.8807, -0.8807, -0.8807,  ...,  2.4483,  2.3088,  2.2043],\n",
       "          [-0.8807, -0.8807, -0.8807,  ...,  2.5006,  2.3437,  2.2391],\n",
       "          [-0.8458, -0.8458, -0.8458,  ...,  2.5703,  2.4134,  2.2740],\n",
       "          ...,\n",
       "          [-0.6541, -0.6541, -0.6541,  ..., -0.7761, -0.8110, -0.8110],\n",
       "          [-0.6541, -0.6541, -0.6890,  ..., -0.8110, -0.8110, -0.8110],\n",
       "          [-0.6541, -0.6541, -0.6890,  ..., -0.8110, -0.8110, -0.8110]]]),\n",
       " 'pose': tensor([-1.9089e+00, -8.0997e-02, -2.3677e+00, -9.8924e-02, -3.0711e-04,\n",
       "         -2.4451e-02, -1.0507e-01, -6.9477e-02,  8.7044e-02,  2.4887e-02,\n",
       "          1.6155e-02, -1.7074e-03,  1.2007e-01, -2.1454e-01, -1.3477e-02,\n",
       "          1.2674e-01,  2.4554e-01,  6.4687e-03, -8.6501e-02, -6.0474e-02,\n",
       "          3.2624e-03, -5.2289e-02,  3.0747e-01, -1.1806e-01, -5.1623e-02,\n",
       "         -4.5264e-01,  4.0699e-01,  1.2337e-01, -1.1334e-02,  1.6016e-02,\n",
       "         -1.1016e-01, -3.6268e-02,  2.2188e-01, -9.5843e-02,  4.4763e-01,\n",
       "         -7.0465e-01, -1.6852e-01,  4.6276e-02, -3.0359e-02, -1.7487e-01,\n",
       "          1.7161e-02,  1.1813e-01, -1.7751e-01, -4.6749e-02, -9.9245e-02,\n",
       "         -3.7423e-02, -1.5431e-02,  4.5531e-02, -1.9336e-01, -8.2201e-02,\n",
       "         -2.0183e-01, -1.4997e-01, -1.9142e-02,  1.5995e-01, -1.5833e-01,\n",
       "         -7.1285e-02, -1.6069e-01, -2.2441e-01,  2.8532e-02,  2.1259e-02,\n",
       "          1.7630e-01,  5.3535e-02,  3.2956e-01,  3.2631e-01, -8.0993e-03,\n",
       "         -2.8134e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]),\n",
       " 'betas': tensor([ 0.7595,  1.0583, -1.0908, -2.8565,  0.8400, -1.4367, -3.3934,  0.4560,\n",
       "         -0.2364, -0.2460]),\n",
       " 'imgname': '../Datasets/3dhp/S1/Seq1/imageFrames/video_0/frame_000001.jpg',\n",
       " 'pose_3d': tensor([[-0.1233,  0.9841,  0.0051,  1.0000],\n",
       "         [-0.0364,  0.5539, -0.0429,  1.0000],\n",
       "         [ 0.0244,  0.0026, -0.1166,  1.0000],\n",
       "         [-0.0244, -0.0026,  0.1166,  1.0000],\n",
       "         [-0.0667,  0.5519,  0.0873,  1.0000],\n",
       "         [-0.1733,  0.9799,  0.0669,  1.0000],\n",
       "         [ 0.0750, -0.3201, -0.6944,  1.0000],\n",
       "         [ 0.0392, -0.3484, -0.4537,  1.0000],\n",
       "         [ 0.0249, -0.4092, -0.1430,  1.0000],\n",
       "         [-0.0098, -0.3969,  0.1103,  1.0000],\n",
       "         [-0.0368, -0.3951,  0.4261,  1.0000],\n",
       "         [-0.0368, -0.3745,  0.6703,  1.0000],\n",
       "         [ 0.0132, -0.4815, -0.0158,  1.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0055, -0.2270, -0.0082,  1.0000],\n",
       "         [ 0.0301, -0.5663, -0.0170,  1.0000],\n",
       "         [ 0.0099, -0.7379, -0.0279,  1.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]),\n",
       " 'keypoints': tensor([[ 1.1607e+00, -3.9286e+00,  8.3386e-01],\n",
       "         [ 1.3393e-01, -2.9018e+00,  8.0508e-01],\n",
       "         [ 3.9286e-01, -2.8214e+00,  7.8527e-01],\n",
       "         [ 4.5536e-01, -2.3214e+00,  4.6000e-01],\n",
       "         [ 7.5893e-01, -2.3125e+00,  3.9977e-01],\n",
       "         [-1.7857e-02, -3.0179e+00,  7.2676e-01],\n",
       "         [ 7.4107e-01, -1.7411e+00,  7.6092e-02],\n",
       "         [-1.3188e+01, -1.7661e+01,  0.0000e+00],\n",
       "         [ 8.0357e-02, -2.1429e-01,  7.0854e-01],\n",
       "         [ 3.0357e-01, -1.3393e-01,  7.7057e-01],\n",
       "         [-4.4643e-02,  2.0625e+00,  8.1198e-01],\n",
       "         [-5.4464e-01,  4.4018e+00,  8.3073e-01],\n",
       "         [ 0.0000e+00, -2.9464e-01,  6.3660e-01],\n",
       "         [-7.1429e-02,  1.9107e+00,  5.5800e-01],\n",
       "         [-5.2679e-01,  3.9107e+00,  4.7490e-01],\n",
       "         [ 1.0089e+00, -4.0536e+00,  8.3430e-01],\n",
       "         [-1.3188e+01, -1.7661e+01,  0.0000e+00],\n",
       "         [ 5.4464e-01, -3.9107e+00,  8.7813e-01],\n",
       "         [-1.3188e+01, -1.7661e+01,  0.0000e+00],\n",
       "         [ 2.1429e-01,  4.4643e+00,  1.9448e-01],\n",
       "         [ 1.4286e-01,  4.3482e+00,  1.6394e-01],\n",
       "         [-7.9464e-01,  4.0536e+00,  2.4279e-01],\n",
       "         [ 2.4107e-01,  4.8036e+00,  7.2327e-01],\n",
       "         [ 5.3571e-02,  4.8393e+00,  7.2270e-01],\n",
       "         [-7.7679e-01,  4.6161e+00,  7.9870e-01],\n",
       "         [-4.9107e-01,  4.7321e+00,  1.0000e+00],\n",
       "         [-1.7857e-02,  2.4375e+00,  1.0000e+00],\n",
       "         [ 3.2143e-01, -5.6250e-01,  1.0000e+00],\n",
       "         [ 5.3571e-02, -7.6786e-01,  1.0000e+00],\n",
       "         [-1.6964e-01,  2.2500e+00,  1.0000e+00],\n",
       "         [-7.5000e-01,  4.5804e+00,  1.0000e+00],\n",
       "         [ 6.7857e-01, -2.3125e+00,  1.0000e+00],\n",
       "         [ 4.1964e-01, -2.5268e+00,  1.0000e+00],\n",
       "         [ 3.3036e-01, -2.9018e+00,  1.0000e+00],\n",
       "         [ 1.3393e-01, -2.8393e+00,  1.0000e+00],\n",
       "         [ 8.9286e-03, -2.8304e+00,  1.0000e+00],\n",
       "         [ 2.6786e-02, -2.7321e+00,  1.0000e+00],\n",
       "         [ 2.5893e-01, -3.3036e+00,  1.0000e+00],\n",
       "         [-1.3188e+01, -1.7661e+01,  0.0000e+00],\n",
       "         [ 1.8750e-01, -6.6071e-01,  1.0000e+00],\n",
       "         [-1.3188e+01, -1.7661e+01,  0.0000e+00],\n",
       "         [ 1.5179e-01, -1.9018e+00,  1.0000e+00],\n",
       "         [ 3.4821e-01, -3.7679e+00,  1.0000e+00],\n",
       "         [ 2.4107e-01, -4.7232e+00,  1.0000e+00],\n",
       "         [-1.3188e+01, -1.7661e+01,  0.0000e+00],\n",
       "         [-1.3188e+01, -1.7661e+01,  0.0000e+00],\n",
       "         [-1.3188e+01, -1.7661e+01,  0.0000e+00],\n",
       "         [-1.3188e+01, -1.7661e+01,  0.0000e+00],\n",
       "         [-1.3188e+01, -1.7661e+01,  0.0000e+00]]),\n",
       " 'has_smpl': 1.0,\n",
       " 'has_pose_3d': 1,\n",
       " 'scale': 0.7342628417563928,\n",
       " 'center': array([1013.741, 1263.357], dtype=float32),\n",
       " 'orig_shape': array([2048, 2048]),\n",
       " 'is_flipped': 0,\n",
       " 'rot_angle': -2.0,\n",
       " 'gender': -1,\n",
       " 'sample_index': 0,\n",
       " 'dataset_name': 'mpi-inf-3dhp',\n",
       " 'maskname': '',\n",
       " 'partname': ''}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = BaseDataset(options, 'mpi-inf-3dhp') # Loading mpii to check is has_smpl = True\n",
    "\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_t = train_ds[0]['img']\n",
    "\n",
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[3200]['betas'] - train_ds[0]['betas'] # Betas are the same for the length of the clip, and for each clip with the same subject!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.resnet as resnet\n",
    "import numpy as np\n",
    "import math\n",
    "from utils.geometry import rot6d_to_rotmat\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\" Redefinition of Bottleneck residual block\n",
    "        Adapted from the official PyTorch implementation\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class HMR(nn.Module):\n",
    "    \"\"\" SMPL Iterative Regressor with ResNet50 backbone\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, layers, smpl_mean_params):\n",
    "        self.inplanes = 64\n",
    "        super(HMR, self).__init__()\n",
    "        npose = 24 * 6\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc1 = nn.Linear(512 * block.expansion + npose + 13, 1024) # -10 to remove betas from fully connected layer\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.decpose = nn.Linear(1024, npose)\n",
    "        # self.decshape = nn.Linear(1024, 10)\n",
    "        self.deccam = nn.Linear(1024, 3)\n",
    "        nn.init.xavier_uniform_(self.decpose.weight, gain=0.01)\n",
    "        # nn.init.xavier_uniform_(self.decshape.weight, gain=0.01)\n",
    "        nn.init.xavier_uniform_(self.deccam.weight, gain=0.01)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        mean_params = np.load(smpl_mean_params)\n",
    "        init_pose = torch.from_numpy(mean_params['pose'][:]).unsqueeze(0)\n",
    "        # init_shape = torch.from_numpy(mean_params['shape'][:].astype('float32')).unsqueeze(0)\n",
    "        init_cam = torch.from_numpy(mean_params['cam']).unsqueeze(0)\n",
    "        self.register_buffer('init_pose', init_pose)\n",
    "        # self.register_buffer('init_shape', init_shape)\n",
    "        self.register_buffer('init_cam', init_cam)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, betas, init_pose=None, init_cam=None, n_iter=3): # Removed init_shape=None, inserted betas\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        if init_pose is None:\n",
    "            init_pose = self.init_pose.expand(batch_size, -1)\n",
    "        #if init_shape is None:\n",
    "        #    init_shape = self.init_shape.expand(batch_size, -1)\n",
    "        if init_cam is None:\n",
    "            init_cam = self.init_cam.expand(batch_size, -1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        xf = self.avgpool(x4)\n",
    "        xf = xf.view(xf.size(0), -1)\n",
    "\n",
    "        pred_pose = init_pose\n",
    "        #pred_shape = init_shape\n",
    "        gt_shape = betas\n",
    "        pred_cam = init_cam\n",
    "        for i in range(n_iter):\n",
    "            xc = torch.cat([xf, pred_pose, gt_shape, pred_cam],1) # replaced pred_shape with gt_shape\n",
    "            xc = self.fc1(xc)\n",
    "            xc = self.drop1(xc)\n",
    "            xc = self.fc2(xc)\n",
    "            xc = self.drop2(xc)\n",
    "            pred_pose = self.decpose(xc) + pred_pose\n",
    "            # pred_shape = self.decshape(xc) + pred_shape\n",
    "            pred_cam = self.deccam(xc) + pred_cam\n",
    "            \n",
    "        \n",
    "        pred_rotmat = rot6d_to_rotmat(pred_pose).view(batch_size, 24, 3, 3)\n",
    "\n",
    "        return pred_rotmat, pred_cam # removed pred_shape\n",
    "\n",
    "def hmr(smpl_mean_params, pretrained=True, **kwargs):\n",
    "    \"\"\" Constructs an HMR model with ResNet50 backbone.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = HMR(Bottleneck, [3, 4, 6, 3],  smpl_mean_params, **kwargs)\n",
    "    if pretrained:\n",
    "        resnet_imagenet = resnet.resnet50(pretrained=True)\n",
    "        model.load_state_dict(resnet_imagenet.state_dict(),strict=False)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "model = hmr(config.SMPL_MEAN_PARAMS, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_ds[0]['img'].unsqueeze(0)\n",
    "betas = train_ds[0]['betas'].unsqueeze(0)\n",
    "\n",
    "model(x,betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchgeometry import angle_axis_to_rotation_matrix, rotation_matrix_to_angle_axis\n",
    "import cv2\n",
    "\n",
    "from datasets import MixedDataset\n",
    "from datasets import BaseDataset\n",
    "from models import hmr, SMPL\n",
    "from smplify import SMPLify\n",
    "from utils.geometry import batch_rodrigues, perspective_projection, estimate_translation\n",
    "from utils.renderer import Renderer\n",
    "from utils import BaseTrainer\n",
    "\n",
    "import config\n",
    "import constants\n",
    "# from .fits_dict import FitsDict\n",
    "\n",
    "\n",
    "class Trainer(BaseTrainer):\n",
    "    \n",
    "    def init_fn(self):\n",
    "        #self.train_ds = MixedDataset(self.options, ignore_3d=self.options.ignore_3d, is_train=True)\n",
    "        self.train_ds = BaseDataset(self.options, '3dpw', is_train=False)\n",
    "        # self.train_ds.dataset_dict = {'3dpw': 0} # Creating a dataset_dict attribute to mimic mixed_dataset behavior (doesn't quite work)\n",
    "\n",
    "        self.model = hmr(config.SMPL_MEAN_PARAMS, pretrained=True).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(),\n",
    "                                          lr=self.options.lr,\n",
    "                                          weight_decay=0)\n",
    "        self.smpl = SMPL(config.SMPL_MODEL_DIR,\n",
    "                         batch_size=self.options.batch_size,\n",
    "                         create_transl=False).to(self.device)\n",
    "        # Per-vertex loss on the shape\n",
    "        self.criterion_shape = nn.L1Loss().to(self.device)\n",
    "        # Keypoint (2D and 3D) loss\n",
    "        # No reduction because confidence weighting needs to be applied\n",
    "        self.criterion_keypoints = nn.MSELoss(reduction='none').to(self.device)\n",
    "        # Loss for SMPL parameter regression\n",
    "        self.criterion_regr = nn.MSELoss().to(self.device)\n",
    "        self.models_dict = {'model': self.model}\n",
    "        self.optimizers_dict = {'optimizer': self.optimizer}\n",
    "        self.focal_length = constants.FOCAL_LENGTH\n",
    "\n",
    "        # Initialize SMPLify fitting module\n",
    "        self.smplify = SMPLify(step_size=1e-2, batch_size=self.options.batch_size, num_iters=self.options.num_smplify_iters, focal_length=self.focal_length)\n",
    "        if self.options.pretrained_checkpoint is not None:\n",
    "            self.load_pretrained(checkpoint_file=self.options.pretrained_checkpoint)\n",
    "\n",
    "        # Load dictionary of fits\n",
    "        #self.fits_dict = FitsDict(self.options, self.train_ds)\n",
    "\n",
    "        # Create renderer\n",
    "        self.renderer = Renderer(focal_length=self.focal_length, img_res=self.options.img_res, faces=self.smpl.faces)\n",
    "\n",
    "    #def finalize(self):\n",
    "        #self.fits_dict.save()\n",
    "\n",
    "    def keypoint_loss(self, pred_keypoints_2d, gt_keypoints_2d, openpose_weight, gt_weight):\n",
    "        \"\"\" Compute 2D reprojection loss on the keypoints.\n",
    "        The loss is weighted by the confidence.\n",
    "        The available keypoints are different for each dataset.\n",
    "        \"\"\"\n",
    "        conf = gt_keypoints_2d[:, :, -1].unsqueeze(-1).clone()\n",
    "        conf[:, :25] *= openpose_weight\n",
    "        conf[:, 25:] *= gt_weight\n",
    "        loss = (conf * self.criterion_keypoints(pred_keypoints_2d, gt_keypoints_2d[:, :, :-1])).mean()\n",
    "        return loss\n",
    "\n",
    "    def keypoint_3d_loss(self, pred_keypoints_3d, gt_keypoints_3d, has_pose_3d):\n",
    "        \"\"\"Compute 3D keypoint loss for the examples that 3D keypoint annotations are available.\n",
    "        The loss is weighted by the confidence.\n",
    "        \"\"\"\n",
    "        pred_keypoints_3d = pred_keypoints_3d[:, 25:, :]\n",
    "        conf = gt_keypoints_3d[:, :, -1].unsqueeze(-1).clone()\n",
    "        gt_keypoints_3d = gt_keypoints_3d[:, :, :-1].clone()\n",
    "        gt_keypoints_3d = gt_keypoints_3d[has_pose_3d == 1]\n",
    "        conf = conf[has_pose_3d == 1]\n",
    "        pred_keypoints_3d = pred_keypoints_3d[has_pose_3d == 1]\n",
    "        if len(gt_keypoints_3d) > 0:\n",
    "            gt_pelvis = (gt_keypoints_3d[:, 2,:] + gt_keypoints_3d[:, 3,:]) / 2\n",
    "            gt_keypoints_3d = gt_keypoints_3d - gt_pelvis[:, None, :]\n",
    "            pred_pelvis = (pred_keypoints_3d[:, 2,:] + pred_keypoints_3d[:, 3,:]) / 2\n",
    "            pred_keypoints_3d = pred_keypoints_3d - pred_pelvis[:, None, :]\n",
    "            return (conf * self.criterion_keypoints(pred_keypoints_3d, gt_keypoints_3d)).mean()\n",
    "        else:\n",
    "            return torch.FloatTensor(1).fill_(0.).to(self.device)\n",
    "\n",
    "    def shape_loss(self, pred_vertices, gt_vertices, has_smpl):\n",
    "        \"\"\"Compute per-vertex loss on the shape for the examples that SMPL annotations are available.\"\"\"\n",
    "        pred_vertices_with_shape = pred_vertices[has_smpl == 1]\n",
    "        gt_vertices_with_shape = gt_vertices[has_smpl == 1]\n",
    "        if len(gt_vertices_with_shape) > 0:\n",
    "            return self.criterion_shape(pred_vertices_with_shape, gt_vertices_with_shape)\n",
    "        else:\n",
    "            return torch.FloatTensor(1).fill_(0.).to(self.device)\n",
    "\n",
    "    def smpl_losses(self, pred_rotmat, gt_pose, has_smpl): # Remove pred_betas and gt_betas\n",
    "        pred_rotmat_valid = pred_rotmat[has_smpl == 1]\n",
    "        gt_rotmat_valid = batch_rodrigues(gt_pose.view(-1,3)).view(-1, 24, 3, 3)[has_smpl == 1]\n",
    "        # pred_betas_valid = pred_betas[has_smpl == 1]\n",
    "        #gt_betas_valid = gt_betas[has_smpl == 1]\n",
    "        if len(pred_rotmat_valid) > 0:\n",
    "            loss_regr_pose = self.criterion_regr(pred_rotmat_valid, gt_rotmat_valid)\n",
    "            #loss_regr_betas = self.criterion_regr(pred_betas_valid, gt_betas_valid)\n",
    "        else:\n",
    "            loss_regr_pose = torch.FloatTensor(1).fill_(0.).to(self.device)\n",
    "            #loss_regr_betas = torch.FloatTensor(1).fill_(0.).to(self.device)\n",
    "        return loss_regr_pose # remove loss_regr_betas\n",
    "\n",
    "    def train_step(self, input_batch):\n",
    "        self.model.train()\n",
    "\n",
    "        # Get data from the batch\n",
    "        images = input_batch['img'] # input image\n",
    "        gt_keypoints_2d = input_batch['keypoints'] # 2D keypoints\n",
    "        gt_pose = input_batch['pose'] # SMPL pose parameters\n",
    "        gt_betas = input_batch['betas'] # SMPL beta parameters\n",
    "        gt_joints = input_batch['pose_3d'] # 3D pose\n",
    "        has_smpl = input_batch['has_smpl'].byte() # flag that indicates whether SMPL parameters are valid\n",
    "        has_pose_3d = input_batch['has_pose_3d'].byte() # flag that indicates whether 3D pose is valid\n",
    "        is_flipped = input_batch['is_flipped'] # flag that indicates whether image was flipped during data augmentation\n",
    "        rot_angle = input_batch['rot_angle'] # rotation angle used for data augmentation\n",
    "        dataset_name = input_batch['dataset_name'] # name of the dataset the image comes from\n",
    "        indices = input_batch['sample_index'] # index of example inside its dataset\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        # Get GT vertices and model joints\n",
    "        # Note that gt_model_joints is different from gt_joints as it comes from SMPL\n",
    "        gt_out = self.smpl(betas=gt_betas, body_pose=gt_pose[:,3:], global_orient=gt_pose[:,:3])\n",
    "        gt_model_joints = gt_out.joints\n",
    "        gt_vertices = gt_out.vertices\n",
    "\n",
    "        # Get current best fits from the dictionary # Commenting out all the optimized fits saved in the dictionary\n",
    "        #opt_pose, opt_betas = self.fits_dict[(dataset_name, indices.cpu(), rot_angle.cpu(), is_flipped.cpu())]\n",
    "        #opt_pose = opt_pose.to(self.device)\n",
    "        #opt_betas = opt_betas.to(self.device)\n",
    "        #opt_output = self.smpl(betas=gt_betas, body_pose=opt_pose[:,3:], global_orient=opt_pose[:,:3]) # Bypass the need for opt_betas by replacing it with gt_betas here\n",
    "        #opt_vertices = opt_output.vertices\n",
    "        #opt_joints = opt_output.joints\n",
    "\n",
    "\n",
    "        # De-normalize 2D keypoints from [-1,1] to pixel space\n",
    "        gt_keypoints_2d_orig = gt_keypoints_2d.clone()\n",
    "        gt_keypoints_2d_orig[:, :, :-1] = 0.5 * self.options.img_res * (gt_keypoints_2d_orig[:, :, :-1] + 1)\n",
    "\n",
    "        # Estimate camera translation given the model joints and 2D keypoints\n",
    "        # by minimizing a weighted least squares loss\n",
    "        #gt_cam_t = estimate_translation(gt_model_joints, gt_keypoints_2d_orig, focal_length=self.focal_length, img_size=self.options.img_res) # Respectfully commenting this out\n",
    "\n",
    "        #opt_cam_t = estimate_translation(opt_joints, gt_keypoints_2d_orig, focal_length=self.focal_length, img_size=self.options.img_res)\n",
    "\n",
    "\n",
    "        #opt_joint_loss = self.smplify.get_fitting_loss(opt_pose, gt_betas, opt_cam_t, # replace opt_betas with gt_betas\n",
    "        #                                               0.5 * self.options.img_res * torch.ones(batch_size, 2, device=self.device),\n",
    "        #                                               gt_keypoints_2d_orig).mean(dim=-1)\n",
    "\n",
    "        # Feed images in the network to predict camera and SMPL parameters\n",
    "        pred_rotmat, pred_camera = self.model(images, gt_betas) # Remove pred_betas, Feed betas into the network\n",
    "\n",
    "        pred_output = self.smpl(betas=gt_betas, body_pose=pred_rotmat[:,1:], global_orient=pred_rotmat[:,0].unsqueeze(1), pose2rot=False) # Replace pred_betas with gt_betas\n",
    "        pred_vertices = pred_output.vertices\n",
    "        pred_joints = pred_output.joints\n",
    "\n",
    "        # Convert Weak Perspective Camera [s, tx, ty] to camera translation [tx, ty, tz] in 3D given the bounding box size\n",
    "        # This camera translation can be used in a full perspective projection\n",
    "        pred_cam_t = torch.stack([pred_camera[:,1],\n",
    "                                  pred_camera[:,2],\n",
    "                                  2*self.focal_length/(self.options.img_res * pred_camera[:,0] +1e-9)],dim=-1)\n",
    "\n",
    "\n",
    "        camera_center = torch.zeros(batch_size, 2, device=self.device)\n",
    "        pred_keypoints_2d = perspective_projection(pred_joints,\n",
    "                                                   rotation=torch.eye(3, device=self.device).unsqueeze(0).expand(batch_size, -1, -1),\n",
    "                                                   translation=pred_cam_t,\n",
    "                                                   focal_length=self.focal_length,\n",
    "                                                   camera_center=camera_center)\n",
    "        # Normalize keypoints to [-1,1]\n",
    "        pred_keypoints_2d = pred_keypoints_2d / (self.options.img_res / 2.)\n",
    "\n",
    "        if self.options.run_smplify:\n",
    "\n",
    "            # Convert predicted rotation matrices to axis-angle\n",
    "            pred_rotmat_hom = torch.cat([pred_rotmat.detach().view(-1, 3, 3).detach(), torch.tensor([0,0,1], dtype=torch.float32,\n",
    "                device=self.device).view(1, 3, 1).expand(batch_size * 24, -1, -1)], dim=-1)\n",
    "            pred_pose = rotation_matrix_to_angle_axis(pred_rotmat_hom).contiguous().view(batch_size, -1)\n",
    "            # tgm.rotation_matrix_to_angle_axis returns NaN for 0 rotation, so manually hack it\n",
    "            pred_pose[torch.isnan(pred_pose)] = 0.0\n",
    "\n",
    "            # Run SMPLify optimization starting from the network prediction\n",
    "            new_opt_vertices, new_opt_joints,\\\n",
    "            new_opt_pose, new_opt_betas,\\\n",
    "            new_opt_cam_t, new_opt_joint_loss = self.smplify(\n",
    "                                        pred_pose.detach(), gt_betas.detach(), # Replace pred_betas.detach() with gt_betas\n",
    "                                        pred_cam_t.detach(),\n",
    "                                        0.5 * self.options.img_res * torch.ones(batch_size, 2, device=self.device),\n",
    "                                        gt_keypoints_2d_orig)\n",
    "            new_opt_joint_loss = new_opt_joint_loss.mean(dim=-1)\n",
    "\n",
    "            # Will update the dictionary for the examples where the new loss is less than the current one\n",
    "            #update = (new_opt_joint_loss < opt_joint_loss)\n",
    "            \n",
    "\n",
    "            #opt_joint_loss[update] = new_opt_joint_loss[update]\n",
    "            #opt_vertices[update, :] = new_opt_vertices[update, :]\n",
    "            #opt_joints[update, :] = new_opt_joints[update, :]\n",
    "            #opt_pose[update, :] = new_opt_pose[update, :]\n",
    "            #opt_betas[update, :] = new_opt_betas[update, :]\n",
    "            #opt_cam_t[update, :] = new_opt_cam_t[update, :]\n",
    "\n",
    "\n",
    "            #self.fits_dict[(dataset_name, indices.cpu(), rot_angle.cpu(), is_flipped.cpu(), update.cpu())] = (opt_pose.cpu(), opt_betas.cpu())\n",
    "\n",
    "        else:\n",
    "            update = torch.zeros(batch_size, device=self.device).byte()\n",
    "\n",
    "        # Replace extreme betas with zero betas\n",
    "        #opt_betas[(opt_betas.abs() > 3).any(dim=-1)] = 0.\n",
    "\n",
    "        # Replace the optimized parameters with the ground truth parameters, if available\n",
    "        #opt_vertices[has_smpl, :, :] = gt_vertices[has_smpl, :, :]\n",
    "        #opt_cam_t[has_smpl, :] = gt_cam_t[has_smpl, :]\n",
    "        #opt_joints[has_smpl, :, :] = gt_model_joints[has_smpl, :, :]\n",
    "        #opt_pose[has_smpl, :] = gt_pose[has_smpl, :]\n",
    "        #opt_betas[has_smpl, :] = gt_betas[has_smpl, :]\n",
    "\n",
    "        # Assert whether a fit is valid by comparing the joint loss with the threshold\n",
    "        #valid_fit = (opt_joint_loss < self.options.smplify_threshold).to(self.device)\n",
    "        # Add the examples with GT parameters to the list of valid fits\n",
    "        #valid_fit = valid_fit | has_smpl\n",
    "\n",
    "        #opt_keypoints_2d = perspective_projection(opt_joints,\n",
    "        #                                          rotation=torch.eye(3, device=self.device).unsqueeze(0).expand(batch_size, -1, -1),\n",
    "        #                                          translation=opt_cam_t,\n",
    "        #                                          focal_length=self.focal_length,\n",
    "        #                                          camera_center=camera_center)\n",
    "\n",
    "\n",
    "        #opt_keypoints_2d = opt_keypoints_2d / (self.options.img_res / 2.)\n",
    "\n",
    "\n",
    "        # Compute loss on SMPL parameters\n",
    "        loss_regr_pose = self.smpl_losses(pred_rotmat, gt_pose, 1) # Remove loss_regr_betas, pred_betas, and opt_betas, replace opt_pose with gt_pose and valid_fit with 1\n",
    "\n",
    "        # Compute 2D reprojection loss for the keypoints\n",
    "        loss_keypoints = self.keypoint_loss(pred_keypoints_2d, gt_keypoints_2d,\n",
    "                                            self.options.openpose_train_weight,\n",
    "                                            self.options.gt_train_weight)\n",
    "\n",
    "        # Compute 3D keypoint loss\n",
    "        loss_keypoints_3d = self.keypoint_3d_loss(pred_joints, gt_joints, has_pose_3d)\n",
    "\n",
    "        # Per-vertex loss for the shape\n",
    "        loss_shape = self.shape_loss(pred_vertices, gt_vertices, 1) # replace opt_vertices with gt_vertices, valid_fit with 1\n",
    "\n",
    "        # Compute total loss\n",
    "        # The last component is a loss that forces the network to predict positive depth values\n",
    "        loss = self.options.shape_loss_weight * loss_shape +\\\n",
    "               self.options.keypoint_loss_weight * loss_keypoints +\\\n",
    "               self.options.keypoint_loss_weight * loss_keypoints_3d +\\\n",
    "               self.options.pose_loss_weight * loss_regr_pose +\\\n",
    "               ((torch.exp(-pred_camera[:,0]*10)) ** 2 ).mean()\n",
    "        loss *= 60\n",
    "        # Remove self.options.beta_loss_weight * loss_regr_betas +\\\n",
    "\n",
    "        # Do backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Pack output arguments for tensorboard logging\n",
    "        output = {'pred_vertices': pred_vertices.detach(),\n",
    "                  #'opt_vertices': opt_vertices, # Remove optimized values\n",
    "                  'pred_cam_t': pred_cam_t.detach()\n",
    "                  #'opt_cam_t': opt_cam_t}\n",
    "                 }\n",
    "        losses = {'loss': loss.detach().item(),\n",
    "                  'loss_keypoints': loss_keypoints.detach().item(),\n",
    "                  'loss_keypoints_3d': loss_keypoints_3d.detach().item(),\n",
    "                  'loss_regr_pose': loss_regr_pose.detach().item(),\n",
    "                  # 'loss_regr_betas': loss_regr_betas.detach().item(), # Remove loss_regr_betas\n",
    "                  'loss_shape': loss_shape.detach().item()}\n",
    "\n",
    "        return output, losses\n",
    "\n",
    "    def train_summaries(self, input_batch, output, losses):\n",
    "        images = input_batch['img']\n",
    "        images = images * torch.tensor([0.229, 0.224, 0.225], device=images.device).reshape(1,3,1,1)\n",
    "        images = images + torch.tensor([0.485, 0.456, 0.406], device=images.device).reshape(1,3,1,1)\n",
    "\n",
    "        pred_vertices = output['pred_vertices']\n",
    "        #opt_vertices = output['opt_vertices']\n",
    "        pred_cam_t = output['pred_cam_t']\n",
    "        #opt_cam_t = output['opt_cam_t']\n",
    "        images_pred = self.renderer.visualize_tb(pred_vertices, pred_cam_t, images)\n",
    "        #images_opt = self.renderer.visualize_tb(opt_vertices, opt_cam_t, images)\n",
    "        self.summary_writer.add_image('pred_shape', images_pred, self.step_count)\n",
    "        #self.summary_writer.add_image('opt_shape', images_opt, self.step_count)\n",
    "        for loss_name, val in losses.items():\n",
    "            self.summary_writer.add_scalar(loss_name, val, self.step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
