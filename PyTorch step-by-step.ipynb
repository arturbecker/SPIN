{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a model, there are two initialization steps:\n",
    "\n",
    "    Random initialization of parameters/weights (we have only two, a and b) — lines 3 and 4;\n",
    "    Initialization of hyper-parameters (in our case, only learning rate and number of epochs) — lines 9 and 11;\n",
    "\n",
    "Make sure to always initialize your random seed to ensure reproducibility of your results. As usual, the random seed is 42, the least random of all random seeds one could possibly choose :-)\n",
    "\n",
    "For each epoch, there are four training steps:\n",
    "\n",
    "    Compute model’s predictions — this is the forward pass — line 15;\n",
    "    Compute the loss, using predictions and and labels and the appropriate loss function for the task at hand — lines 18 and 20;\n",
    "    Compute the gradients for every parameter — lines 23 and 24;\n",
    "    Update the parameters — lines 27 and 28;\n",
    "\n",
    "Just keep in mind that, if you don’t use batch gradient descent (our example does),you’ll have to write an inner loop to perform the four training steps for either each individual point (stochastic) or n points (mini-batch). We’ll see a mini-batch example later down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating test and validation sets, it's important to\n",
    "# shuffle the array of indices. That allows us to randomize both\n",
    "# examples x and ground truth y in the same way:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using a sequential model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "lr = 1e-1\n",
    "epochs = 1000\n",
    "\n",
    "model = nn.Sequential(OrderedDict([('linear',nn.Linear(1,1,bias=True))])).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1,epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x_train_tensor)\n",
    "    error = y_train_tensor - y_hat\n",
    "    loss = (error**2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(model[0].weight) # That's how we can access the first layer, in case its unnamed\n",
    "print(model.linear.bias) # Alternatively, in case we named the layer\n",
    "print()\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression in Numpy (a linear regression takes the shape of y = a + bx + e\n",
    "\n",
    "a = np.random.randn(1,1)\n",
    "b = np.random.randn(1,1)\n",
    "\n",
    "lr = 1e-1\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(1,epochs):\n",
    "    # Forward pass - compute the model predictions\n",
    "    y_hat = a + b * x_train\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = 1/len(x_train)*np.sum((y_train-y_hat)**2)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    grad_a = -2*1/len(x_train)*np.sum(y_train-y_hat)\n",
    "    grad_b = -2*1/len(x_train)*np.sum(x_train*(y_train-y_hat))\n",
    "    \n",
    "    # Update the parameters\n",
    "    a = a-lr*grad_a\n",
    "    b = b-lr*grad_b\n",
    "    \n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "#from torchviz import make_dot\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "# and then we send them to the chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST\n",
    "# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n",
    "# since we want to apply gradient descent on these parameters, we need\n",
    "# to set REQUIRES_GRAD = TRUE\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(a, b)\n",
    "\n",
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just send them to device, right?\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(a, b)\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient...\n",
    "\n",
    "# THIRD\n",
    "# We can either create regular tensors and send them to the device (as we did with our data)\n",
    "a = torch.randn(1, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "a.requires_grad_()\n",
    "b.requires_grad_()\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimplementation of linear regression using Torch\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "lr = 1e-1\n",
    "epochs = 1000\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "a = torch.randn(1, requires_grad = True, device = device)\n",
    "b = torch.randn(1, requires_grad = True, device = device)\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    y_hat = a+b*x_train_tensor\n",
    "    error = y_train_tensor - y_hat\n",
    "    loss = (error**2).mean()\n",
    "    loss.backward()\n",
    "    print(a.grad)\n",
    "    print(b.grad)\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz\n",
    "\n",
    "torchviz.make_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset\n",
    "# Notice that for simple dataset comprised of two tensors, the pre-built\n",
    "# TensorDataset class is already enough\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x = x_train\n",
    "        self.y = y_train\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "    \n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])\n",
    "\n",
    "#train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "#print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data loader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[1.9625]], device='cuda:0')), ('linear.bias', tensor([1.0147], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Putting it all together: Dataset class, dataloader, splitting,\n",
    "# and linear regression using a sequential model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x = x_train\n",
    "        self.y = y_train\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "x_tensor = torch.from_numpy(x).float() # Since we are building the dataset, let's not send it to the GPU.\n",
    "y_tensor = torch.from_numpy(y).float() # In a real-world scenario, thhis is advisable to save GPU RAM\n",
    "    \n",
    "dataset = CustomDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_data, eval_data = random_split(dataset, [80,20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16)\n",
    "eval_loader = DataLoader(dataset=eval_data, batch_size=20)\n",
    "\n",
    "def make_train_step(optimizer, loss_fn, model):\n",
    "    def train_step(x, y):\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        y_hat = model(x)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "    return train_step\n",
    "\n",
    "lr = 1e-1\n",
    "epochs = 1000\n",
    "\n",
    "model = nn.Sequential(OrderedDict([('linear',nn.Linear(1,1,bias=True))])).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "losses = []\n",
    "\n",
    "train_step = make_train_step(optimizer, loss_fn, model)\n",
    "\n",
    "for epoch in range(1,epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[1.9625]], device='cuda:0')), ('linear.bias', tensor([1.0147], device='cuda:0'))])\n",
      "\n",
      "Mean error: 0.014371141255949741\n"
     ]
    }
   ],
   "source": [
    "# Evalution (basically the same as training, but without computing\n",
    "# gradients and updating parameters\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in eval_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        model.eval()\n",
    "        y_hat = model(x_batch)\n",
    "        val_loss = loss_fn(y_batch,y_hat)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "print(model.state_dict())\n",
    "print()\n",
    "print(\"Mean error: \"+str(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
