{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script can be used to evaluate a trained model on 3D pose/shape and masks/part segmentation. You first need to download the datasets and preprocess them.\n",
    "Example usage:\n",
    "```\n",
    "python3 eval.py --checkpoint=data/model_checkpoint.pt --dataset=h36m-p1 --log_freq=20\n",
    "```\n",
    "Running the above command will compute the MPJPE and Reconstruction Error on the Human3.6M dataset (Protocol I). The ```--dataset``` option can take different values based on the type of evaluation you want to perform:\n",
    "1. Human3.6M Protocol 1 ```--dataset=h36m-p1```\n",
    "2. Human3.6M Protocol 2 ```--dataset=h36m-p2```\n",
    "3. 3DPW ```--dataset=3dpw```\n",
    "4. LSP ```--dataset=lsp```\n",
    "5. MPI-INF-3DHP ```--dataset=mpi-inf-3dhp```\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import torchgeometry as tgm\n",
    "\n",
    "import config\n",
    "import constants\n",
    "from models import hmr, SMPL\n",
    "from datasets import BaseDataset\n",
    "from utils.imutils import uncrop\n",
    "from utils.pose_utils import reconstruction_error\n",
    "from utils.part_utils import PartRenderer\n",
    "\n",
    "from orientation_evaluation import orientation_evaluation\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# Define command-line arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--checkpoint', default=None, help='Path to network checkpoint')\n",
    "parser.add_argument('--dataset', default='h36m-p1', choices=['h36m-p1', 'h36m-p2', 'lsp', '3dpw', 'mpi-inf-3dhp'], help='Choose evaluation dataset')\n",
    "parser.add_argument('--log_freq', default=50, type=int, help='Frequency of printing intermediate results')\n",
    "parser.add_argument('--batch_size', default=32, help='Batch size for testing')\n",
    "parser.add_argument('--shuffle', default=False, action='store_true', help='Shuffle data')\n",
    "parser.add_argument('--num_workers', default=8, type=int, help='Number of processes for data loading')\n",
    "parser.add_argument('--result_file', default=None, help='If set, save detections to a .npz file')\n",
    "\n",
    "def run_evaluation(model, dataset_name, dataset, result_file,\n",
    "                   batch_size=32, img_res=224, \n",
    "                   num_workers=32, shuffle=False, log_freq=50):\n",
    "    \"\"\"Run evaluation on the datasets and metrics we report in the paper. \"\"\"\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # Transfer model to the GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Load SMPL model\n",
    "    smpl_neutral = SMPL(config.SMPL_MODEL_DIR,\n",
    "                        create_transl=False).to(device)\n",
    "    smpl_male = SMPL(config.SMPL_MODEL_DIR,\n",
    "                     gender='male',\n",
    "                     create_transl=False).to(device)\n",
    "    smpl_female = SMPL(config.SMPL_MODEL_DIR,\n",
    "                       gender='female',\n",
    "                       create_transl=False).to(device)\n",
    "    \n",
    "    renderer = PartRenderer()\n",
    "    \n",
    "    # Regressor for H36m joints\n",
    "    J_regressor = torch.from_numpy(np.load(config.JOINT_REGRESSOR_H36M)).float()\n",
    "    \n",
    "    save_results = result_file is not None\n",
    "    # Disable shuffling if you want to save the results\n",
    "    if save_results:\n",
    "        shuffle=False\n",
    "    # Create dataloader for the dataset\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    \n",
    "    # Pose metrics\n",
    "    # MPJPE and Reconstruction error for the non-parametric and parametric shapes\n",
    "    mpjpe = np.zeros(len(dataset))\n",
    "    recon_err = np.zeros(len(dataset))\n",
    "    mpjpe_smpl = np.zeros(len(dataset))\n",
    "    recon_err_smpl = np.zeros(len(dataset))\n",
    "    \n",
    "    #Including per joint position error:\n",
    "    pjpe = torch.zeros(len(dataset), 14)\n",
    "    \n",
    "    # Including mean per joint angular error (reduced and per part)\n",
    "    mpjae = np.zeros(len(dataset))\n",
    "    mpjae_per_part = torch.zeros(len(dataset), 24, 3)\n",
    "\n",
    "    # Shape metrics\n",
    "    # Mean per-vertex error\n",
    "    shape_err = np.zeros(len(dataset))\n",
    "    shape_err_smpl = np.zeros(len(dataset))\n",
    "\n",
    "    # Mask and part metrics\n",
    "    # Accuracy\n",
    "    accuracy = 0.\n",
    "    parts_accuracy = 0.\n",
    "    # True positive, false positive and false negative\n",
    "    tp = np.zeros((2,1))\n",
    "    fp = np.zeros((2,1))\n",
    "    fn = np.zeros((2,1))\n",
    "    parts_tp = np.zeros((7,1))\n",
    "    parts_fp = np.zeros((7,1))\n",
    "    parts_fn = np.zeros((7,1))\n",
    "    # Pixel count accumulators\n",
    "    pixel_count = 0\n",
    "    parts_pixel_count = 0\n",
    "\n",
    "    # Store SMPL parameters\n",
    "    smpl_pose = np.zeros((len(dataset), 72))\n",
    "    smpl_betas = np.zeros((len(dataset), 10))\n",
    "    smpl_camera = np.zeros((len(dataset), 3))\n",
    "    pred_joints = np.zeros((len(dataset), 17, 3))\n",
    "\n",
    "    eval_pose = False\n",
    "    eval_masks = False\n",
    "    eval_parts = False\n",
    "    eval_orientation = False # Adding the orientation parameter\n",
    "    # Choose appropriate evaluation for each dataset\n",
    "    if dataset_name == 'h36m-p1' or dataset_name == 'h36m-p2' or dataset_name == 'mpi-inf-3dhp':\n",
    "        eval_pose = True\n",
    "    elif dataset_name == 'lsp':\n",
    "        eval_masks = True\n",
    "        eval_parts = True\n",
    "        annot_path = config.DATASET_FOLDERS['upi-s1h']\n",
    "    elif dataset_name == '3dpw':\n",
    "        eval_orientation = True\n",
    "        eval_pose = True\n",
    "        \n",
    "\n",
    "    joint_mapper_h36m = constants.H36M_TO_J17 if dataset_name == 'mpi-inf-3dhp' else constants.H36M_TO_J14\n",
    "    joint_mapper_gt = constants.J24_TO_J17 if dataset_name == 'mpi-inf-3dhp' else constants.J24_TO_J14\n",
    "    # Iterate over the entire dataset\n",
    "    for step, batch in enumerate(tqdm(data_loader, desc='Eval', total=len(data_loader))):\n",
    "        # Get ground truth annotations from the batch\n",
    "        gt_pose = batch['pose'].to(device)\n",
    "        gt_betas = batch['betas'].to(device)\n",
    "        gt_vertices = smpl_neutral(betas=gt_betas, body_pose=gt_pose[:, 3:], global_orient=gt_pose[:, :3]).vertices\n",
    "        images = batch['img'].to(device)\n",
    "        gender = batch['gender'].to(device)\n",
    "        curr_batch_size = images.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_rotmat, pred_betas, pred_camera = model(images)\n",
    "            pred_output = smpl_neutral(betas=pred_betas, body_pose=pred_rotmat[:,1:], global_orient=pred_rotmat[:,0].unsqueeze(1), pose2rot=False)\n",
    "            pred_vertices = pred_output.vertices\n",
    "\n",
    "        if save_results:\n",
    "            rot_pad = torch.tensor([0,0,1], dtype=torch.float32, device=device).view(1,3,1)\n",
    "            rotmat = torch.cat((pred_rotmat.view(-1, 3, 3), rot_pad.expand(curr_batch_size * 24, -1, -1)), dim=-1)\n",
    "            pred_pose = tgm.rotation_matrix_to_angle_axis(rotmat).contiguous().view(-1, 72)\n",
    "            smpl_pose[step * batch_size:step * batch_size + curr_batch_size, :] = pred_pose.cpu().numpy()\n",
    "            smpl_betas[step * batch_size:step * batch_size + curr_batch_size, :]  = pred_betas.cpu().numpy()\n",
    "            smpl_camera[step * batch_size:step * batch_size + curr_batch_size, :]  = pred_camera.cpu().numpy()\n",
    "        \n",
    "        # Orientation evaluation\n",
    "        orientation_error_per_part, orientation_error, orientation_error_new = \\\n",
    "        orientation_evaluation(gt_pose, pred_rotmat, batch_size, curr_batch_size, step)\n",
    "        \n",
    "        mpjae[step * batch_size:step * batch_size + curr_batch_size] = orientation_error_new\n",
    "        mpjae_per_part[step*batch_size : step*batch_size + curr_batch_size] = orientation_error_per_part\n",
    "            \n",
    "        # 3D pose evaluation\n",
    "        if eval_pose:\n",
    "            # Regressor broadcasting\n",
    "            J_regressor_batch = J_regressor[None, :].expand(pred_vertices.shape[0], -1, -1).to(device)\n",
    "            # Get 14 ground truth joints\n",
    "            if 'h36m' in dataset_name or 'mpi-inf' in dataset_name:\n",
    "                gt_keypoints_3d = batch['pose_3d'].cuda()\n",
    "                gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_gt, :-1]\n",
    "            # For 3DPW get the 14 common joints from the rendered shape\n",
    "            else:\n",
    "                gt_vertices = smpl_male(global_orient=gt_pose[:,:3], body_pose=gt_pose[:,3:], betas=gt_betas).vertices \n",
    "                gt_vertices_female = smpl_female(global_orient=gt_pose[:,:3], body_pose=gt_pose[:,3:], betas=gt_betas).vertices \n",
    "                gt_vertices[gender==1, :, :] = gt_vertices_female[gender==1, :, :]\n",
    "                gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices) # torch.Size([32, 17, 3]) # This returns 17 joints\n",
    "                print(\"gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices)\", gt_keypoints_3d.shape, gt_keypoints_3d)\n",
    "                gt_pelvis = gt_keypoints_3d[:, [0],:].clone()\n",
    "                gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :] # torch.Size([32, 14, 3]) # But only 14 are used, the joint_mapper is [6, 5, 4, 1, 2, 3, 16, 15, 14, 11, 12, 13, 8, 10]\n",
    "                print(\"joint_mapper_h36m\", joint_mapper_h36m) \n",
    "                print(\"gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :]\", gt_keypoints_3d.shape, gt_keypoints_3d)\n",
    "                gt_keypoints_3d = gt_keypoints_3d - gt_pelvis\n",
    "                print(\"gt_keypoints_3d = gt_keypoints_3d - gt_pelvis\", gt_keypoints_3d.shape, gt_keypoints_3d)\n",
    "\n",
    "\n",
    "            # Get 14 predicted joints from the mesh\n",
    "            pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices)\n",
    "            print(\"pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices)\", pred_keypoints_3d.shape, pred_keypoints_3d)\n",
    "            if save_results:\n",
    "                pred_joints[step * batch_size:step * batch_size + curr_batch_size, :, :]  = pred_keypoints_3d.cpu().numpy()\n",
    "            pred_pelvis = pred_keypoints_3d[:, [0],:].clone()\n",
    "            pred_keypoints_3d = pred_keypoints_3d[:, joint_mapper_h36m, :]\n",
    "            pred_keypoints_3d = pred_keypoints_3d - pred_pelvis # [32, 14, 3]\n",
    "            \n",
    "            # Absolute error (MPJPE)\n",
    "            error = torch.sqrt(((pred_keypoints_3d - gt_keypoints_3d) ** 2).sum(dim=-1)).mean(dim=-1).cpu().numpy()\n",
    "            \n",
    "            mpjpe[step * batch_size:step * batch_size + curr_batch_size] = error\n",
    "            \n",
    "            # Per part error\n",
    "            per_part_error = torch.sqrt(((pred_keypoints_3d - gt_keypoints_3d) ** 2).sum(dim=-1)) # Not really necessary to send it to cpu as a np array for now\n",
    "            \n",
    "            pjpe[step * batch_size:step * batch_size + curr_batch_size] = per_part_error*1000 # Converting from meters to milimeters\n",
    "            \n",
    "            # Reconstuction_error\n",
    "            r_error = reconstruction_error(pred_keypoints_3d.cpu().numpy(), gt_keypoints_3d.cpu().numpy(), reduction=None)\n",
    "            recon_err[step * batch_size:step * batch_size + curr_batch_size] = r_error\n",
    "\n",
    "\n",
    "        # If mask or part evaluation, render the mask and part images\n",
    "        if eval_masks or eval_parts:\n",
    "            mask, parts = renderer(pred_vertices, pred_camera)\n",
    "\n",
    "        # Mask evaluation (for LSP)\n",
    "        if eval_masks:\n",
    "            center = batch['center'].cpu().numpy()\n",
    "            scale = batch['scale'].cpu().numpy()\n",
    "            # Dimensions of original image\n",
    "            orig_shape = batch['orig_shape'].cpu().numpy()\n",
    "            for i in range(curr_batch_size):\n",
    "                # After rendering, convert imate back to original resolution\n",
    "                pred_mask = uncrop(mask[i].cpu().numpy(), center[i], scale[i], orig_shape[i]) > 0\n",
    "                # Load gt mask\n",
    "                gt_mask = cv2.imread(os.path.join(annot_path, batch['maskname'][i]), 0) > 0\n",
    "                # Evaluation consistent with the original UP-3D code\n",
    "                accuracy += (gt_mask == pred_mask).sum()\n",
    "                pixel_count += np.prod(np.array(gt_mask.shape))\n",
    "                for c in range(2):\n",
    "                    cgt = gt_mask == c\n",
    "                    cpred = pred_mask == c\n",
    "                    tp[c] += (cgt & cpred).sum()\n",
    "                    fp[c] +=  (~cgt & cpred).sum()\n",
    "                    fn[c] +=  (cgt & ~cpred).sum()\n",
    "                f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "        # Part evaluation (for LSP)\n",
    "        if eval_parts:\n",
    "            center = batch['center'].cpu().numpy()\n",
    "            scale = batch['scale'].cpu().numpy()\n",
    "            orig_shape = batch['orig_shape'].cpu().numpy()\n",
    "            for i in range(curr_batch_size):\n",
    "                pred_parts = uncrop(parts[i].cpu().numpy().astype(np.uint8), center[i], scale[i], orig_shape[i])\n",
    "                # Load gt part segmentation\n",
    "                gt_parts = cv2.imread(os.path.join(annot_path, batch['partname'][i]), 0)\n",
    "                # Evaluation consistent with the original UP-3D code\n",
    "                # 6 parts + background\n",
    "                for c in range(7):\n",
    "                   cgt = gt_parts == c\n",
    "                   cpred = pred_parts == c\n",
    "                   cpred[gt_parts == 255] = 0\n",
    "                   parts_tp[c] += (cgt & cpred).sum()\n",
    "                   parts_fp[c] +=  (~cgt & cpred).sum()\n",
    "                   parts_fn[c] +=  (cgt & ~cpred).sum()\n",
    "                gt_parts[gt_parts == 255] = 0\n",
    "                pred_parts[pred_parts == 255] = 0\n",
    "                parts_f1 = 2 * parts_tp / (2 * parts_tp + parts_fp + parts_fn)\n",
    "                parts_accuracy += (gt_parts == pred_parts).sum()\n",
    "                parts_pixel_count += np.prod(np.array(gt_parts.shape))\n",
    "\n",
    "        # Print intermediate results during evaluation\n",
    "        if step % log_freq == log_freq - 1:\n",
    "            if eval_pose:\n",
    "                print('MPJPE: ' + str(1000 * mpjpe[:step * batch_size].mean()))\n",
    "                print('Reconstruction Error: ' + str(1000 * recon_err[:step * batch_size].mean()))\n",
    "                print()\n",
    "            if eval_masks:\n",
    "                print('Accuracy: ', accuracy / pixel_count)\n",
    "                print('F1: ', f1.mean())\n",
    "                print()\n",
    "            if eval_parts:\n",
    "                print('Parts Accuracy: ', parts_accuracy / parts_pixel_count)\n",
    "                print('Parts F1 (BG): ', parts_f1[[0,1,2,3,4,5,6]].mean())\n",
    "                print()\n",
    "            if eval_orientation:\n",
    "                print('Orientation error: ' + str(mpjae[:step * batch_size].mean()))\n",
    "\n",
    "    # Save reconstructions to a file for further processing\n",
    "    if save_results:\n",
    "        np.savez(result_file, pred_joints=pred_joints, pose=smpl_pose, betas=smpl_betas, camera=smpl_camera)\n",
    "    # Print final results during evaluation\n",
    "    print('*** Final Results ***')\n",
    "    print()\n",
    "    if eval_pose:\n",
    "        print('MPJPE: ' + str(1000 * mpjpe.mean()))\n",
    "        print('Reconstruction Error: ' + str(1000 * recon_err.mean()))\n",
    "        print()\n",
    "        #torch.save(pjpe, 'pjpe.pt')\n",
    "    if eval_masks:\n",
    "        print('Accuracy: ', accuracy / pixel_count)\n",
    "        print('F1: ', f1.mean())\n",
    "        print()\n",
    "    if eval_parts:\n",
    "        print('Parts Accuracy: ', parts_accuracy / parts_pixel_count)\n",
    "        print('Parts F1 (BG): ', parts_f1[[0,1,2,3,4,5,6]].mean())\n",
    "        print()\n",
    "    if eval_orientation:\n",
    "        print('Orientation Error: ' + str(mpjae.mean()))\n",
    "        print('Orientation Error per part: ', mpjae_per_part)\n",
    "        #torch.save(mpjae_per_part, 'mpjae_per_part.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval:   0%|          | 0/1110 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices) torch.Size([32, 17, 3]) tensor([[[-4.4040e-03, -2.6157e-01,  3.3186e-02],\n",
      "         [ 1.3561e-01, -2.5373e-01,  3.8715e-02],\n",
      "         [ 9.4056e-02,  1.7967e-01,  1.3922e-01],\n",
      "         ...,\n",
      "         [-1.4890e-01, -6.8714e-01, -2.8414e-02],\n",
      "         [-2.3759e-01, -4.2739e-01,  7.7644e-03],\n",
      "         [-2.0387e-01, -1.8171e-01,  1.2894e-02]],\n",
      "\n",
      "        [[-4.3842e-03, -2.6167e-01,  3.2996e-02],\n",
      "         [ 1.3568e-01, -2.5387e-01,  3.8124e-02],\n",
      "         [ 9.5192e-02,  1.7975e-01,  1.3809e-01],\n",
      "         ...,\n",
      "         [-1.4957e-01, -6.8678e-01, -2.7853e-02],\n",
      "         [-2.3817e-01, -4.2651e-01,  6.0912e-03],\n",
      "         [-2.0659e-01, -1.8038e-01,  1.0852e-02]],\n",
      "\n",
      "        [[-4.3657e-03, -2.6179e-01,  3.2999e-02],\n",
      "         [ 1.3577e-01, -2.5415e-01,  3.7902e-02],\n",
      "         [ 9.6454e-02,  1.7967e-01,  1.3741e-01],\n",
      "         ...,\n",
      "         [-1.5024e-01, -6.8669e-01, -2.6847e-02],\n",
      "         [-2.4047e-01, -4.2656e-01,  6.4115e-03],\n",
      "         [-2.1082e-01, -1.8007e-01,  1.0837e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.3040e-03, -2.6429e-01,  3.6928e-02],\n",
      "         [ 1.3742e-01, -2.5228e-01,  1.3047e-02],\n",
      "         [ 1.0529e-01,  1.8719e-01, -4.2280e-02],\n",
      "         ...,\n",
      "         [-1.3817e-01, -6.7942e-01, -1.5956e-02],\n",
      "         [-2.3779e-01, -4.2437e-01,  4.1691e-02],\n",
      "         [-2.1441e-01, -1.7721e-01,  2.2711e-02]],\n",
      "\n",
      "        [[-9.1445e-04, -2.6446e-01,  3.6561e-02],\n",
      "         [ 1.3665e-01, -2.5195e-01,  7.5299e-03],\n",
      "         [ 1.0398e-01,  1.8658e-01, -4.5553e-02],\n",
      "         ...,\n",
      "         [-1.3590e-01, -6.8021e-01, -1.3932e-02],\n",
      "         [-2.3612e-01, -4.2632e-01,  4.8604e-02],\n",
      "         [-2.1376e-01, -1.7946e-01,  2.6024e-02]],\n",
      "\n",
      "        [[-4.8403e-04, -2.6448e-01,  3.5972e-02],\n",
      "         [ 1.3557e-01, -2.5057e-01,  1.9464e-03],\n",
      "         [ 1.0428e-01,  1.8707e-01, -4.4364e-02],\n",
      "         ...,\n",
      "         [-1.3216e-01, -6.8148e-01, -1.0502e-02],\n",
      "         [-2.3396e-01, -4.2930e-01,  5.7292e-02],\n",
      "         [-2.1301e-01, -1.8265e-01,  3.0743e-02]]], device='cuda:0')\n",
      "joint_mapper_h36m [6, 5, 4, 1, 2, 3, 16, 15, 14, 11, 12, 13, 8, 10]\n",
      "gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :] torch.Size([32, 14, 3]) tensor([[[-7.1361e-02,  5.5840e-01,  3.0648e-01],\n",
      "         [-7.6798e-02,  1.6518e-01,  1.3472e-01],\n",
      "         [-1.4504e-01, -2.6817e-01,  2.8451e-02],\n",
      "         ...,\n",
      "         [ 2.1594e-01, -1.7611e-01,  1.4366e-02],\n",
      "         [-3.1639e-03, -7.4294e-01, -4.8289e-02],\n",
      "         [-1.2465e-03, -9.3463e-01, -1.0913e-01]],\n",
      "\n",
      "        [[-7.2651e-02,  5.5553e-01,  3.1332e-01],\n",
      "         [-7.8468e-02,  1.6528e-01,  1.3472e-01],\n",
      "         [-1.4504e-01, -2.6820e-01,  2.8621e-02],\n",
      "         ...,\n",
      "         [ 2.1350e-01, -1.7452e-01,  1.4662e-02],\n",
      "         [-3.5586e-03, -7.4266e-01, -4.7450e-02],\n",
      "         [-1.2683e-03, -9.3611e-01, -1.0562e-01]],\n",
      "\n",
      "        [[-7.3333e-02,  5.5259e-01,  3.1972e-01],\n",
      "         [-8.0484e-02,  1.6548e-01,  1.3440e-01],\n",
      "         [-1.4509e-01, -2.6818e-01,  2.8849e-02],\n",
      "         ...,\n",
      "         [ 2.1270e-01, -1.7367e-01,  1.6389e-02],\n",
      "         [-3.9240e-03, -7.4253e-01, -4.6319e-02],\n",
      "         [-1.1279e-03, -9.3717e-01, -1.0231e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.9807e-02,  5.2674e-01,  3.7722e-01],\n",
      "         [-7.5433e-02,  1.6090e-01,  1.5324e-01],\n",
      "         [-1.3968e-01, -2.7595e-01,  6.0622e-02],\n",
      "         ...,\n",
      "         [ 2.1087e-01, -1.7172e-01, -1.3630e-01],\n",
      "         [-1.0988e-03, -7.3207e-01, -7.3656e-02],\n",
      "         [-1.4593e-02, -9.2159e-01, -1.4568e-01]],\n",
      "\n",
      "        [[-4.6639e-02,  5.2335e-01,  3.8479e-01],\n",
      "         [-7.4693e-02,  1.6056e-01,  1.5631e-01],\n",
      "         [-1.3819e-01, -2.7666e-01,  6.5467e-02],\n",
      "         ...,\n",
      "         [ 2.0933e-01, -1.7063e-01, -1.4604e-01],\n",
      "         [-1.2964e-03, -7.3169e-01, -7.7340e-02],\n",
      "         [-1.6871e-02, -9.2034e-01, -1.5086e-01]],\n",
      "\n",
      "        [[-4.5917e-02,  5.1979e-01,  3.9318e-01],\n",
      "         [-7.5211e-02,  1.5931e-01,  1.6122e-01],\n",
      "         [-1.3635e-01, -2.7817e-01,  6.9929e-02],\n",
      "         ...,\n",
      "         [ 2.0730e-01, -1.6804e-01, -1.5433e-01],\n",
      "         [-2.0179e-04, -7.3139e-01, -7.9965e-02],\n",
      "         [-1.8872e-02, -9.1979e-01, -1.5327e-01]]], device='cuda:0')\n",
      "gt_keypoints_3d = gt_keypoints_3d - gt_pelvis torch.Size([32, 14, 3]) tensor([[[-6.6957e-02,  8.1997e-01,  2.7329e-01],\n",
      "         [-7.2394e-02,  4.2675e-01,  1.0153e-01],\n",
      "         [-1.4063e-01, -6.5941e-03, -4.7352e-03],\n",
      "         ...,\n",
      "         [ 2.2034e-01,  8.5460e-02, -1.8820e-02],\n",
      "         [ 1.2401e-03, -4.8137e-01, -8.1475e-02],\n",
      "         [ 3.1575e-03, -6.7305e-01, -1.4232e-01]],\n",
      "\n",
      "        [[-6.8267e-02,  8.1720e-01,  2.8032e-01],\n",
      "         [-7.4084e-02,  4.2694e-01,  1.0172e-01],\n",
      "         [-1.4066e-01, -6.5353e-03, -4.3755e-03],\n",
      "         ...,\n",
      "         [ 2.1788e-01,  8.7152e-02, -1.8335e-02],\n",
      "         [ 8.2554e-04, -4.8100e-01, -8.0446e-02],\n",
      "         [ 3.1159e-03, -6.7444e-01, -1.3862e-01]],\n",
      "\n",
      "        [[-6.8967e-02,  8.1439e-01,  2.8672e-01],\n",
      "         [-7.6118e-02,  4.2727e-01,  1.0140e-01],\n",
      "         [-1.4073e-01, -6.3881e-03, -4.1498e-03],\n",
      "         ...,\n",
      "         [ 2.1707e-01,  8.8120e-02, -1.6610e-02],\n",
      "         [ 4.4177e-04, -4.8074e-01, -7.9317e-02],\n",
      "         [ 3.2378e-03, -6.7538e-01, -1.3531e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.8503e-02,  7.9103e-01,  3.4030e-01],\n",
      "         [-7.4129e-02,  4.2519e-01,  1.1632e-01],\n",
      "         [-1.3838e-01, -1.1659e-02,  2.3694e-02],\n",
      "         ...,\n",
      "         [ 2.1217e-01,  9.2574e-02, -1.7322e-01],\n",
      "         [ 2.0525e-04, -4.6778e-01, -1.1058e-01],\n",
      "         [-1.3289e-02, -6.5730e-01, -1.8261e-01]],\n",
      "\n",
      "        [[-4.5725e-02,  7.8781e-01,  3.4823e-01],\n",
      "         [-7.3778e-02,  4.2502e-01,  1.1975e-01],\n",
      "         [-1.3727e-01, -1.2200e-02,  2.8906e-02],\n",
      "         ...,\n",
      "         [ 2.1024e-01,  9.3824e-02, -1.8260e-01],\n",
      "         [-3.8197e-04, -4.6723e-01, -1.1390e-01],\n",
      "         [-1.5957e-02, -6.5588e-01, -1.8742e-01]],\n",
      "\n",
      "        [[-4.5432e-02,  7.8427e-01,  3.5720e-01],\n",
      "         [-7.4727e-02,  4.2379e-01,  1.2525e-01],\n",
      "         [-1.3586e-01, -1.3692e-02,  3.3957e-02],\n",
      "         ...,\n",
      "         [ 2.0778e-01,  9.6439e-02, -1.9030e-01],\n",
      "         [ 2.8224e-04, -4.6691e-01, -1.1594e-01],\n",
      "         [-1.8388e-02, -6.5530e-01, -1.8924e-01]]], device='cuda:0')\n",
      "pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices) torch.Size([32, 17, 3]) tensor([[[-0.0068, -0.2879,  0.0420],\n",
      "         [ 0.1230, -0.2809,  0.0514],\n",
      "         [ 0.1019,  0.1383, -0.0547],\n",
      "         ...,\n",
      "         [-0.1617, -0.7285,  0.0148],\n",
      "         [-0.2471, -0.4593,  0.0111],\n",
      "         [-0.2142, -0.2435, -0.0892]],\n",
      "\n",
      "        [[-0.0069, -0.2875,  0.0417],\n",
      "         [ 0.1227, -0.2804,  0.0526],\n",
      "         [ 0.1012,  0.1392, -0.0527],\n",
      "         ...,\n",
      "         [-0.1616, -0.7266,  0.0080],\n",
      "         [-0.2455, -0.4571,  0.0065],\n",
      "         [-0.2130, -0.2399, -0.0911]],\n",
      "\n",
      "        [[-0.0066, -0.2879,  0.0421],\n",
      "         [ 0.1233, -0.2810,  0.0508],\n",
      "         [ 0.1007,  0.1377, -0.0574],\n",
      "         ...,\n",
      "         [-0.1621, -0.7279,  0.0135],\n",
      "         [-0.2472, -0.4587,  0.0120],\n",
      "         [-0.2168, -0.2415, -0.0863]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0035, -0.2909,  0.0435],\n",
      "         [ 0.1258, -0.2841,  0.0311],\n",
      "         [ 0.1031,  0.1257, -0.0969],\n",
      "         ...,\n",
      "         [-0.1598, -0.7308,  0.0451],\n",
      "         [-0.2449, -0.4618,  0.0584],\n",
      "         [-0.2353, -0.2475, -0.0488]],\n",
      "\n",
      "        [[-0.0031, -0.2888,  0.0426],\n",
      "         [ 0.1253, -0.2824,  0.0262],\n",
      "         [ 0.1035,  0.1262, -0.1012],\n",
      "         ...,\n",
      "         [-0.1552, -0.7282,  0.0532],\n",
      "         [-0.2398, -0.4599,  0.0684],\n",
      "         [-0.2297, -0.2471, -0.0399]],\n",
      "\n",
      "        [[-0.0024, -0.2871,  0.0402],\n",
      "         [ 0.1251, -0.2793,  0.0197],\n",
      "         [ 0.0979,  0.1351, -0.0927],\n",
      "         ...,\n",
      "         [-0.1519, -0.7258,  0.0452],\n",
      "         [-0.2346, -0.4564,  0.0647],\n",
      "         [-0.2245, -0.2430, -0.0452]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval:   0%|          | 1/1110 [00:27<8:25:00, 27.32s/it]\u001b[A\n",
      "Eval:   0%|          | 2/1110 [00:31<6:18:08, 20.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices) torch.Size([32, 17, 3]) tensor([[[-3.6191e-05, -2.6449e-01,  3.4947e-02],\n",
      "         [ 1.3403e-01, -2.4831e-01, -4.1961e-03],\n",
      "         [ 1.0712e-01,  1.8828e-01, -3.8759e-02],\n",
      "         ...,\n",
      "         [-1.2682e-01, -6.8291e-01, -6.3629e-03],\n",
      "         [-2.3079e-01, -4.3292e-01,  6.6794e-02],\n",
      "         [-2.1254e-01, -1.8623e-01,  3.8161e-02]],\n",
      "\n",
      "        [[ 2.9096e-04, -2.6443e-01,  3.3789e-02],\n",
      "         [ 1.3193e-01, -2.4636e-01, -1.0960e-02],\n",
      "         [ 1.1220e-01,  1.8946e-01, -2.9663e-02],\n",
      "         ...,\n",
      "         [-1.2152e-01, -6.8451e-01, -5.3881e-04],\n",
      "         [-2.2683e-01, -4.3657e-01,  7.7498e-02],\n",
      "         [-2.1302e-01, -1.8968e-01,  4.8513e-02]],\n",
      "\n",
      "        [[ 5.3468e-04, -2.6437e-01,  3.2913e-02],\n",
      "         [ 1.2958e-01, -2.4548e-01, -1.7779e-02],\n",
      "         [ 1.1664e-01,  1.9057e-01, -2.0151e-02],\n",
      "         ...,\n",
      "         [-1.1887e-01, -6.8643e-01,  6.9304e-03],\n",
      "         [-2.2339e-01, -4.4012e-01,  8.9928e-02],\n",
      "         [-2.1514e-01, -1.9291e-01,  6.1821e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.7594e-02, -2.5667e-01,  1.3470e-02],\n",
      "         [-2.1600e-02, -2.1072e-01, -1.1306e-01],\n",
      "         [ 2.9266e-02,  2.0939e-01,  2.0667e-02],\n",
      "         ...,\n",
      "         [ 3.6435e-02, -7.1113e-01, -1.0422e-02],\n",
      "         [ 7.0480e-02, -4.9757e-01,  1.6465e-01],\n",
      "         [-1.8893e-02, -2.8118e-01,  2.3948e-01]],\n",
      "\n",
      "        [[ 1.7492e-02, -2.5656e-01,  1.3521e-02],\n",
      "         [-2.4130e-02, -2.1232e-01, -1.1278e-01],\n",
      "         [ 3.3501e-02,  2.0884e-01,  1.5492e-02],\n",
      "         ...,\n",
      "         [ 3.7131e-02, -7.1130e-01, -5.9976e-03],\n",
      "         [ 7.1798e-02, -4.9611e-01,  1.6699e-01],\n",
      "         [-1.7593e-02, -2.7951e-01,  2.4150e-01]],\n",
      "\n",
      "        [[ 1.7746e-02, -2.5599e-01,  1.3714e-02],\n",
      "         [-2.5995e-02, -2.1363e-01, -1.1238e-01],\n",
      "         [ 3.1946e-02,  2.0893e-01,  1.0118e-02],\n",
      "         ...,\n",
      "         [ 4.5265e-02, -7.1033e-01, -1.4654e-03],\n",
      "         [ 7.8016e-02, -4.9260e-01,  1.6912e-01],\n",
      "         [-1.4804e-02, -2.7713e-01,  2.4300e-01]]], device='cuda:0')\n",
      "joint_mapper_h36m [6, 5, 4, 1, 2, 3, 16, 15, 14, 11, 12, 13, 8, 10]\n",
      "gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :] torch.Size([32, 14, 3]) tensor([[[-0.0475,  0.5153,  0.4037],\n",
      "         [-0.0764,  0.1569,  0.1684],\n",
      "         [-0.1341, -0.2806,  0.0741],\n",
      "         ...,\n",
      "         [ 0.2051, -0.1645, -0.1612],\n",
      "         [ 0.0022, -0.7311, -0.0823],\n",
      "         [-0.0204, -0.9196, -0.1543]],\n",
      "\n",
      "        [[-0.0500,  0.5104,  0.4150],\n",
      "         [-0.0781,  0.1548,  0.1754],\n",
      "         [-0.1315, -0.2825,  0.0786],\n",
      "         ...,\n",
      "         [ 0.2024, -0.1614, -0.1667],\n",
      "         [ 0.0041, -0.7312, -0.0830],\n",
      "         [-0.0233, -0.9201, -0.1532]],\n",
      "\n",
      "        [[-0.0503,  0.5039,  0.4278],\n",
      "         [-0.0808,  0.1534,  0.1813],\n",
      "         [-0.1288, -0.2832,  0.0838],\n",
      "         ...,\n",
      "         [ 0.1982, -0.1606, -0.1702],\n",
      "         [ 0.0026, -0.7322, -0.0818],\n",
      "         [-0.0290, -0.9208, -0.1509]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0164,  0.5022,  0.4260],\n",
      "         [-0.0563,  0.1060,  0.2722],\n",
      "         [ 0.0596, -0.3020,  0.1402],\n",
      "         ...,\n",
      "         [-0.2627, -0.1518, -0.1562],\n",
      "         [-0.0389, -0.7171, -0.1443],\n",
      "         [-0.1122, -0.9001, -0.1423]],\n",
      "\n",
      "        [[-0.0213,  0.5084,  0.4153],\n",
      "         [-0.0532,  0.1091,  0.2681],\n",
      "         [ 0.0619, -0.3001,  0.1399],\n",
      "         ...,\n",
      "         [-0.2597, -0.1563, -0.1396],\n",
      "         [-0.0403, -0.7189, -0.1388],\n",
      "         [-0.1125, -0.9026, -0.1358]],\n",
      "\n",
      "        [[-0.0336,  0.5154,  0.4025],\n",
      "         [-0.0538,  0.1125,  0.2634],\n",
      "         [ 0.0643, -0.2976,  0.1399],\n",
      "         ...,\n",
      "         [-0.2549, -0.1635, -0.1248],\n",
      "         [-0.0339, -0.7211, -0.1332],\n",
      "         [-0.1019, -0.9066, -0.1292]]], device='cuda:0')\n",
      "gt_keypoints_3d = gt_keypoints_3d - gt_pelvis torch.Size([32, 14, 3]) tensor([[[-0.0475,  0.7798,  0.3688],\n",
      "         [-0.0764,  0.4214,  0.1335],\n",
      "         [-0.1340, -0.0161,  0.0391],\n",
      "         ...,\n",
      "         [ 0.2052,  0.1000, -0.1962],\n",
      "         [ 0.0022, -0.4666, -0.1172],\n",
      "         [-0.0203, -0.6551, -0.1893]],\n",
      "\n",
      "        [[-0.0503,  0.7749,  0.3812],\n",
      "         [-0.0784,  0.4193,  0.1416],\n",
      "         [-0.1318, -0.0181,  0.0448],\n",
      "         ...,\n",
      "         [ 0.2021,  0.1030, -0.2005],\n",
      "         [ 0.0038, -0.4668, -0.1168],\n",
      "         [-0.0236, -0.6557, -0.1870]],\n",
      "\n",
      "        [[-0.0508,  0.7683,  0.3949],\n",
      "         [-0.0814,  0.4178,  0.1484],\n",
      "         [-0.1293, -0.0189,  0.0509],\n",
      "         ...,\n",
      "         [ 0.1977,  0.1038, -0.2031],\n",
      "         [ 0.0020, -0.4678, -0.1147],\n",
      "         [-0.0295, -0.6565, -0.1838]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0340,  0.7589,  0.4125],\n",
      "         [-0.0739,  0.3627,  0.2587],\n",
      "         [ 0.0420, -0.0453,  0.1267],\n",
      "         ...,\n",
      "         [-0.2803,  0.1049, -0.1696],\n",
      "         [-0.0565, -0.4604, -0.1578],\n",
      "         [-0.1298, -0.6435, -0.1557]],\n",
      "\n",
      "        [[-0.0387,  0.7650,  0.4018],\n",
      "         [-0.0707,  0.3657,  0.2546],\n",
      "         [ 0.0445, -0.0435,  0.1264],\n",
      "         ...,\n",
      "         [-0.2772,  0.1003, -0.1531],\n",
      "         [-0.0578, -0.4623, -0.1523],\n",
      "         [-0.1300, -0.6460, -0.1494]],\n",
      "\n",
      "        [[-0.0513,  0.7714,  0.3887],\n",
      "         [-0.0716,  0.3684,  0.2496],\n",
      "         [ 0.0465, -0.0416,  0.1262],\n",
      "         ...,\n",
      "         [-0.2726,  0.0925, -0.1385],\n",
      "         [-0.0516, -0.4651, -0.1469],\n",
      "         [-0.1196, -0.6506, -0.1429]]], device='cuda:0')\n",
      "pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices) torch.Size([32, 17, 3]) tensor([[[-1.5528e-03, -2.8516e-01,  3.8178e-02],\n",
      "         [ 1.2467e-01, -2.7624e-01,  1.1711e-02],\n",
      "         [ 9.1544e-02,  1.4309e-01, -8.7590e-02],\n",
      "         ...,\n",
      "         [-1.4944e-01, -7.2233e-01,  4.5321e-02],\n",
      "         [-2.2786e-01, -4.5168e-01,  6.9810e-02],\n",
      "         [-2.1972e-01, -2.3867e-01, -4.2673e-02]],\n",
      "\n",
      "        [[-7.4558e-04, -2.8216e-01,  3.6922e-02],\n",
      "         [ 1.2412e-01, -2.7271e-01,  4.6729e-03],\n",
      "         [ 9.0049e-02,  1.4888e-01, -8.8977e-02],\n",
      "         ...,\n",
      "         [-1.4507e-01, -7.1827e-01,  4.9744e-02],\n",
      "         [-2.2089e-01, -4.4811e-01,  8.4055e-02],\n",
      "         [-2.1628e-01, -2.3683e-01, -3.1558e-02]],\n",
      "\n",
      "        [[ 5.2071e-05, -2.8217e-01,  3.6277e-02],\n",
      "         [ 1.2262e-01, -2.7256e-01, -3.2386e-03],\n",
      "         [ 9.2429e-02,  1.4955e-01, -8.3759e-02],\n",
      "         ...,\n",
      "         [-1.3993e-01, -7.1724e-01,  6.4798e-02],\n",
      "         [-2.1119e-01, -4.4716e-01,  1.0496e-01],\n",
      "         [-2.1332e-01, -2.3748e-01, -1.1563e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1445e-02, -2.7427e-01,  1.2418e-02],\n",
      "         [-2.9435e-03, -2.3758e-01, -1.1083e-01],\n",
      "         [-1.3170e-02,  1.7642e-01, -2.9191e-02],\n",
      "         ...,\n",
      "         [ 1.7290e-02, -7.2144e-01,  6.1726e-02],\n",
      "         [ 2.8438e-02, -4.9277e-01,  2.1935e-01],\n",
      "         [-9.0052e-02, -3.0175e-01,  2.9770e-01]],\n",
      "\n",
      "        [[ 1.1677e-02, -2.7421e-01,  1.2151e-02],\n",
      "         [-6.0091e-03, -2.3818e-01, -1.1094e-01],\n",
      "         [-1.4531e-02,  1.7599e-01, -2.9417e-02],\n",
      "         ...,\n",
      "         [ 2.1487e-02, -7.2289e-01,  6.3962e-02],\n",
      "         [ 2.5334e-02, -4.9645e-01,  2.2490e-01],\n",
      "         [-1.0283e-01, -3.1199e-01,  3.0305e-01]],\n",
      "\n",
      "        [[ 1.1210e-02, -2.7137e-01,  1.1398e-02],\n",
      "         [-1.5098e-02, -2.3739e-01, -1.1047e-01],\n",
      "         [-1.7869e-02,  1.7912e-01, -3.0438e-02],\n",
      "         ...,\n",
      "         [ 2.8033e-02, -7.1749e-01,  6.6700e-02],\n",
      "         [ 3.4036e-02, -4.8430e-01,  2.1699e-01],\n",
      "         [-9.3604e-02, -2.9631e-01,  2.8980e-01]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval:   0%|          | 3/1110 [00:32<4:27:35, 14.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices) torch.Size([32, 17, 3]) tensor([[[ 1.7895e-02, -2.5558e-01,  1.4108e-02],\n",
      "         [-2.6830e-02, -2.1497e-01, -1.1207e-01],\n",
      "         [ 3.1522e-02,  2.0895e-01,  3.0737e-03],\n",
      "         ...,\n",
      "         [ 5.2375e-02, -7.0882e-01,  4.1224e-03],\n",
      "         [ 8.4470e-02, -4.8809e-01,  1.7150e-01],\n",
      "         [-1.0864e-02, -2.7316e-01,  2.4384e-01]],\n",
      "\n",
      "        [[ 1.8016e-02, -2.5551e-01,  1.4647e-02],\n",
      "         [-2.6830e-02, -2.1708e-01, -1.1213e-01],\n",
      "         [ 3.1673e-02,  2.0853e-01, -4.4098e-03],\n",
      "         ...,\n",
      "         [ 5.8635e-02, -7.0722e-01,  1.0116e-02],\n",
      "         [ 9.0338e-02, -4.8334e-01,  1.7372e-01],\n",
      "         [-6.8090e-03, -2.6855e-01,  2.4387e-01]],\n",
      "\n",
      "        [[ 1.8008e-02, -2.5548e-01,  1.4948e-02],\n",
      "         [-2.7822e-02, -2.1933e-01, -1.1218e-01],\n",
      "         [ 2.8088e-02,  2.0851e-01, -1.3227e-02],\n",
      "         ...,\n",
      "         [ 6.5306e-02, -7.0518e-01,  1.5492e-02],\n",
      "         [ 9.5819e-02, -4.7793e-01,  1.7486e-01],\n",
      "         [-3.8533e-03, -2.6373e-01,  2.4263e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.6715e-04, -2.5698e-01,  5.3115e-03],\n",
      "         [-1.3642e-01, -2.4135e-01, -1.9562e-02],\n",
      "         [-1.4571e-01,  1.5319e-01,  1.7821e-01],\n",
      "         ...,\n",
      "         [ 1.3385e-01, -6.8598e-01, -7.3053e-02],\n",
      "         [ 2.4337e-01, -4.4039e-01, -7.1552e-03],\n",
      "         [ 2.0127e-01, -2.1596e-01,  8.9842e-02]],\n",
      "\n",
      "        [[ 8.3882e-05, -2.5690e-01,  5.3383e-03],\n",
      "         [-1.3696e-01, -2.4226e-01, -1.7358e-02],\n",
      "         [-1.4283e-01,  1.5648e-01,  1.7370e-01],\n",
      "         ...,\n",
      "         [ 1.3348e-01, -6.8536e-01, -8.0979e-02],\n",
      "         [ 2.4484e-01, -4.4061e-01, -1.6526e-02],\n",
      "         [ 2.0934e-01, -2.1427e-01,  7.8292e-02]],\n",
      "\n",
      "        [[-1.5137e-04, -2.5668e-01,  5.2051e-03],\n",
      "         [-1.3768e-01, -2.4309e-01, -1.4794e-02],\n",
      "         [-1.3750e-01,  1.6156e-01,  1.6625e-01],\n",
      "         ...,\n",
      "         [ 1.3647e-01, -6.8451e-01, -8.9119e-02],\n",
      "         [ 2.4953e-01, -4.4124e-01, -2.4178e-02],\n",
      "         [ 2.1737e-01, -2.1428e-01,  7.0308e-02]]], device='cuda:0')\n",
      "joint_mapper_h36m [6, 5, 4, 1, 2, 3, 16, 15, 14, 11, 12, 13, 8, 10]\n",
      "gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :] torch.Size([32, 14, 3]) tensor([[[-0.0390,  0.5237,  0.3857],\n",
      "         [-0.0525,  0.1173,  0.2565],\n",
      "         [ 0.0654, -0.2955,  0.1403],\n",
      "         ...,\n",
      "         [-0.2474, -0.1692, -0.1124],\n",
      "         [-0.0281, -0.7229, -0.1266],\n",
      "         [-0.0926, -0.9104, -0.1210]],\n",
      "\n",
      "        [[-0.0361,  0.5324,  0.3649],\n",
      "         [-0.0515,  0.1225,  0.2484],\n",
      "         [ 0.0657, -0.2933,  0.1414],\n",
      "         ...,\n",
      "         [-0.2376, -0.1733, -0.1038],\n",
      "         [-0.0226, -0.7247, -0.1199],\n",
      "         [-0.0843, -0.9138, -0.1128]],\n",
      "\n",
      "        [[-0.0282,  0.5391,  0.3513],\n",
      "         [-0.0520,  0.1279,  0.2393],\n",
      "         [ 0.0668, -0.2911,  0.1421],\n",
      "         ...,\n",
      "         [-0.2285, -0.1768, -0.0996],\n",
      "         [-0.0164, -0.7263, -0.1138],\n",
      "         [-0.0749, -0.9168, -0.1050]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1885,  0.5800, -0.0516],\n",
      "         [ 0.1142,  0.1730,  0.0577],\n",
      "         [ 0.1377, -0.2716,  0.0299],\n",
      "         ...,\n",
      "         [-0.2229, -0.3579,  0.2155],\n",
      "         [-0.0128, -0.7376, -0.0837],\n",
      "         [-0.0245, -0.9360, -0.0464]],\n",
      "\n",
      "        [[ 0.1941,  0.5737, -0.0659],\n",
      "         [ 0.1169,  0.1733,  0.0607],\n",
      "         [ 0.1379, -0.2706,  0.0277],\n",
      "         ...,\n",
      "         [-0.2126, -0.3630,  0.2159],\n",
      "         [-0.0137, -0.7363, -0.0902],\n",
      "         [-0.0257, -0.9354, -0.0565]],\n",
      "\n",
      "        [[ 0.1939,  0.5654, -0.0790],\n",
      "         [ 0.1187,  0.1738,  0.0693],\n",
      "         [ 0.1381, -0.2692,  0.0248],\n",
      "         ...,\n",
      "         [-0.2025, -0.3737,  0.2142],\n",
      "         [-0.0106, -0.7355, -0.0977],\n",
      "         [-0.0225, -0.9344, -0.0671]]], device='cuda:0')\n",
      "gt_keypoints_3d = gt_keypoints_3d - gt_pelvis torch.Size([32, 14, 3]) tensor([[[-0.0569,  0.7792,  0.3716],\n",
      "         [-0.0704,  0.3729,  0.2424],\n",
      "         [ 0.0475, -0.0399,  0.1262],\n",
      "         ...,\n",
      "         [-0.2652,  0.0864, -0.1265],\n",
      "         [-0.0460, -0.4674, -0.1407],\n",
      "         [-0.1105, -0.6548, -0.1351]],\n",
      "\n",
      "        [[-0.0541,  0.7879,  0.3503],\n",
      "         [-0.0695,  0.3780,  0.2337],\n",
      "         [ 0.0477, -0.0378,  0.1268],\n",
      "         ...,\n",
      "         [-0.2556,  0.0822, -0.1184],\n",
      "         [-0.0406, -0.4692, -0.1345],\n",
      "         [-0.1023, -0.6583, -0.1274]],\n",
      "\n",
      "        [[-0.0462,  0.7945,  0.3364],\n",
      "         [-0.0700,  0.3834,  0.2244],\n",
      "         [ 0.0488, -0.0356,  0.1271],\n",
      "         ...,\n",
      "         [-0.2466,  0.0787, -0.1145],\n",
      "         [-0.0344, -0.4709, -0.1287],\n",
      "         [-0.0929, -0.6613, -0.1199]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1883,  0.8370, -0.0569],\n",
      "         [ 0.1139,  0.4300,  0.0524],\n",
      "         [ 0.1374, -0.0146,  0.0246],\n",
      "         ...,\n",
      "         [-0.2232, -0.1009,  0.2102],\n",
      "         [-0.0131, -0.4806, -0.0890],\n",
      "         [-0.0248, -0.6791, -0.0517]],\n",
      "\n",
      "        [[ 0.1940,  0.8306, -0.0712],\n",
      "         [ 0.1168,  0.4302,  0.0553],\n",
      "         [ 0.1378, -0.0137,  0.0224],\n",
      "         ...,\n",
      "         [-0.2127, -0.1061,  0.2106],\n",
      "         [-0.0137, -0.4794, -0.0956],\n",
      "         [-0.0257, -0.6785, -0.0618]],\n",
      "\n",
      "        [[ 0.1940,  0.8221, -0.0842],\n",
      "         [ 0.1188,  0.4305,  0.0641],\n",
      "         [ 0.1383, -0.0126,  0.0196],\n",
      "         ...,\n",
      "         [-0.2023, -0.1170,  0.2090],\n",
      "         [-0.0105, -0.4788, -0.1029],\n",
      "         [-0.0223, -0.6777, -0.0723]]], device='cuda:0')\n",
      "pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices) torch.Size([32, 17, 3]) tensor([[[ 0.0112, -0.2710,  0.0118],\n",
      "         [-0.0194, -0.2400, -0.1102],\n",
      "         [-0.0158,  0.1798, -0.0384],\n",
      "         ...,\n",
      "         [ 0.0333, -0.7166,  0.0745],\n",
      "         [ 0.0420, -0.4792,  0.2190],\n",
      "         [-0.0852, -0.2902,  0.2919]],\n",
      "\n",
      "        [[ 0.0107, -0.2702,  0.0125],\n",
      "         [-0.0211, -0.2422, -0.1099],\n",
      "         [-0.0137,  0.1796, -0.0483],\n",
      "         ...,\n",
      "         [ 0.0417, -0.7153,  0.0818],\n",
      "         [ 0.0503, -0.4798,  0.2292],\n",
      "         [-0.0800, -0.2976,  0.3121]],\n",
      "\n",
      "        [[ 0.0105, -0.2704,  0.0130],\n",
      "         [-0.0216, -0.2431, -0.1099],\n",
      "         [-0.0104,  0.1794, -0.0542],\n",
      "         ...,\n",
      "         [ 0.0459, -0.7163,  0.0835],\n",
      "         [ 0.0533, -0.4844,  0.2372],\n",
      "         [-0.0836, -0.3159,  0.3344]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0014, -0.2793,  0.0039],\n",
      "         [-0.1345, -0.2703, -0.0155],\n",
      "         [-0.1444,  0.1334,  0.1565],\n",
      "         ...,\n",
      "         [ 0.1441, -0.7206,  0.0523],\n",
      "         [ 0.2428, -0.4562,  0.0661],\n",
      "         [ 0.1890, -0.2697,  0.2025]],\n",
      "\n",
      "        [[-0.0011, -0.2769,  0.0042],\n",
      "         [-0.1332, -0.2682, -0.0178],\n",
      "         [-0.1459,  0.1416,  0.1457],\n",
      "         ...,\n",
      "         [ 0.1441, -0.7162,  0.0484],\n",
      "         [ 0.2396, -0.4496,  0.0684],\n",
      "         [ 0.1926, -0.2568,  0.2031]],\n",
      "\n",
      "        [[-0.0014, -0.2729,  0.0042],\n",
      "         [-0.1345, -0.2641, -0.0139],\n",
      "         [-0.1446,  0.1514,  0.1333],\n",
      "         ...,\n",
      "         [ 0.1464, -0.7100,  0.0391],\n",
      "         [ 0.2487, -0.4463,  0.0608],\n",
      "         [ 0.1999, -0.2605,  0.2015]]], device='cuda:0')\n",
      "gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices) torch.Size([32, 17, 3]) tensor([[[-6.5249e-04, -2.5617e-01,  4.7393e-03],\n",
      "         [-1.3874e-01, -2.4344e-01, -1.1265e-02],\n",
      "         [-1.3034e-01,  1.6657e-01,  1.5984e-01],\n",
      "         ...,\n",
      "         [ 1.3759e-01, -6.8317e-01, -9.7505e-02],\n",
      "         [ 2.5401e-01, -4.4231e-01, -2.9769e-02],\n",
      "         [ 2.2402e-01, -2.1640e-01,  6.7320e-02]],\n",
      "\n",
      "        [[-1.2944e-03, -2.5574e-01,  4.1136e-03],\n",
      "         [-1.3989e-01, -2.4365e-01, -7.4275e-03],\n",
      "         [-1.2444e-01,  1.7016e-01,  1.5552e-01],\n",
      "         ...,\n",
      "         [ 1.3611e-01, -6.8170e-01, -1.0542e-01],\n",
      "         [ 2.5674e-01, -4.4383e-01, -3.3276e-02],\n",
      "         [ 2.2857e-01, -2.2055e-01,  6.9653e-02]],\n",
      "\n",
      "        [[-1.7695e-03, -2.5513e-01,  3.2178e-03],\n",
      "         [-1.4084e-01, -2.4490e-01, -3.4334e-03],\n",
      "         [-1.2463e-01,  1.7134e-01,  1.5354e-01],\n",
      "         ...,\n",
      "         [ 1.3621e-01, -6.7942e-01, -1.1176e-01],\n",
      "         [ 2.5821e-01, -4.4336e-01, -3.4796e-02],\n",
      "         [ 2.2999e-01, -2.2455e-01,  7.6632e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9.8308e-03, -2.5343e-01,  4.8152e-03],\n",
      "         [-1.0020e-01, -2.2211e-01, -7.6079e-02],\n",
      "         [-1.8833e-01,  1.7251e-01,  8.2295e-02],\n",
      "         ...,\n",
      "         [ 1.0152e-01, -6.9617e-01, -9.4917e-03],\n",
      "         [ 3.5413e-02, -5.3605e-01,  1.7829e-01],\n",
      "         [-1.9070e-01, -5.4689e-01,  1.3687e-01]],\n",
      "\n",
      "        [[ 1.0481e-02, -2.5326e-01,  5.2261e-03],\n",
      "         [-9.6148e-02, -2.2087e-01, -7.9790e-02],\n",
      "         [-1.8430e-01,  1.7475e-01,  7.9251e-02],\n",
      "         ...,\n",
      "         [ 9.9099e-02, -6.9547e-01, -5.3350e-03],\n",
      "         [ 3.6107e-02, -5.3358e-01,  1.8437e-01],\n",
      "         [-1.9068e-01, -5.4907e-01,  1.4903e-01]],\n",
      "\n",
      "        [[ 1.1251e-02, -2.5318e-01,  5.6044e-03],\n",
      "         [-9.1734e-02, -2.2002e-01, -8.3690e-02],\n",
      "         [-1.7657e-01,  1.7829e-01,  7.5021e-02],\n",
      "         ...,\n",
      "         [ 9.7128e-02, -6.9533e-01, -1.5889e-03],\n",
      "         [ 3.7493e-02, -5.3292e-01,  1.9163e-01],\n",
      "         [-1.9004e-01, -5.5287e-01,  1.6564e-01]]], device='cuda:0')\n",
      "joint_mapper_h36m [6, 5, 4, 1, 2, 3, 16, 15, 14, 11, 12, 13, 8, 10]\n",
      "gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :] torch.Size([32, 14, 3]) tensor([[[ 0.1944,  0.5545, -0.0873],\n",
      "         [ 0.1223,  0.1737,  0.0835],\n",
      "         [ 0.1383, -0.2677,  0.0202],\n",
      "         ...,\n",
      "         [-0.1959, -0.3881,  0.2119],\n",
      "         [-0.0099, -0.7339, -0.1058],\n",
      "         [-0.0220, -0.9330, -0.0781]],\n",
      "\n",
      "        [[ 0.1932,  0.5404, -0.0908],\n",
      "         [ 0.1286,  0.1720,  0.1026],\n",
      "         [ 0.1383, -0.2666,  0.0151],\n",
      "         ...,\n",
      "         [-0.1926, -0.4070,  0.2112],\n",
      "         [-0.0119, -0.7320, -0.1134],\n",
      "         [-0.0252, -0.9315, -0.0876]],\n",
      "\n",
      "        [[ 0.1806,  0.5246, -0.0920],\n",
      "         [ 0.1312,  0.1701,  0.1241],\n",
      "         [ 0.1383, -0.2641,  0.0092],\n",
      "         ...,\n",
      "         [-0.1919, -0.4312,  0.2124],\n",
      "         [-0.0118, -0.7305, -0.1192],\n",
      "         [-0.0255, -0.9301, -0.0935]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1117,  0.5646,  0.2840],\n",
      "         [ 0.0679,  0.1476,  0.1924],\n",
      "         [ 0.1204, -0.2846,  0.0855],\n",
      "         ...,\n",
      "         [-0.3446, -0.5159, -0.0731],\n",
      "         [-0.0009, -0.7268, -0.1206],\n",
      "         [-0.0666, -0.9175, -0.1092]],\n",
      "\n",
      "        [[ 0.1084,  0.5656,  0.2809],\n",
      "         [ 0.0647,  0.1481,  0.1923],\n",
      "         [ 0.1176, -0.2856,  0.0899],\n",
      "         ...,\n",
      "         [-0.3479, -0.5136, -0.0810],\n",
      "         [-0.0013, -0.7262, -0.1183],\n",
      "         [-0.0702, -0.9160, -0.1062]],\n",
      "\n",
      "        [[ 0.1057,  0.5664,  0.2788],\n",
      "         [ 0.0623,  0.1482,  0.1937],\n",
      "         [ 0.1148, -0.2862,  0.0945],\n",
      "         ...,\n",
      "         [-0.3508, -0.5119, -0.0900],\n",
      "         [-0.0014, -0.7257, -0.1164],\n",
      "         [-0.0734, -0.9147, -0.1035]]], device='cuda:0')\n",
      "gt_keypoints_3d = gt_keypoints_3d - gt_pelvis torch.Size([32, 14, 3]) tensor([[[ 0.1950,  0.8106, -0.0921],\n",
      "         [ 0.1230,  0.4299,  0.0787],\n",
      "         [ 0.1390, -0.0116,  0.0155],\n",
      "         ...,\n",
      "         [-0.1952, -0.1319,  0.2072],\n",
      "         [-0.0093, -0.4777, -0.1105],\n",
      "         [-0.0214, -0.6768, -0.0828]],\n",
      "\n",
      "        [[ 0.1945,  0.7962, -0.0949],\n",
      "         [ 0.1299,  0.4277,  0.0985],\n",
      "         [ 0.1396, -0.0109,  0.0109],\n",
      "         ...,\n",
      "         [-0.1913, -0.1513,  0.2071],\n",
      "         [-0.0106, -0.4763, -0.1175],\n",
      "         [-0.0240, -0.6758, -0.0917]],\n",
      "\n",
      "        [[ 0.1824,  0.7797, -0.0952],\n",
      "         [ 0.1330,  0.4252,  0.1208],\n",
      "         [ 0.1401, -0.0090,  0.0059],\n",
      "         ...,\n",
      "         [-0.1901, -0.1760,  0.2092],\n",
      "         [-0.0101, -0.4753, -0.1225],\n",
      "         [-0.0238, -0.6750, -0.0967]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1019,  0.8180,  0.2792],\n",
      "         [ 0.0580,  0.4011,  0.1875],\n",
      "         [ 0.1105, -0.0311,  0.0806],\n",
      "         ...,\n",
      "         [-0.3544, -0.2625, -0.0780],\n",
      "         [-0.0108, -0.4734, -0.1254],\n",
      "         [-0.0765, -0.6641, -0.1140]],\n",
      "\n",
      "        [[ 0.0979,  0.8189,  0.2757],\n",
      "         [ 0.0542,  0.4014,  0.1870],\n",
      "         [ 0.1071, -0.0323,  0.0847],\n",
      "         ...,\n",
      "         [-0.3584, -0.2603, -0.0862],\n",
      "         [-0.0118, -0.4729, -0.1235],\n",
      "         [-0.0807, -0.6628, -0.1115]],\n",
      "\n",
      "        [[ 0.0945,  0.8196,  0.2732],\n",
      "         [ 0.0510,  0.4014,  0.1881],\n",
      "         [ 0.1035, -0.0330,  0.0889],\n",
      "         ...,\n",
      "         [-0.3621, -0.2587, -0.0956],\n",
      "         [-0.0126, -0.4725, -0.1220],\n",
      "         [-0.0847, -0.6615, -0.1091]]], device='cuda:0')\n",
      "pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices) torch.Size([32, 17, 3]) tensor([[[-0.0019, -0.2715,  0.0049],\n",
      "         [-0.1363, -0.2615, -0.0106],\n",
      "         [-0.1396,  0.1581,  0.1228],\n",
      "         ...,\n",
      "         [ 0.1434, -0.7102,  0.0320],\n",
      "         [ 0.2506, -0.4487,  0.0475],\n",
      "         [ 0.2066, -0.2680,  0.1934]],\n",
      "\n",
      "        [[-0.0016, -0.2711,  0.0050],\n",
      "         [-0.1363, -0.2615, -0.0103],\n",
      "         [-0.1429,  0.1625,  0.1032],\n",
      "         ...,\n",
      "         [ 0.1450, -0.7106,  0.0254],\n",
      "         [ 0.2513, -0.4498,  0.0449],\n",
      "         [ 0.1993, -0.2717,  0.1907]],\n",
      "\n",
      "        [[-0.0021, -0.2721,  0.0033],\n",
      "         [-0.1371, -0.2622, -0.0085],\n",
      "         [-0.1380,  0.1541,  0.1236],\n",
      "         ...,\n",
      "         [ 0.1445, -0.7132,  0.0246],\n",
      "         [ 0.2582, -0.4569,  0.0475],\n",
      "         [ 0.2040, -0.2841,  0.1964]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0101, -0.2641,  0.0052],\n",
      "         [-0.0959, -0.2482, -0.0793],\n",
      "         [-0.1873,  0.1514,  0.0382],\n",
      "         ...,\n",
      "         [ 0.0769, -0.7218,  0.0840],\n",
      "         [ 0.0561, -0.5475,  0.2821],\n",
      "         [-0.1443, -0.6189,  0.2928]],\n",
      "\n",
      "        [[ 0.0098, -0.2627,  0.0056],\n",
      "         [-0.0961, -0.2463, -0.0794],\n",
      "         [-0.1761,  0.1578,  0.0392],\n",
      "         ...,\n",
      "         [ 0.0810, -0.7174,  0.0827],\n",
      "         [ 0.0706, -0.5336,  0.2730],\n",
      "         [-0.1264, -0.6085,  0.3006]],\n",
      "\n",
      "        [[ 0.0094, -0.2632,  0.0059],\n",
      "         [-0.0946, -0.2462, -0.0805],\n",
      "         [-0.1671,  0.1584,  0.0410],\n",
      "         ...,\n",
      "         [ 0.0820, -0.7137,  0.0896],\n",
      "         [ 0.0658, -0.5214,  0.2703],\n",
      "         [-0.1348, -0.5865,  0.2974]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval:   0%|          | 4/1110 [00:33<3:10:54, 10.36s/it]\u001b[A\n",
      "Eval:   0%|          | 5/1110 [00:33<2:15:57,  7.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices) torch.Size([32, 17, 3]) tensor([[[ 0.0119, -0.2529,  0.0060],\n",
      "         [-0.0880, -0.2194, -0.0867],\n",
      "         [-0.1650,  0.1830,  0.0714],\n",
      "         ...,\n",
      "         [ 0.0951, -0.6954,  0.0020],\n",
      "         [ 0.0378, -0.5342,  0.1996],\n",
      "         [-0.1903, -0.5574,  0.1836]],\n",
      "\n",
      "        [[ 0.0125, -0.2524,  0.0060],\n",
      "         [-0.0847, -0.2184, -0.0893],\n",
      "         [-0.1525,  0.1869,  0.0690],\n",
      "         ...,\n",
      "         [ 0.0945, -0.6951,  0.0056],\n",
      "         [ 0.0380, -0.5377,  0.2083],\n",
      "         [-0.1904, -0.5626,  0.1991]],\n",
      "\n",
      "        [[ 0.0126, -0.2519,  0.0064],\n",
      "         [-0.0826, -0.2174, -0.0905],\n",
      "         [-0.1376,  0.1918,  0.0661],\n",
      "         ...,\n",
      "         [ 0.0933, -0.6945,  0.0080],\n",
      "         [ 0.0372, -0.5422,  0.2158],\n",
      "         [-0.1915, -0.5672,  0.2104]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0121, -0.2523,  0.0092],\n",
      "         [-0.0751, -0.2178, -0.0944],\n",
      "         [-0.0745,  0.2058,  0.0331],\n",
      "         ...,\n",
      "         [ 0.0881, -0.6972,  0.0153],\n",
      "         [ 0.0648, -0.5180,  0.2128],\n",
      "         [-0.1482, -0.5789,  0.1910]],\n",
      "\n",
      "        [[ 0.0124, -0.2520,  0.0091],\n",
      "         [-0.0742, -0.2172, -0.0949],\n",
      "         [-0.0730,  0.2059,  0.0340],\n",
      "         ...,\n",
      "         [ 0.0854, -0.6968,  0.0162],\n",
      "         [ 0.0627, -0.5168,  0.2134],\n",
      "         [-0.1500, -0.5818,  0.2019]],\n",
      "\n",
      "        [[ 0.0125, -0.2519,  0.0091],\n",
      "         [-0.0734, -0.2166, -0.0952],\n",
      "         [-0.0707,  0.2061,  0.0347],\n",
      "         ...,\n",
      "         [ 0.0821, -0.6966,  0.0174],\n",
      "         [ 0.0598, -0.5178,  0.2150],\n",
      "         [-0.1533, -0.5824,  0.2091]]], device='cuda:0')\n",
      "joint_mapper_h36m [6, 5, 4, 1, 2, 3, 16, 15, 14, 11, 12, 13, 8, 10]\n",
      "gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :] torch.Size([32, 14, 3]) tensor([[[ 1.0334e-01,  5.6726e-01,  2.7726e-01],\n",
      "         [ 5.9547e-02,  1.4879e-01,  1.9456e-01],\n",
      "         [ 1.1253e-01, -2.8627e-01,  9.8294e-02],\n",
      "         ...,\n",
      "         [-3.5392e-01, -5.1062e-01, -9.9717e-02],\n",
      "         [-1.4549e-03, -7.2522e-01, -1.1471e-01],\n",
      "         [-7.6413e-02, -9.1323e-01, -1.0125e-01]],\n",
      "\n",
      "        [[ 9.9591e-02,  5.6775e-01,  2.7689e-01],\n",
      "         [ 5.5355e-02,  1.4915e-01,  1.9543e-01],\n",
      "         [ 1.1080e-01, -2.8602e-01,  1.0084e-01],\n",
      "         ...,\n",
      "         [-3.5630e-01, -5.1104e-01, -1.0941e-01],\n",
      "         [-2.2826e-04, -7.2454e-01, -1.1333e-01],\n",
      "         [-7.7378e-02, -9.1196e-01, -9.9990e-02]],\n",
      "\n",
      "        [[ 9.5861e-02,  5.6810e-01,  2.7684e-01],\n",
      "         [ 5.1000e-02,  1.4986e-01,  1.9473e-01],\n",
      "         [ 1.0891e-01, -2.8621e-01,  1.0283e-01],\n",
      "         ...,\n",
      "         [-3.5789e-01, -5.1125e-01, -1.1900e-01],\n",
      "         [ 9.8641e-04, -7.2362e-01, -1.1298e-01],\n",
      "         [-7.8002e-02, -9.1056e-01, -1.0019e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1571e-01,  5.7575e-01,  2.3898e-01],\n",
      "         [ 5.6278e-02,  1.5513e-01,  1.8318e-01],\n",
      "         [ 1.0109e-01, -2.8653e-01,  1.1271e-01],\n",
      "         ...,\n",
      "         [-3.2110e-01, -5.1282e-01, -4.1560e-02],\n",
      "         [-3.6505e-03, -7.2483e-01, -1.0665e-01],\n",
      "         [-9.3516e-02, -9.0746e-01, -8.7201e-02]],\n",
      "\n",
      "        [[ 1.1448e-01,  5.7535e-01,  2.4060e-01],\n",
      "         [ 5.5129e-02,  1.5475e-01,  1.8431e-01],\n",
      "         [ 1.0066e-01, -2.8663e-01,  1.1308e-01],\n",
      "         ...,\n",
      "         [-3.2333e-01, -5.0952e-01, -5.0198e-02],\n",
      "         [-5.4679e-03, -7.2411e-01, -1.0642e-01],\n",
      "         [-9.6119e-02, -9.0608e-01, -8.7039e-02]],\n",
      "\n",
      "        [[ 1.1419e-01,  5.7504e-01,  2.4161e-01],\n",
      "         [ 5.4197e-02,  1.5440e-01,  1.8508e-01],\n",
      "         [ 1.0013e-01, -2.8696e-01,  1.1347e-01],\n",
      "         ...,\n",
      "         [-3.2373e-01, -5.0672e-01, -5.6682e-02],\n",
      "         [-7.6655e-03, -7.2340e-01, -1.0608e-01],\n",
      "         [-9.9021e-02, -9.0489e-01, -8.7250e-02]]], device='cuda:0')\n",
      "gt_keypoints_3d = gt_keypoints_3d - gt_pelvis torch.Size([32, 14, 3]) tensor([[[ 0.0914,  0.8202,  0.2713],\n",
      "         [ 0.0476,  0.4017,  0.1886],\n",
      "         [ 0.1006, -0.0333,  0.0923],\n",
      "         ...,\n",
      "         [-0.3658, -0.2577, -0.1057],\n",
      "         [-0.0134, -0.4723, -0.1207],\n",
      "         [-0.0883, -0.6603, -0.1072]],\n",
      "\n",
      "        [[ 0.0870,  0.8201,  0.2709],\n",
      "         [ 0.0428,  0.4015,  0.1894],\n",
      "         [ 0.0983, -0.0337,  0.0948],\n",
      "         ...,\n",
      "         [-0.3688, -0.2587, -0.1154],\n",
      "         [-0.0128, -0.4722, -0.1193],\n",
      "         [-0.0899, -0.6596, -0.1060]],\n",
      "\n",
      "        [[ 0.0833,  0.8200,  0.2704],\n",
      "         [ 0.0384,  0.4017,  0.1883],\n",
      "         [ 0.0963, -0.0343,  0.0964],\n",
      "         ...,\n",
      "         [-0.3705, -0.2594, -0.1254],\n",
      "         [-0.0116, -0.4717, -0.1194],\n",
      "         [-0.0906, -0.6587, -0.1066]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1036,  0.8281,  0.2298],\n",
      "         [ 0.0441,  0.4074,  0.1740],\n",
      "         [ 0.0890, -0.0342,  0.1035],\n",
      "         ...,\n",
      "         [-0.3332, -0.2605, -0.0507],\n",
      "         [-0.0158, -0.4725, -0.1158],\n",
      "         [-0.1056, -0.6551, -0.0964]],\n",
      "\n",
      "        [[ 0.1021,  0.8274,  0.2315],\n",
      "         [ 0.0428,  0.4068,  0.1752],\n",
      "         [ 0.0883, -0.0346,  0.1040],\n",
      "         ...,\n",
      "         [-0.3357, -0.2575, -0.0593],\n",
      "         [-0.0178, -0.4721, -0.1155],\n",
      "         [-0.1085, -0.6540, -0.0962]],\n",
      "\n",
      "        [[ 0.1017,  0.8269,  0.2325],\n",
      "         [ 0.0417,  0.4063,  0.1759],\n",
      "         [ 0.0876, -0.0351,  0.1043],\n",
      "         ...,\n",
      "         [-0.3363, -0.2548, -0.0658],\n",
      "         [-0.0202, -0.4715, -0.1152],\n",
      "         [-0.1116, -0.6530, -0.0964]]], device='cuda:0')\n",
      "pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices) torch.Size([32, 17, 3]) tensor([[[ 0.0084, -0.2637,  0.0064],\n",
      "         [-0.0977, -0.2458, -0.0778],\n",
      "         [-0.1550,  0.1604,  0.0508],\n",
      "         ...,\n",
      "         [ 0.0857, -0.7191,  0.0799],\n",
      "         [ 0.0834, -0.5340,  0.2680],\n",
      "         [-0.1110, -0.6097,  0.2992]],\n",
      "\n",
      "        [[ 0.0077, -0.2655,  0.0069],\n",
      "         [-0.1005, -0.2497, -0.0742],\n",
      "         [-0.1498,  0.1572,  0.0468],\n",
      "         ...,\n",
      "         [ 0.0889, -0.7200,  0.0845],\n",
      "         [ 0.0895, -0.5331,  0.2699],\n",
      "         [-0.1025, -0.6066,  0.3120]],\n",
      "\n",
      "        [[ 0.0077, -0.2665,  0.0079],\n",
      "         [-0.0983, -0.2505, -0.0761],\n",
      "         [-0.1357,  0.1581,  0.0346],\n",
      "         ...,\n",
      "         [ 0.0845, -0.7223,  0.0897],\n",
      "         [ 0.0838, -0.5382,  0.2765],\n",
      "         [-0.1074, -0.6116,  0.3148]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0083, -0.2622,  0.0112],\n",
      "         [-0.0758, -0.2424, -0.0937],\n",
      "         [-0.0913,  0.1791, -0.0129],\n",
      "         ...,\n",
      "         [ 0.0636, -0.7138,  0.0972],\n",
      "         [ 0.0534, -0.5201,  0.2776],\n",
      "         [-0.1499, -0.5746,  0.2765]],\n",
      "\n",
      "        [[ 0.0082, -0.2622,  0.0111],\n",
      "         [-0.0778, -0.2430, -0.0923],\n",
      "         [-0.0942,  0.1782, -0.0108],\n",
      "         ...,\n",
      "         [ 0.0645, -0.7136,  0.0986],\n",
      "         [ 0.0540, -0.5179,  0.2763],\n",
      "         [-0.1511, -0.5668,  0.2729]],\n",
      "\n",
      "        [[ 0.0081, -0.2626,  0.0112],\n",
      "         [-0.0780, -0.2430, -0.0923],\n",
      "         [-0.0936,  0.1790, -0.0104],\n",
      "         ...,\n",
      "         [ 0.0643, -0.7145,  0.0963],\n",
      "         [ 0.0530, -0.5153,  0.2715],\n",
      "         [-0.1531, -0.5623,  0.2664]]], device='cuda:0')\n",
      "gt_keypoints_3d = torch.matmul(J_regressor_batch, gt_vertices) torch.Size([32, 17, 3]) tensor([[[ 0.0128, -0.2517,  0.0092],\n",
      "         [-0.0726, -0.2163, -0.0957],\n",
      "         [-0.0697,  0.2060,  0.0353],\n",
      "         ...,\n",
      "         [ 0.0808, -0.6964,  0.0185],\n",
      "         [ 0.0573, -0.5196,  0.2166],\n",
      "         [-0.1563, -0.5830,  0.2137]],\n",
      "\n",
      "        [[ 0.0130, -0.2516,  0.0092],\n",
      "         [-0.0717, -0.2164, -0.0962],\n",
      "         [-0.0689,  0.2058,  0.0354],\n",
      "         ...,\n",
      "         [ 0.0801, -0.6964,  0.0197],\n",
      "         [ 0.0542, -0.5206,  0.2184],\n",
      "         [-0.1596, -0.5832,  0.2169]],\n",
      "\n",
      "        [[ 0.0132, -0.2516,  0.0092],\n",
      "         [-0.0705, -0.2160, -0.0969],\n",
      "         [-0.0665,  0.2059,  0.0360],\n",
      "         ...,\n",
      "         [ 0.0774, -0.6971,  0.0203],\n",
      "         [ 0.0504, -0.5223,  0.2187],\n",
      "         [-0.1644, -0.5823,  0.2172]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0160, -0.2551,  0.0169],\n",
      "         [-0.0193, -0.2208, -0.1132],\n",
      "         [-0.0605,  0.2076, -0.0156],\n",
      "         ...,\n",
      "         [ 0.0275, -0.7121,  0.0614],\n",
      "         [-0.0575, -0.5652,  0.2559],\n",
      "         [-0.2600, -0.6180,  0.1764]],\n",
      "\n",
      "        [[ 0.0160, -0.2553,  0.0172],\n",
      "         [-0.0175, -0.2209, -0.1133],\n",
      "         [-0.0540,  0.2083, -0.0202],\n",
      "         ...,\n",
      "         [ 0.0251, -0.7127,  0.0621],\n",
      "         [-0.0572, -0.5632,  0.2561],\n",
      "         [-0.2588, -0.6166,  0.1755]],\n",
      "\n",
      "        [[ 0.0159, -0.2554,  0.0175],\n",
      "         [-0.0165, -0.2213, -0.1134],\n",
      "         [-0.0516,  0.2084, -0.0229],\n",
      "         ...,\n",
      "         [ 0.0256, -0.7134,  0.0625],\n",
      "         [-0.0547, -0.5625,  0.2560],\n",
      "         [-0.2554, -0.6169,  0.1747]]], device='cuda:0')\n",
      "joint_mapper_h36m [6, 5, 4, 1, 2, 3, 16, 15, 14, 11, 12, 13, 8, 10]\n",
      "gt_keypoints_3d = gt_keypoints_3d[:, joint_mapper_h36m, :] torch.Size([32, 14, 3]) tensor([[[ 0.1112,  0.5751,  0.2423],\n",
      "         [ 0.0516,  0.1542,  0.1863],\n",
      "         [ 0.0998, -0.2869,  0.1139],\n",
      "         ...,\n",
      "         [-0.3216, -0.5060, -0.0613],\n",
      "         [-0.0081, -0.7228, -0.1058],\n",
      "         [-0.0989, -0.9043, -0.0876]],\n",
      "\n",
      "        [[ 0.1077,  0.5752,  0.2427],\n",
      "         [ 0.0482,  0.1540,  0.1883],\n",
      "         [ 0.0993, -0.2866,  0.1146],\n",
      "         ...,\n",
      "         [-0.3179, -0.5073, -0.0641],\n",
      "         [-0.0080, -0.7226, -0.1053],\n",
      "         [-0.0979, -0.9042, -0.0876]],\n",
      "\n",
      "        [[ 0.1080,  0.5741,  0.2440],\n",
      "         [ 0.0458,  0.1529,  0.1920],\n",
      "         [ 0.0985, -0.2869,  0.1153],\n",
      "         ...,\n",
      "         [-0.3139, -0.5089, -0.0651],\n",
      "         [-0.0098, -0.7223, -0.1053],\n",
      "         [-0.0996, -0.9039, -0.0889]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1028,  0.5697,  0.2816],\n",
      "         [ 0.0305,  0.1541,  0.1935],\n",
      "         [ 0.0527, -0.2888,  0.1475],\n",
      "         ...,\n",
      "         [-0.2944, -0.5686, -0.1404],\n",
      "         [-0.0064, -0.7295, -0.0847],\n",
      "         [-0.1354, -0.8858, -0.0934]],\n",
      "\n",
      "        [[ 0.1068,  0.5702,  0.2772],\n",
      "         [ 0.0320,  0.1544,  0.1922],\n",
      "         [ 0.0509, -0.2891,  0.1484],\n",
      "         ...,\n",
      "         [-0.2939, -0.5661, -0.1429],\n",
      "         [-0.0085, -0.7301, -0.0840],\n",
      "         [-0.1400, -0.8841, -0.0924]],\n",
      "\n",
      "        [[ 0.1062,  0.5710,  0.2739],\n",
      "         [ 0.0315,  0.1548,  0.1913],\n",
      "         [ 0.0497, -0.2890,  0.1489],\n",
      "         ...,\n",
      "         [-0.2910, -0.5655, -0.1445],\n",
      "         [-0.0075, -0.7307, -0.0834],\n",
      "         [-0.1404, -0.8832, -0.0920]]], device='cuda:0')\n",
      "gt_keypoints_3d = gt_keypoints_3d - gt_pelvis torch.Size([32, 14, 3]) tensor([[[ 0.0984,  0.8268,  0.2332],\n",
      "         [ 0.0388,  0.4059,  0.1772],\n",
      "         [ 0.0870, -0.0352,  0.1048],\n",
      "         ...,\n",
      "         [-0.3343, -0.2543, -0.0704],\n",
      "         [-0.0209, -0.4711, -0.1150],\n",
      "         [-0.1117, -0.6525, -0.0968]],\n",
      "\n",
      "        [[ 0.0947,  0.8268,  0.2335],\n",
      "         [ 0.0352,  0.4056,  0.1791],\n",
      "         [ 0.0863, -0.0350,  0.1054],\n",
      "         ...,\n",
      "         [-0.3309, -0.2557, -0.0733],\n",
      "         [-0.0210, -0.4710, -0.1145],\n",
      "         [-0.1109, -0.6526, -0.0968]],\n",
      "\n",
      "        [[ 0.0948,  0.8257,  0.2348],\n",
      "         [ 0.0326,  0.4046,  0.1827],\n",
      "         [ 0.0852, -0.0353,  0.1061],\n",
      "         ...,\n",
      "         [-0.3271, -0.2573, -0.0743],\n",
      "         [-0.0230, -0.4707, -0.1145],\n",
      "         [-0.1128, -0.6522, -0.0981]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0868,  0.8248,  0.2648],\n",
      "         [ 0.0144,  0.4091,  0.1766],\n",
      "         [ 0.0367, -0.0338,  0.1307],\n",
      "         ...,\n",
      "         [-0.3104, -0.3135, -0.1572],\n",
      "         [-0.0224, -0.4744, -0.1016],\n",
      "         [-0.1514, -0.6307, -0.1103]],\n",
      "\n",
      "        [[ 0.0908,  0.8255,  0.2600],\n",
      "         [ 0.0160,  0.4097,  0.1750],\n",
      "         [ 0.0349, -0.0338,  0.1312],\n",
      "         ...,\n",
      "         [-0.3099, -0.3108, -0.1601],\n",
      "         [-0.0245, -0.4748, -0.1012],\n",
      "         [-0.1560, -0.6288, -0.1096]],\n",
      "\n",
      "        [[ 0.0902,  0.8264,  0.2565],\n",
      "         [ 0.0156,  0.4102,  0.1739],\n",
      "         [ 0.0338, -0.0336,  0.1315],\n",
      "         ...,\n",
      "         [-0.3070, -0.3101, -0.1620],\n",
      "         [-0.0235, -0.4753, -0.1009],\n",
      "         [-0.1563, -0.6278, -0.1095]]], device='cuda:0')\n",
      "pred_keypoints_3d = torch.matmul(J_regressor_batch, pred_vertices) torch.Size([32, 17, 3]) tensor([[[ 0.0085, -0.2631,  0.0115],\n",
      "         [-0.0737, -0.2430, -0.0951],\n",
      "         [-0.0882,  0.1793, -0.0152],\n",
      "         ...,\n",
      "         [ 0.0615, -0.7160,  0.0992],\n",
      "         [ 0.0443, -0.5196,  0.2763],\n",
      "         [-0.1619, -0.5653,  0.2619]],\n",
      "\n",
      "        [[ 0.0083, -0.2630,  0.0117],\n",
      "         [-0.0729, -0.2431, -0.0955],\n",
      "         [-0.0836,  0.1795, -0.0168],\n",
      "         ...,\n",
      "         [ 0.0620, -0.7152,  0.1000],\n",
      "         [ 0.0498, -0.5173,  0.2749],\n",
      "         [-0.1524, -0.5727,  0.2617]],\n",
      "\n",
      "        [[ 0.0089, -0.2635,  0.0122],\n",
      "         [-0.0673, -0.2427, -0.0990],\n",
      "         [-0.0818,  0.1796, -0.0196],\n",
      "         ...,\n",
      "         [ 0.0580, -0.7161,  0.1006],\n",
      "         [ 0.0409, -0.5154,  0.2716],\n",
      "         [-0.1606, -0.5685,  0.2469]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0131, -0.2669,  0.0210],\n",
      "         [ 0.0109, -0.2393, -0.1144],\n",
      "         [-0.0453,  0.1797, -0.0474],\n",
      "         ...,\n",
      "         [ 0.0280, -0.7501,  0.0908],\n",
      "         [-0.0607, -0.6597,  0.3206],\n",
      "         [-0.2117, -0.6853,  0.1775]],\n",
      "\n",
      "        [[ 0.0129, -0.2678,  0.0230],\n",
      "         [ 0.0305, -0.2397, -0.1105],\n",
      "         [-0.0307,  0.1774, -0.0510],\n",
      "         ...,\n",
      "         [ 0.0128, -0.7380,  0.0981],\n",
      "         [-0.1038, -0.5947,  0.2844],\n",
      "         [-0.2040, -0.6300,  0.1053]],\n",
      "\n",
      "        [[ 0.0131, -0.2680,  0.0244],\n",
      "         [ 0.0385, -0.2398, -0.1087],\n",
      "         [-0.0225,  0.1772, -0.0529],\n",
      "         ...,\n",
      "         [ 0.0079, -0.7389,  0.1021],\n",
      "         [-0.1132, -0.6013,  0.2880],\n",
      "         [-0.1975, -0.6310,  0.1005]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval:   1%|          | 6/1110 [00:34<1:39:30,  5.41s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-956fd475e6e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                    log_freq=args.log_freq)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-0d14a9cd3715>\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(model, dataset_name, dataset, result_file, batch_size, img_res, num_workers, shuffle, log_freq)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mpred_rotmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_betas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_camera\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mpred_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmpl_neutral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_betas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody_pose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_rotmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_orient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_rotmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose2rot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mpred_vertices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/SPIN/models/hmr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, init_pose, init_shape, init_cam, n_iter)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mx4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mxf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/SPIN/models/hmr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    args = parser.parse_args(['--checkpoint=data/model_checkpoint.pt','--dataset=3dpw', '--log_freq=20'])\n",
    "    # Here we inserted our own arguments list\n",
    "    \n",
    "    model = hmr(config.SMPL_MEAN_PARAMS)\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    # Setup evaluation dataset\n",
    "    dataset = BaseDataset(None, args.dataset, is_train=False)\n",
    "    # Run evaluation\n",
    "    run_evaluation(model, args.dataset, dataset, args.result_file,\n",
    "                   batch_size=args.batch_size,\n",
    "                   shuffle=args.shuffle,\n",
    "                   log_freq=args.log_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35515, 14])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pjpe = torch.load('pjpe.pt')\n",
    "pjpe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hips',\n",
       " 'leftUpLeg',\n",
       " 'rightUpLeg',\n",
       " 'spine',\n",
       " 'leftLeg',\n",
       " 'rightLeg',\n",
       " 'spine1',\n",
       " 'leftFoot',\n",
       " 'rightFoot',\n",
       " 'spine2',\n",
       " 'leftToeBase',\n",
       " 'rightToeBase',\n",
       " 'neck',\n",
       " 'leftForeArm']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            \n",
    "# SMPL joint names according to https://github.com/gulvarol/surreal/blob/5e3193741ddb429f2decb9c0873e252447058dc5/datageneration/misc/smpl_relations/smpl_relations.py#L19\n",
    "\n",
    "SMPL_joint_names = ['hips',\n",
    "                    'leftUpLeg',\n",
    "                    'rightUpLeg',\n",
    "                    'spine',\n",
    "                    'leftLeg',\n",
    "                    'rightLeg',\n",
    "                    'spine1',\n",
    "                    'leftFoot',\n",
    "                    'rightFoot',\n",
    "                    'spine2',\n",
    "                    'leftToeBase',\n",
    "                    'rightToeBase',\n",
    "                    'neck',\n",
    "                    'leftShoulder',\n",
    "                    'rightShoulder',\n",
    "                    'head',\n",
    "                    'leftArm',\n",
    "                    'rightArm',\n",
    "                    'leftForeArm',\n",
    "                    'rightForeArm',\n",
    "                    'leftHand',\n",
    "                    'rightHand',\n",
    "                    'leftHandIndex1',\n",
    "                    'rightHandIndex1']\n",
    "\n",
    "# The subset of joints from SMPL used on the pose evaluation of e.g. 3dpw (only the 14 first items)\n",
    "\n",
    "J24_TO_J17 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 14, 16, 17]\n",
    "J24_TO_J14 = J24_TO_J17[:14]\n",
    "\n",
    "SMPL_14 = [SMPL_joint_names[i] for i in J24_TO_J14]\n",
    "SMPL_14 # This seems quite odd because there's hardly any upper body joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 [0, 1, 2, 4, 4, 5, 5, 7, 7, 8, 8, 12, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 24, 24, 25, 25, 26, 26, 27, 27, 28, 28, 29, 30, 31, 32, 33, 34, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n"
     ]
    }
   ],
   "source": [
    "J24_TO_J17 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 14, 16, 17]\n",
    "\n",
    "# Map joints to SMPL joints\n",
    "JOINT_MAP = {\n",
    "'OP Nose': 24, 'OP Neck': 12, 'OP RShoulder': 17,\n",
    "'OP RElbow': 19, 'OP RWrist': 21, 'OP LShoulder': 16,\n",
    "'OP LElbow': 18, 'OP LWrist': 20, 'OP MidHip': 0,\n",
    "'OP RHip': 2, 'OP RKnee': 5, 'OP RAnkle': 8,\n",
    "'OP LHip': 1, 'OP LKnee': 4, 'OP LAnkle': 7,\n",
    "'OP REye': 25, 'OP LEye': 26, 'OP REar': 27,\n",
    "'OP LEar': 28, 'OP LBigToe': 29, 'OP LSmallToe': 30,\n",
    "'OP LHeel': 31, 'OP RBigToe': 32, 'OP RSmallToe': 33, 'OP RHeel': 34,\n",
    "'Right Ankle': 8, 'Right Knee': 5, 'Right Hip': 45,\n",
    "'Left Hip': 46, 'Left Knee': 4, 'Left Ankle': 7,\n",
    "'Right Wrist': 21, 'Right Elbow': 19, 'Right Shoulder': 17,\n",
    "'Left Shoulder': 16, 'Left Elbow': 18, 'Left Wrist': 20,\n",
    "'Neck (LSP)': 47, 'Top of Head (LSP)': 48,\n",
    "'Pelvis (MPII)': 49, 'Thorax (MPII)': 50,\n",
    "'Spine (H36M)': 51, 'Jaw (H36M)': 52,\n",
    "'Head (H36M)': 53, 'Nose': 24, 'Left Eye': 26,\n",
    "'Right Eye': 25, 'Left Ear': 28, 'Right Ear': 27\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(len(JOINT_MAP.values()), sorted(JOINT_MAP.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OP Neck': 12,\n",
       " 'OP LElbow': 18,\n",
       " 'OP MidHip': 0,\n",
       " 'OP RHip': 2,\n",
       " 'OP RKnee': 5,\n",
       " 'OP RAnkle': 8,\n",
       " 'OP LHip': 1,\n",
       " 'OP LKnee': 4,\n",
       " 'OP LAnkle': 7,\n",
       " 'Right Ankle': 8,\n",
       " 'Right Knee': 5,\n",
       " 'Left Knee': 4,\n",
       " 'Left Ankle': 7,\n",
       " 'Left Elbow': 18}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_subset = {k:v for k,v in JOINT_MAP.items() if v in J24_TO_J17[:14]}\n",
    "\n",
    "joint_dict = {}\n",
    "\n",
    "for k in parts_subset:\n",
    "    joint_dict[str(k)] = JOINT_MAP[k]\n",
    "\n",
    "joint_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: 'OP Neck',\n",
       " 18: 'Left Elbow',\n",
       " 0: 'OP MidHip',\n",
       " 2: 'OP RHip',\n",
       " 5: 'Right Knee',\n",
       " 8: 'Right Ankle',\n",
       " 1: 'OP LHip',\n",
       " 4: 'Left Knee',\n",
       " 7: 'Left Ankle'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_subset = {v:k for k,v in JOINT_MAP.items() if v in J24_TO_J17[:14]}\n",
    "parts_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('OP Nose', 24), ('OP Neck', 12), ('OP RShoulder', 17), ('OP RElbow', 19), ('OP RWrist', 21), ('OP LShoulder', 16), ('OP LElbow', 18), ('OP LWrist', 20), ('OP MidHip', 0), ('OP RHip', 2), ('OP RKnee', 5), ('OP RAnkle', 8), ('OP LHip', 1), ('OP LKnee', 4), ('OP LAnkle', 7), ('OP REye', 25), ('OP LEye', 26), ('OP REar', 27), ('OP LEar', 28), ('OP LBigToe', 29), ('OP LSmallToe', 30), ('OP LHeel', 31), ('OP RBigToe', 32), ('OP RSmallToe', 33), ('OP RHeel', 34), ('Right Ankle', 8), ('Right Knee', 5), ('Right Hip', 45), ('Left Hip', 46), ('Left Knee', 4), ('Left Ankle', 7), ('Right Wrist', 21), ('Right Elbow', 19), ('Right Shoulder', 17), ('Left Shoulder', 16), ('Left Elbow', 18), ('Left Wrist', 20), ('Neck (LSP)', 47), ('Top of Head (LSP)', 48), ('Pelvis (MPII)', 49), ('Thorax (MPII)', 50), ('Spine (H36M)', 51), ('Jaw (H36M)', 52), ('Head (H36M)', 53), ('Nose', 24), ('Left Eye', 26), ('Right Eye', 25), ('Left Ear', 28), ('Right Ear', 27)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JOINT_MAP.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J24_TO_J17[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP Nose         \n",
      "OP Neck         \n",
      "OP RShoulder    \n",
      "OP RElbow       \n",
      "OP RWrist       \n",
      "OP LShoulder    \n",
      "OP LElbow       \n",
      "OP LWrist       \n",
      "OP MidHip       \n",
      "OP RHip         \n",
      "OP RKnee        \n",
      "OP RAnkle       \n",
      "OP LHip         \n",
      "OP LKnee        \n",
      "OP LAnkle       \n",
      "OP REye         \n",
      "OP LEye         \n",
      "OP REar         \n",
      "OP LEar         \n",
      "OP LBigToe      \n",
      "OP LSmallToe    \n",
      "OP LHeel        \n",
      "OP RBigToe      \n",
      "OP RSmallToe    \n",
      "OP RHeel        \n",
      "Right Ankle     \n",
      "Right Knee      \n",
      "Right Hip       \n",
      "Left Hip        \n",
      "Left Knee       \n",
      "Left Ankle      \n",
      "Right Wrist     \n",
      "Right Elbow     \n",
      "Right Shoulder  \n",
      "Left Shoulder   \n",
      "Left Elbow      \n",
      "Left Wrist      \n",
      "Neck (LSP)      \n",
      "Top of Head (LSP)\n",
      "Pelvis (MPII)   \n",
      "Thorax (MPII)   \n",
      "Spine (H36M)    \n",
      "Jaw (H36M)      \n",
      "Head (H36M)     \n",
      "Nose            \n",
      "Left Eye        \n",
      "Right Eye       \n",
      "Left Ear        \n",
      "Right Ear       \n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key in JOINT_MAP:    \n",
    "    print('{:16}'.format(key))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([157.5282,  98.7288,  29.9631,  29.8926, 100.6508, 159.0087, 141.9562,\n",
       "         91.9257,  69.3990,  72.2796,  97.8108, 141.5400,  67.9245,  98.5646])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pjpe.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35515, 24, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpjae = torch.load('mpjae_per_part.pt')\n",
    "mpjae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[98.1797,  9.4942, 51.9239],\n",
       "        [10.0485,  5.6163,  4.0774],\n",
       "        [ 9.8444,  4.6058,  4.8752],\n",
       "        [ 9.1616,  2.0758,  2.2850],\n",
       "        [16.9512,  5.1322,  6.1049],\n",
       "        [16.4851,  4.1317,  4.9168],\n",
       "        [ 4.4222,  2.1591,  2.0369],\n",
       "        [ 4.7690,  8.3333,  3.8979],\n",
       "        [ 5.5424, 11.1734,  6.7159],\n",
       "        [ 3.0120,  1.6893,  1.0108],\n",
       "        [11.9835,  8.3257,  8.1128],\n",
       "        [ 3.9105,  6.4853, 11.5482],\n",
       "        [ 5.9942,  8.0072,  5.4115],\n",
       "        [ 3.6484,  9.7804, 10.5708],\n",
       "        [ 4.6065,  7.8352, 10.8048],\n",
       "        [10.2832,  6.6511,  5.5037],\n",
       "        [ 8.7521,  7.7237, 12.1250],\n",
       "        [13.8173,  8.5829, 11.1074],\n",
       "        [16.6280, 18.6774, 13.1059],\n",
       "        [15.8921, 16.8754, 15.4179],\n",
       "        [ 8.4844,  6.3963, 17.3813],\n",
       "        [ 5.1584,  4.7116, 12.7035],\n",
       "        [ 3.2690,  2.1730,  1.5460],\n",
       "        [ 1.7931,  1.0212,  1.5960]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpjae_mean = mpjae.mean(dim=0)\n",
    "mpjae_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 6890)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "joint_regressor = np.load('data/J_regressor_extra.npy')\n",
    "\n",
    "joint_regressor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first sanity check should be whether ground truth compared to ground truth yields zero error\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "img_path = 'examples/image_00502_crop.jpg'\n",
    "\n",
    "pickle_path = 'data/3dpw/sequenceFiles/validation/courtyard_basketball_01.pkl'\n",
    "\n",
    "frame = 502\n",
    "\n",
    "# Load the .pkl sequence file containing the ground-truth information from 3dpw\n",
    "\n",
    "seq = pkl.load(open(pickle_path,'rb'),encoding='latin-1') # opening the sequence file, latin-1 encoding for making it compatible with python3\n",
    "\n",
    "gt_pose = torch.tensor(seq['poses'][0][frame]).unsqueeze(0)\n",
    "\n",
    "# print(gt_pose)\n",
    "\n",
    "pred_rotmat = torch.zeros(1, 24, 3, 3)\n",
    "\n",
    "q = R.from_rotvec(gt_pose.reshape(1,24,-1)[0])\n",
    "\n",
    "pred_rotmat = torch.tensor(R.as_dcm(q)).unsqueeze(0)\n",
    "\n",
    "# print(pred_rotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "curr_batch_size = 1\n",
    "\n",
    "print(\"gt_pose\", gt_pose.shape, gt_pose)\n",
    "\n",
    "print(\"pred_rotmat\", pred_rotmat.shape, pred_rotmat)\n",
    "\n",
    "# Get ground truth orientation (already stored in gt_pose)\n",
    "gt_rotvec = torch.zeros(curr_batch_size,24,3) # Have to have an array of this shape to input into the rotation object (from 32,72 to 32,24,3)\n",
    "i = 0\n",
    "for row in gt_pose:\n",
    "    gt_rotvec[i] = torch.reshape(row,(24, -1))\n",
    "    i+=1\n",
    "print(\"gt_rotvec\", gt_rotvec.shape, gt_rotvec)\n",
    "    \n",
    "gt_euler = np.zeros((curr_batch_size, 24, 3)) # Using numpy here because it works with the rotation library\n",
    "i = 0\n",
    "for row in gt_rotvec:\n",
    "    r = R.from_rotvec(row)\n",
    "    gt_euler[i] = R.as_euler(r, 'xyz', degrees=True)\n",
    "    i+=1\n",
    "\n",
    "print(\"gt_euler\", gt_euler.shape, gt_euler)\n",
    "\n",
    "# Get Euler representation of the predictions too:\n",
    "\n",
    "pred_euler = np.zeros((curr_batch_size,24,3)) # Has to be a numpy array because it works with Rotation\n",
    "\n",
    "# For each row in pred_rotmat convert it to a Rotation object and write it into a corresponding\n",
    "# row in pred_euler as Euler angles\n",
    "\n",
    "i=0\n",
    "for row in pred_rotmat:\n",
    "    r = R.from_dcm(row.cpu())\n",
    "    pred_euler[i] = R.as_euler(r, 'xyz', degrees=True)\n",
    "    i+=1\n",
    "\n",
    "print(\"pred_euler\", pred_euler.shape, pred_euler)\n",
    "    \n",
    "    \n",
    "orientation_error_non_reduced = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                torch.from_numpy(pred_euler).to(device))**2)\n",
    "print(orientation_error_non_reduced)\n",
    "    \n",
    "orientation_error = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                torch.from_numpy(pred_euler).to(device))**2).sum(dim=-1).mean(dim=-1)\n",
    "# The reduction above is wrong. For a 90 degree error in one angle, it averages out 3.75 degrees, which\n",
    "# is 90/24. The correct reduction would be a mean of 1.25 (90/72), because there are 72 angles (3 for each part)\n",
    "# To remove the root, add [:,1:,:] to gt_euler and pred_euler above\n",
    "\n",
    "orientation_error_new = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                torch.from_numpy(pred_euler).to(device))**2).mean()\n",
    "# This reduction is more accurate because it averages the error per part and then the error across parts\n",
    "# It is equivalent to .mean(dim=-1).mean(dim=-1)\n",
    "\n",
    "print(\"orientation_error\")\n",
    "print(orientation_error)\n",
    "print()\n",
    "print(\"orientation_error_new\")\n",
    "print(orientation_error_new)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_orientation(gt, pred):\n",
    "    # Taking as input two axis_angle representations\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    curr_batch_size = 1\n",
    "\n",
    "    gt_pose = gt\n",
    "\n",
    "    print(\"gt_pose\", gt_pose.shape, gt_pose)\n",
    "\n",
    "    q = R.from_rotvec(pred.reshape(1,24,-1)[0])\n",
    "\n",
    "    pred_rotmat = torch.tensor(R.as_dcm(q)).unsqueeze(0)\n",
    "\n",
    "    print(\"pred_rotmat\", pred_rotmat.shape, pred_rotmat)\n",
    "\n",
    "    # Get ground truth orientation (already stored in gt_pose)\n",
    "    gt_rotvec = torch.zeros(curr_batch_size,24,3) # Have to have an array of this shape to input into the rotation object (from 32,72 to 32,24,3)\n",
    "    i = 0\n",
    "    for row in gt_pose:\n",
    "        gt_rotvec[i] = torch.reshape(row,(24, -1))\n",
    "        i+=1\n",
    "    print(\"gt_rotvec\", gt_rotvec.shape, gt_rotvec)\n",
    "\n",
    "    gt_euler = np.zeros((curr_batch_size, 24, 3)) # Using numpy here because it works with the rotation library\n",
    "    i = 0\n",
    "    for row in gt_rotvec:\n",
    "        r = R.from_rotvec(row)\n",
    "        gt_euler[i] = R.as_euler(r, 'xyz', degrees=True)\n",
    "        i+=1\n",
    "\n",
    "    print(\"gt_euler\", gt_euler.shape, gt_euler)\n",
    "\n",
    "    # Get Euler representation of the predictions too:\n",
    "\n",
    "    pred_euler = np.zeros((curr_batch_size,24,3)) # Has to be a numpy array because it works with Rotation\n",
    "\n",
    "    # For each row in pred_rotmat convert it to a Rotation object and write it into a corresponding\n",
    "    # row in pred_euler as Euler angles\n",
    "\n",
    "    i=0\n",
    "    for row in pred_rotmat:\n",
    "        r = R.from_dcm(row.cpu())\n",
    "        pred_euler[i] = R.as_euler(r, 'xyz', degrees=True)\n",
    "        i+=1\n",
    "\n",
    "    print(\"pred_euler\", pred_euler.shape, pred_euler)\n",
    "\n",
    "\n",
    "    orientation_error_non_reduced = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                    torch.from_numpy(pred_euler).to(device))**2)\n",
    "    print(\"error per part\", orientation_error_non_reduced)\n",
    "\n",
    "    orientation_error = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                    torch.from_numpy(pred_euler).to(device))**2).sum(dim=-1).mean(dim=-1)\n",
    "    # The reduction above is wrong. For a 90 degree error in one angle, it averages out 3.75 degrees, which\n",
    "    # is 90/24. The correct reduction would be a mean of 1.25 (90/72), because there are 72 angles (3 for each part)\n",
    "    # To remove the root, add [:,1:,:] to gt_euler and pred_euler above\n",
    "\n",
    "    orientation_error_new = torch.sqrt((torch.from_numpy(gt_euler).to(device) -\n",
    "                                    torch.from_numpy(pred_euler).to(device))**2).mean()\n",
    "    # This reduction is more accurate because it averages the error per part and then the error across parts\n",
    "    # It is equivalent to .mean(dim=-1).mean(dim=-1)\n",
    "\n",
    "    print(\"orientation_error\")\n",
    "    print(orientation_error.item())\n",
    "    print()\n",
    "    print(\"orientation_error_new\")\n",
    "    print(orientation_error_new.item())\n",
    "    print()\n",
    "\n",
    "    #moe[step * batch_size:step * batch_size + curr_batch_size] = orientation_error.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def compare_orientation_rotvec(gt, pred):\n",
    "    # Taking as input two axis_angle representations\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    curr_batch_size = 1\n",
    "\n",
    "    gt_pose = gt\n",
    "\n",
    "    print(\"gt_pose\", gt_pose.shape, gt_pose)\n",
    "    print(\"pred_pose\", pred.shape, pred)\n",
    "\n",
    "    # Get ground truth orientation (already stored in gt_pose)\n",
    "    gt_rotvec = torch.zeros((curr_batch_size,24,3), dtype=torch.double) # Have to have an array of this shape to input into the rotation object (from 32,72 to 32,24,3)\n",
    "    i = 0\n",
    "    for row in gt_pose:\n",
    "        gt_rotvec[i] = torch.reshape(row,(24, -1))\n",
    "        i+=1\n",
    "    print(\"gt_rotvec\", gt_rotvec.shape, gt_rotvec)\n",
    "    \n",
    "    # Get prediction as rotation vectors\n",
    "    \n",
    "    r = R.from_rotvec(pred.reshape(1,24,-1)[0])\n",
    "    pred_rotvec = torch.tensor(R.as_rotvec(r)).unsqueeze(0)\n",
    "\n",
    "    print(\"pred_rotvec\", pred_rotvec.shape, pred_rotvec)\n",
    "\n",
    "    orientation_error_non_reduced = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2))\n",
    "    \n",
    "    print(\"error per part\", orientation_error_non_reduced)\n",
    "\n",
    "    orientation_error = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2).sum(dim=-1).mean(dim=-1))\n",
    "    # The reduction above is wrong. For a 90 degree error in one angle, it averages out 3.75 degrees, which\n",
    "    # is 90/24. The correct reduction would be a mean of 1.25 (90/72), because there are 72 angles (3 for each part)\n",
    "    # To remove the root, add [:,1:,:] to gt_euler and pred_euler above\n",
    "\n",
    "    orientation_error_new = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2).mean())\n",
    "    # This reduction is more accurate because it averages the error per part and then the error across parts\n",
    "    # It is equivalent to .mean(dim=-1).mean(dim=-1)\n",
    "\n",
    "    print(\"orientation_error\")\n",
    "    print(orientation_error.item())\n",
    "    print()\n",
    "    print(\"orientation_error_new\")\n",
    "    print(orientation_error_new.item())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def compare_orientation_rotvec(gt_pose, pred_rotmat):\n",
    "    # Taking as input one axis angle representation and one rotation matrix representation\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    curr_batch_size = 1\n",
    "\n",
    "    gt_pose = gt\n",
    "\n",
    "    #print(\"gt_pose\", gt_pose.shape, gt_pose)\n",
    "    #print(\"pred_pose\", pred.shape, pred)\n",
    "\n",
    "    # Get ground truth orientation (already stored in gt_pose)\n",
    "    gt_rotvec = torch.zeros((curr_batch_size,24,3), dtype=torch.double) # Have to have an array of this shape to input into the rotation object (from 32,72 to 32,24,3)\n",
    "    i = 0\n",
    "    for row in gt_pose:\n",
    "        gt_rotvec[i] = torch.reshape(row,(24, -1))\n",
    "        i+=1\n",
    "    print(\"gt_rotvec\", gt_rotvec.shape, gt_rotvec)\n",
    "    \n",
    "    # Get prediction as rotation vectors\n",
    "    \n",
    "    pred_rotvec = torch.zeros((curr_batch_size,24,3)) # Has to be a numpy array because it works with Rotation\n",
    "\n",
    "    # For each row in pred_rotmat convert it to a Rotation object and write it into a corresponding\n",
    "    # row in pred_rotvec as rotation vectors\n",
    "\n",
    "    i=0\n",
    "    for row in pred_rotmat:\n",
    "        r = R.from_dcm(row.cpu())\n",
    "        pred_rotvec[i] = R.as_rotvec(r)\n",
    "        i+=1\n",
    "\n",
    "    print(\"pred_rotvec\", pred_rotvec.shape, pred_rotvec)\n",
    "\n",
    "    orientation_error_non_reduced = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2))\n",
    "    \n",
    "    print(\"error per part\", orientation_error_non_reduced)\n",
    "\n",
    "    orientation_error = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2).sum(dim=-1).mean(dim=-1))\n",
    "    # The reduction above is wrong. For a 90 degree error in one angle, it averages out 3.75 degrees, which\n",
    "    # is 90/24. The correct reduction would be a mean of 1.25 (90/72), because there are 72 angles (3 for each part)\n",
    "    # To remove the root, add [:,1:,:] to gt_euler and pred_euler above\n",
    "\n",
    "    orientation_error_new = np.degrees(torch.sqrt((gt_rotvec - pred_rotvec)**2).mean())\n",
    "    # This reduction is more accurate because it averages the error per part and then the error across parts\n",
    "    # It is equivalent to .mean(dim=-1).mean(dim=-1)\n",
    "\n",
    "    print(\"orientation_error\")\n",
    "    print(orientation_error.item())\n",
    "    print()\n",
    "    print(\"orientation_error_new\")\n",
    "    print(orientation_error_new.item())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_pose torch.Size([1, 72]) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "pred_pose torch.Size([1, 72]) tensor([[0.7854, 0.7854, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "gt_rotvec torch.Size([1, 24, 3]) tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]], dtype=torch.float64)\n",
      "pred_rotvec torch.Size([1, 24, 3]) tensor([[[0.7854, 0.7854, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]]], dtype=torch.float64)\n",
      "error per part tensor([[[45.0000, 45.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], dtype=torch.float64)\n",
      "orientation_error\n",
      "3.750000104353257\n",
      "\n",
      "orientation_error_new\n",
      "1.250000034784419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(1,72)\n",
    "b = torch.zeros(1,72)\n",
    "\n",
    "b[0][0] = np.pi/4\n",
    "b[0][1] = np.pi/4\n",
    "\n",
    "compare_orientation_rotvec(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [[(1/np.sqrt(2)),0,(1/np.sqrt(2))],\n",
    "    [1/2, (1/np.sqrt(2)), -1/2],\n",
    "    [-1/2, (1/np.sqrt(2)), 1/2]]\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a rotation of 45 degrees around x multiplied on the\n",
    "# left by a rotation of 45 degrees around y\n",
    "\n",
    "m = [[(1/np.sqrt(2)),1/2,1/2],\n",
    "    [0, (1/np.sqrt(2)), -(1/np.sqrt(2))],\n",
    "    [-(1/np.sqrt(2)), 1/2, 1/2]]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = R.from_dcm(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R.as_euler(n, 'xyz', degrees=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = R.from_euler('xyz', [45, 45, 0], degrees=True)\n",
    "\n",
    "print(o.as_dcm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
